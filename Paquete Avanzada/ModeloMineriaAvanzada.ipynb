{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fGfAaC9dUfO"
      },
      "source": [
        "**LEAD University - Minería de datos**\n",
        "\n",
        "Python Project\n",
        "\n",
        "**Contributors**\n",
        "- Carolina Salas Moreno\n",
        "- Deykel Bernard Salazar\n",
        "- Esteban Ramirez Montano\n",
        "- Kristhel Porras Mata\n",
        "- Marla Gomez Hernández\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pxx9r1xc-1c"
      },
      "source": [
        "## Requirements\n",
        "**Step 1:** Please install Microsoft C++ Build Tools in your machine.\n",
        "\n",
        "**Step 2:** Install Python 3.11.7\n",
        "\n",
        "**Step 3:** Run the following code if this is your first time running it `pip install -r requirements.txt`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZXUiJAacFRT"
      },
      "source": [
        "# Importar las librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dQw57BLb3dFh"
      },
      "outputs": [],
      "source": [
        "# Main Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import umap.umap_ as umap\n",
        "\n",
        "# Data Optimization\n",
        "from sklearn_genetic import GASearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import SelectFromModel \n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn_genetic.space import Integer, Categorical, Continuous\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "#Feature Selection\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LassoCV \n",
        "\n",
        "# Clustering Libraries\n",
        "from abc import ABCMeta, abstractmethod\n",
        "from scipy.cluster.hierarchy import dendrogram, ward, single, complete, average, linkage, fcluster\n",
        "from sklearn.cluster import KMeans\n",
        "from pyclustering.cluster.kmedoids import kmedoids\n",
        "\n",
        "# Dimensionality Reduction\n",
        "from prince import PCA as PCA_Prince\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Evaluation Metrics\n",
        "from sklearn.metrics import silhouette_score, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "# Regression Models\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "\n",
        "# Classification Models\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, RandomForestRegressor\n",
        "\n",
        "# Additional Tools\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScX-De7IdB7L"
      },
      "source": [
        "## EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HIssRCV8cKBa"
      },
      "outputs": [],
      "source": [
        "# Análisis Exploratorio de Datos (EDA)\n",
        "class EDA:\n",
        "    def __init__(self, file=None):\n",
        "        \"\"\"\n",
        "        Inicializa la clase EDA y carga datos desde un archivo CSV si se proporciona.\n",
        "\n",
        "        Parámetros:\n",
        "            file (str): Ruta al archivo CSV. Si no se proporciona, se inicializa un DataFrame vacío.\n",
        "        \"\"\"\n",
        "        self.__df = pd.read_csv(file) if file else pd.DataFrame()\n",
        "\n",
        "    def head_df(self, n=5):\n",
        "        return self.__df.head(n) if not self.__df.empty else \"No se cargaron los datos :(\"\n",
        "\n",
        "    def tail_df(self, n=5):\n",
        "        return self.__df.tail(n) if not self.__df.empty else \"No se cargaron los datos :(\"\n",
        "\n",
        "    def check_data_types(self):\n",
        "        return self.__df.dtypes\n",
        "\n",
        "    def drop_irrelevant_columns(self, columns):\n",
        "        self.__df.drop(columns=columns, inplace=True)\n",
        "\n",
        "    def drop_missing_values(self):\n",
        "        self.__df.dropna(inplace=True)\n",
        "\n",
        "    def detect_outliers(self):\n",
        "        num_df = self.__df.select_dtypes(include=['float64', 'int64'])\n",
        "        if num_df.empty:\n",
        "            return \"No hay columnas numéricas en el DataFrame.\"\n",
        "\n",
        "        Q1 = num_df.quantile(0.25)\n",
        "        Q3 = num_df.quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        outliers = ((num_df < (Q1 - 1.5 * IQR)) | (num_df > (Q3 + 1.5 * IQR))).sum()\n",
        "        Dicc_outliers = {col: outliers[col] for col in num_df.columns if outliers[col] > 0}\n",
        "\n",
        "        return Dicc_outliers if Dicc_outliers else \"No se detectaron valores atípicos en las columnas numéricas.\"\n",
        "\n",
        "    def plot_scatter(self, col1, col2):\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.scatterplot(x=self.__df[col1], y=self.__df[col2])\n",
        "        plt.title(f'Gráfico de Dispersión: {col1} vs {col2}')\n",
        "        plt.xlabel(col1)\n",
        "        plt.ylabel(col2)\n",
        "        plt.grid()\n",
        "        plt.show()\n",
        "\n",
        "    def plot_histogram(self, col):\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.histplot(self.__df[col], kde=True)\n",
        "        plt.title(f'Histograma de {col}')\n",
        "        plt.xlabel(col)\n",
        "        plt.ylabel('Frecuencia')\n",
        "        plt.show()\n",
        "\n",
        "    def plot_heatmap(self):\n",
        "        num_df = self.__df.select_dtypes(include=['float64', 'int64'])\n",
        "        if num_df.empty:\n",
        "            return \"No hay columnas numéricas para generar el mapa de calor.\"\n",
        "\n",
        "        num_df = num_df.loc[:, num_df.apply(lambda x: np.std(x) > 0.01)]\n",
        "\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        sns.heatmap(num_df.corr(), cmap=\"coolwarm\", annot= True, linewidths=0.5, cbar=True) #annot=False es para que no se vean los numeros en los cuadros\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.yticks(rotation=0)\n",
        "        plt.title(\"Correlation heatmap\", fontsize=18)\n",
        "        plt.ion()\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"Clase EDA - DataFrame de la forma: {self.__df.shape}\"\n",
        "\n",
        "    def get_df(self):\n",
        "        \"\"\"Devuelve una copia del df para que las familias de los algoritmos las utilicen\"\"\"\n",
        "        return self.__df.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DataOptimization(EDA):\n",
        "    def __init__(self, datos_eda):\n",
        "        \"\"\"\n",
        "        Usa el DataFrame procesado de la clase EDA.\n",
        "        \"\"\"\n",
        "        df = datos_eda.get_df()\n",
        "        super().__init__()\n",
        "        self.__df = df\n",
        "        self.X_train = None\n",
        "        self.X_test = None\n",
        "        self.y_train = None\n",
        "        self.y_test = None\n",
        "        self.models = {\n",
        "            'LinearRegression': LinearRegression(),\n",
        "            'DecisionTreeRegressor': DecisionTreeRegressor(),\n",
        "            'RandomForestRegressor': RandomForestRegressor(),\n",
        "            'Lasso': Lasso(),\n",
        "            'Ridge': Ridge(),\n",
        "            'KNeighborsRegressor': KNeighborsRegressor(),\n",
        "            'XGBRegressor': XGBRegressor(random_state=42)\n",
        "        }\n",
        "        self.param_grids_genetic = self._get_param_grids_genetic()\n",
        "\n",
        "    def get_data_split(self):\n",
        "        \"\"\"\n",
        "        Asks the user for the percentage of data for training.\n",
        "        \"\"\"\n",
        "        while True:\n",
        "            try:\n",
        "                percent = float(input(\"Enter the percentage for the training set: \"))\n",
        "                if 0 < percent < 100:\n",
        "                    return percent / 100\n",
        "                else:\n",
        "                    print(\"The percentage must be between 1 and 99.\")\n",
        "            except ValueError:\n",
        "                print(\"Invalid number. Try again.\")\n",
        "\n",
        "    def split_df(self, target_column, test_size=None, random_state=42):\n",
        "        \"\"\"\n",
        "        Splits the dataframe into training and test sets.\n",
        "\n",
        "        Parameters:\n",
        "        - target_column: str -> Name of the target column (y).\n",
        "        - test_size: float -> Proportion of the test set (if not provided, it is calculated from the entered percentage).\n",
        "        - random_state: int -> Seed for randomization.\n",
        "\n",
        "        Returns:\n",
        "        - X_train, X_test, y_train, y_test: Split and preprocessed datasets.\n",
        "        \"\"\"\n",
        "        if test_size is None:\n",
        "            # If test_size is not provided, ask the user for the percentage\n",
        "            test_size = 1 - self.get_data_split()\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                # Separate features (X) and target variable (y)\n",
        "                X = self.__df.drop(columns=[target_column])\n",
        "                y = self.__df[target_column]\n",
        "                break  # Exit the loop if there are no errors\n",
        "            except KeyError:\n",
        "                print(f\"The column '{target_column}' does not exist. Try again.\")\n",
        "                print(\"Available columns:\")\n",
        "                print(self.check_data_types())\n",
        "                target_column = input(\"Enter the correct name of the target column: \")\n",
        "\n",
        "        # Preprocess features (X), convert categorical variables to One-Hot Encoding\n",
        "        import pandas as pd\n",
        "        X = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "        # Check if the target variable (y) is categorical and needs encoding\n",
        "        if y.dtypes == 'object' or y.dtypes.name == 'category':\n",
        "            from sklearn.preprocessing import LabelEncoder\n",
        "            le = LabelEncoder()\n",
        "            y = le.fit_transform(y)\n",
        "\n",
        "        # Perform the split\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=test_size, random_state=random_state\n",
        "        )\n",
        "\n",
        "        print(f\"Data split:\\n- Training: {X_train.shape[0]} rows\\n- Test: {X_test.shape[0]} rows\")\n",
        "        return X_train, X_test, y_train, y_test\n",
        "    \n",
        "#------------------------Genectic Search Components--------------------------------------------------------------\n",
        "\n",
        "    def _get_param_grids_genetic(self):\n",
        "        \"\"\"Defines the parameters used for each algorithm in the genetic search.\"\"\"\n",
        "        return {\n",
        "            'LinearRegression': {\n",
        "                \"clf__copy_X\": Categorical([True, False]),\n",
        "                \"clf__fit_intercept\": Categorical([True, False]),\n",
        "                \"clf__positive\": Categorical([True, False])\n",
        "            },\n",
        "            'DecisionTreeRegressor': {\n",
        "                \"clf__max_depth\": Integer(3, 10),\n",
        "                'clf__min_samples_split': Integer(2, 10),\n",
        "                'clf__min_samples_leaf': Integer(1, 4),\n",
        "                'clf__random_state': Categorical([42])\n",
        "            },\n",
        "            'RandomForestRegressor': {\n",
        "                \"clf__n_estimators\": Integer(50, 100),\n",
        "                \"clf__max_depth\": Integer(5, 10),\n",
        "                'clf__min_samples_split': Integer(2, 5),\n",
        "                'clf__random_state': Categorical([42])\n",
        "            },\n",
        "            'Lasso': {\n",
        "                'clf__alpha': Continuous(1.0, 1.0),\n",
        "                'clf__fit_intercept': Categorical([True, False]),\n",
        "                'clf__max_iter': Integer(1000, 2000),\n",
        "                'clf__tol': Continuous(0.0001, 0.001),\n",
        "                'clf__selection': Categorical(['cyclic', 'random'])\n",
        "            },\n",
        "            'Ridge': {\n",
        "                'clf__alpha': Continuous(1.0, 1.0),\n",
        "                'clf__fit_intercept': Categorical([True, False]),\n",
        "                'clf__tol': Continuous(0.0001, 0.001),\n",
        "                'clf__solver': Categorical(['auto', 'svd', 'cholesky'])\n",
        "            },\n",
        "            'KNeighborsRegressor': {\n",
        "                'clf__n_neighbors': Integer(3, 7),\n",
        "                'clf__weights': Categorical(['uniform', 'distance']),\n",
        "                'clf__algorithm': Categorical(['auto', 'ball_tree', 'kd_tree'])\n",
        "            },\n",
        "            'XGBRegressor': {\n",
        "                'clf__learning_rate': Continuous(0.01, 0.1),\n",
        "                'clf__n_estimators': Integer(50, 100),\n",
        "                'clf__max_depth': Integer(3, 5),\n",
        "                'clf__subsample': Continuous(0.8, 1.0),\n",
        "                'clf__colsample_bytree': Continuous(0.8, 1.0)\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def genetic_search(self):\n",
        "        \"\"\"\n",
        "        Optimize regression models using genetic algorithms.\n",
        "        \"\"\"\n",
        "\n",
        "        # Eliminar este if despues del testing ERM\n",
        "        if self.X_train is None or self.X_test is None:\n",
        "            print(\"Error: Debes ejecutar split_df() antes de llamar a genetic_search().\")\n",
        "            return\n",
        "\n",
        "        results = {}\n",
        "        \n",
        "        # Feature selection\n",
        "        lasso_cv = LassoCV(cv=5) \n",
        "        lasso_cv.fit(self.X_train, self.y_train)\n",
        "        f_selection = SelectFromModel(lasso_cv)\n",
        "\n",
        "        modelo_base_rfe = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "        rfe = RFE(modelo_base_rfe, n_features_to_select=n_features)\n",
        "        X_rfe_selected = rfe.fit_transform(self.X_selected, self.y)\n",
        "        \n",
        "        self.selected_features_rfe = self.X_selected.columns[rfe.support_]\n",
        "        self.X_selected = self.X_selected[self.selected_features_rfe]\n",
        "        print(f\"Características finales seleccionadas por RFE: {list(self.selected_features_rfe)}\")\n",
        "\n",
        "\n",
        "        for name, model in self.models.items():\n",
        "            self.X_train = f_selection.transform(self.X_train)\n",
        "            self.X_test = f_selection.transform(self.X_test)\n",
        "            pl = Pipeline([\n",
        "              ('fs', f_selection), \n",
        "              ('clf', model), \n",
        "            ])            \n",
        "            print(f\"Entrenando {name} con método genético...\")\n",
        "            evolved_estimator = GASearchCV(\n",
        "                estimator=pl,\n",
        "                cv=5,\n",
        "                scoring=\"neg_mean_squared_error\",\n",
        "                population_size=10,\n",
        "                generations=5,\n",
        "                tournament_size=3,\n",
        "                elitism=True,\n",
        "                crossover_probability=0.8,\n",
        "                mutation_probability=0.1,\n",
        "                param_grid=self.param_grids_genetic[name],\n",
        "                algorithm=\"eaSimple\",\n",
        "                n_jobs=-1,\n",
        "                error_score='raise',\n",
        "                verbose=True\n",
        "            )\n",
        "            evolved_estimator.fit(self.X_train, self.y_train)\n",
        "            results[name] = {\n",
        "                'best_params': evolved_estimator.best_params_,\n",
        "                'estimator': evolved_estimator.best_estimator_\n",
        "            }\n",
        "        return results\n",
        "\n",
        "#------------------------Exhaustive Search Components--------------------------------------------------------------\n",
        "\n",
        "    def _get_param_grids_exhaustive(self):\n",
        "        \"\"\"Defines the parameters used for each algorithm in the exhaustive search.\"\"\"\n",
        "        return {\n",
        "            \n",
        "            'LinearRegression': {\n",
        "                \"clf__copy_X\": [True, False],\n",
        "                \"clf__fit_intercept\": [True, False],\n",
        "                \"clf__positive\": [True, False]\n",
        "            },\n",
        "            'DecisionTreeRegressor': {\n",
        "                \"clf__max_depth\": [3, 5, 7, 10],\n",
        "                'clf__min_samples_split': [2, 5, 10],\n",
        "                'clf__min_samples_leaf': [1, 2, 4],\n",
        "                'clf__random_state': [42]\n",
        "            },\n",
        "            'RandomForestRegressor': {\n",
        "                \"clf__n_estimators\": [50, 100],\n",
        "                \"clf__max_depth\": [5, 10],\n",
        "                'clf__min_samples_split': [2, 5],\n",
        "                'clf__random_state': [42]\n",
        "            },\n",
        "            'Lasso': {\n",
        "                'clf__alpha': [1.0],\n",
        "                'clf__fit_intercept': [True, False],\n",
        "                'clf__max_iter': [1000, 2000],\n",
        "                'clf__tol': [0.0001, 0.001],\n",
        "                'clf__selection': ['cyclic', 'random']\n",
        "            },\n",
        "            'Ridge': {\n",
        "                'clf__alpha': [1.0],\n",
        "                'clf__fit_intercept': [True, False],\n",
        "                'clf__tol': [0.0001, 0.001],\n",
        "                'clf__solver': ['auto', 'svd', 'cholesky']\n",
        "            },\n",
        "            'KNeighborsRegressor': {\n",
        "                'clf__n_neighbors': [3, 5, 7],\n",
        "                'clf__weights': ['uniform', 'distance'],\n",
        "                'clf__algorithm': ['auto', 'ball_tree', 'kd_tree']\n",
        "            },\n",
        "            'XGBRegressor': {\n",
        "                'clf__learning_rate': [0.01, 0.1],\n",
        "                'clf__n_estimators': [50, 100],\n",
        "                'clf__max_depth': [3, 5],\n",
        "                'clf__subsample': [0.8, 1.0],\n",
        "                'clf__colsample_bytree': [0.8, 1.0]\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def exhaustive_search(self):\n",
        "        \"\"\"\n",
        "        Solves optimization problems by creating a population or group of possible solutions to the problem.\n",
        "        \"\"\"\n",
        "        \"\"\"Realiza la búsqueda exhaustiva de hiperparámetros para cada modelo.\"\"\"\n",
        "        results = {}\n",
        "        lasso_cv = LassoCV(cv=5) \n",
        "        lasso_cv.fit(self.X_train, self.y_train)\n",
        "        f_selection = SelectFromModel(lasso_cv)\n",
        "        for name, model in self.models.items():\n",
        "            self.X_train = f_selection.transform(self.X_train)\n",
        "            self.X_test = f_selection.transform(self.X_test)\n",
        "            pl = Pipeline([\n",
        "              ('clf', model), \n",
        "            ])\n",
        "            print(f\"Entrenando {name} con método exhaustivo...\")\n",
        "            grid_search = GridSearchCV(\n",
        "                estimator=pl,\n",
        "                param_grid=self.param_grids_exhaustive[name],\n",
        "                cv=5,\n",
        "                scoring='neg_mean_squared_error',\n",
        "                n_jobs=-1,\n",
        "                verbose=1\n",
        "            )\n",
        "            grid_search.fit(self.X_train, self.y_train)\n",
        "            results[name] = {\n",
        "                'best_params': grid_search.best_params_,\n",
        "                'estimator': grid_search.best_estimator_\n",
        "            }\n",
        "        return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MI TESTING\n",
        "archivo_csv = \"../dataset/dataset.csv\"\n",
        "eda = EDA(file=archivo_csv)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHJDmyoocftY"
      },
      "outputs": [],
      "source": [
        "class NoSupervisado(EDA):\n",
        "    def __init__(self, datos_eda):\n",
        "        # La clase ya utiliza el df procesado en la clase EDA\n",
        "        df = datos_eda.get_df()\n",
        "        super().__init__()\n",
        "        self.__df = df\n",
        "\n",
        "    @property\n",
        "    def df(self):\n",
        "        return self.__df\n",
        "\n",
        "    @df.setter\n",
        "    def df(self, p_df):\n",
        "        self.__df = p_df\n",
        "\n",
        "    def __byebye_object_values(self):\n",
        "        # Elimina columnas de tipo 'object'\n",
        "        self.__df = self.__df.select_dtypes(exclude=['object'])\n",
        "\n",
        "    def calcular_metricas(self, labels):\n",
        "        \"\"\"\n",
        "        Calcula métricas de evaluación para clustering.\n",
        "        \"\"\"\n",
        "        data = self.__df.dropna()\n",
        "        data = (data - data.mean()) / data.std()\n",
        "        metrics = {\n",
        "            \"Índice de Silueta\": silhouette_score(data, labels),\n",
        "            \"Calinski-Harabasz\": calinski_harabasz_score(data, labels),\n",
        "            \"Davies-Bouldin\": davies_bouldin_score(data, labels)\n",
        "        }\n",
        "        return metrics\n",
        "\n",
        "    def kmeans(self, n_clusters):\n",
        "        self.__byebye_object_values()\n",
        "        data = self.__df\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "        labels = kmeans.fit_predict(data)\n",
        "        metrics = self.calcular_metricas(labels)\n",
        "        print(f\"Métricas para K-Means (n_clusters={n_clusters}): {metrics}\")\n",
        "        return metrics\n",
        "\n",
        "    def k_medoids(self, n_clusters, metric='euclidean'):\n",
        "        self.__byebye_object_values()\n",
        "        data = self.__df\n",
        "        \n",
        "        # Convertir a numpy array si aún no lo es\n",
        "        data_array = np.array(data)\n",
        "        \n",
        "        # Inicialización de medoides (seleccionar índices aleatorios)\n",
        "        np.random.seed(42)  # Para reproducibilidad\n",
        "        initial_medoids = np.random.choice(len(data_array), n_clusters, replace=False).tolist()\n",
        "        \n",
        "        # Crear y ejecutar el algoritmo KMedoids\n",
        "        kmedoids_instance = kmedoids(data_array, initial_medoids)\n",
        "        kmedoids_instance.process()\n",
        "        \n",
        "        # Obtener clusters y medoides\n",
        "        clusters = kmedoids_instance.get_clusters()  # Lista de listas de índices\n",
        "        medoids = kmedoids_instance.get_medoids()    # Lista de índices de medoides\n",
        "        \n",
        "        # Crear etiquetas en formato sklearn (un número para cada punto)\n",
        "        labels = np.zeros(len(data_array), dtype=int)\n",
        "        for cluster_idx, cluster in enumerate(clusters):\n",
        "            for point_idx in cluster:\n",
        "                labels[point_idx] = cluster_idx\n",
        "        \n",
        "        # Calcular métricas\n",
        "        metrics = self.calcular_metricas(labels)\n",
        "        print(f\"Métricas para K-Medoids (n_clusters={n_clusters}, metric={metric}): {metrics}\")\n",
        "        \n",
        "        return metrics\n",
        "\n",
        "    def hac(self, n_clusters=3, method='ward'):\n",
        "        self.__byebye_object_values()\n",
        "        data = self.__df\n",
        "        linkage_matrix = linkage(data, method=method)\n",
        "        labels = fcluster(linkage_matrix, t=n_clusters, criterion='maxclust')\n",
        "        metrics = self.calcular_metricas(labels)\n",
        "        print(f\"Métricas para HAC (n_clusters={n_clusters}, method={method}): {metrics}\")\n",
        "        return metrics\n",
        "\n",
        "    def umap_model(self, n_components=2, n_neighbors=15):\n",
        "        self.__byebye_object_values()\n",
        "        data = self.__df\n",
        "        modelo_umap = UMAP(n_components=n_components, n_neighbors=n_neighbors)\n",
        "        components = modelo_umap.fit_transform(data)\n",
        "        kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "        labels = kmeans.fit_predict(components)\n",
        "        metrics = self.calcular_metricas(labels)\n",
        "        print(f\"Métricas para UMAP (n_components={n_components}, n_neighbors={n_neighbors}): {metrics}\")\n",
        "        return metrics\n",
        "\n",
        "    def comparar_algoritmos(self, n_clusters):\n",
        "\n",
        "        if self.__df.isnull().any().any():\n",
        "          print(\"El DataFrame contiene valores nulos. Se eliminarán automáticamente para continuar.\")\n",
        "          self.__df.dropna(inplace=True)\n",
        "\n",
        "        print(\"\\nEjecutando K-Means...\")\n",
        "        kmeans_metrics = self.kmeans(n_clusters)\n",
        "\n",
        "        print(\"\\nEjecutando K-Medoids...\")\n",
        "        kmedoids_metrics = self.k_medoids(n_clusters)\n",
        "\n",
        "        print(\"\\nEjecutando HAC...\")\n",
        "        hac_metrics = self.hac(n_clusters=n_clusters)\n",
        "\n",
        "        print(\"\\nEjecutando UMAP...\")\n",
        "        umap_metrics = self.umap_model(n_components=2, n_neighbors=15)\n",
        "\n",
        "        resultados = pd.DataFrame({\n",
        "            \"K-Means\": kmeans_metrics,\n",
        "            \"K-Medoids\": kmedoids_metrics,\n",
        "            \"HAC\": hac_metrics,\n",
        "            \"UMAP\": umap_metrics\n",
        "        }).T\n",
        "\n",
        "        print(\"\\nComparación de Algoritmos:\")\n",
        "        print(resultados)\n",
        "        return resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grQ-5IXLjR-7"
      },
      "outputs": [],
      "source": [
        "class Supervisado(EDA):\n",
        "    def __init__(self, datos_eda):\n",
        "        # La clase ya utiliza el df procesado en la clase EDA\n",
        "        df = datos_eda.get_df()\n",
        "        super().__init__()\n",
        "        self.__df = df\n",
        "\n",
        "    @property\n",
        "    def df(self):\n",
        "        return self.__df\n",
        "\n",
        "    @df.setter\n",
        "    def df(self, p_df):\n",
        "        self.__df = p_df\n",
        "\n",
        "#-----------------Cosas del split----------------------------\n",
        "\n",
        "    def obtener_datos_split(self):\n",
        "        \"\"\"\n",
        "        Solicita al usuario el porcentaje de datos para entrenamiento.\n",
        "        \"\"\"\n",
        "        while True:\n",
        "            try:\n",
        "                porcentaje = float(input(\"Introduce el porcentaje para el conjunto de entrenamiento: \"))\n",
        "                if 0 < porcentaje < 100:\n",
        "                    return porcentaje / 100\n",
        "                else:\n",
        "                    print(\"El porcentaje debe estar entre 1 y 99.\")\n",
        "            except ValueError:\n",
        "                print(\"Número no válido, use la mente.\")\n",
        "\n",
        "    def split_df(self, target_column, test_size=None, random_state=42):\n",
        "      \"\"\"\n",
        "      Divide el dataframe en conjuntos de train y test.\n",
        "\n",
        "      Parametros:\n",
        "      - target_column: str -> Nombre de la columna objetivo (y).\n",
        "      - test_size: float -> Proporcion del conjunto de prueba (si no se da un dato, se calcula del porcentaje ingresado).\n",
        "      - random_state: int -> Semilla para la aleatorización.\n",
        "\n",
        "      Returns:\n",
        "      - X_train, X_test, y_train, y_test: Conjuntos divididos y preprocesados.\n",
        "      \"\"\"\n",
        "      if test_size is None:\n",
        "          # Si no se proporciona test_size, solicita al usuario el porcentaje\n",
        "          test_size = 1 - self.obtener_datos_split()\n",
        "\n",
        "      while True:\n",
        "          try:\n",
        "              # Separar las características (X) y la variable objetivo (y)\n",
        "              X = self.__df.drop(columns=[target_column])\n",
        "              y = self.__df[target_column]\n",
        "              break  # Salir del bucle si no hay errores\n",
        "          except KeyError:\n",
        "              print(f\"La columna '{target_column}' no existe. Intente nuevamente.\")\n",
        "              print(\"Columnas disponibles:\")\n",
        "              print(self.check_data_types())\n",
        "              target_column = input(\"Ingrese el nombre correcto de la columna objetivo: \")\n",
        "\n",
        "      # Preprocesar características (X), convertir variables categóricas a One-Hot Encoding\n",
        "      import pandas as pd\n",
        "      X = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "      # Verificar si la variable objetivo (y) es categórica y necesita codificación\n",
        "      if y.dtypes == 'object' or y.dtypes.name == 'category':\n",
        "          from sklearn.preprocessing import LabelEncoder\n",
        "          le = LabelEncoder()\n",
        "          y = le.fit_transform(y)\n",
        "\n",
        "      # Realizar el split\n",
        "      X_train, X_test, y_train, y_test = train_test_split(\n",
        "          X, y, test_size=test_size, random_state=random_state\n",
        "      )\n",
        "\n",
        "      print(f\"Datos divididos:\\n- Entrenamiento: {X_train.shape[0]} filas\\n- Prueba: {X_test.shape[0]} filas\")\n",
        "      return X_train, X_test, y_train, y_test\n",
        "\n",
        "#-----------------Evaluacion de modelos----------------------------\n",
        "    def calcular_metricas(self, modelo, X_test, y_test, predicciones, modelo_nombre):\n",
        "      \"\"\"\n",
        "      Calcula las métricas de evaluación del modelo y guarda los resultados en un diccionario.\n",
        "\n",
        "      Parametros:\n",
        "      - modelo: El modelo en uso\n",
        "      - X_test: Datos de entrenamiento\n",
        "      - y_test: Datos de prueba\n",
        "      - predicciones: Predicciones del modelo\n",
        "      - modelo_nombre: Nombre del modelo\n",
        "\n",
        "      Returns:\n",
        "      - resultados: Diccionario con métricas de evaluación.\n",
        "      \"\"\"\n",
        "      mse = mean_squared_error(y_test, predicciones)\n",
        "      r2 = r2_score(y_test, predicciones)\n",
        "      mae = mean_absolute_error(y_test, predicciones)\n",
        "      rmse = np.sqrt(mse)\n",
        "      tolerancia = 0.1  # 10% de tolerancia\n",
        "      precision_global = np.mean(np.abs(y_test - predicciones) <= (tolerancia * y_test)) * 100\n",
        "\n",
        "      resultados = {\n",
        "          'modelo': modelo_nombre,\n",
        "          'MSE': mse,\n",
        "          'R2': r2,\n",
        "          'MAE': mae,\n",
        "          'RMSE': rmse,\n",
        "          'precision_global': precision_global,\n",
        "          #'predicciones': predicciones.tolist(),\n",
        "          #'valores_reales': y_test.tolist()\n",
        "      }\n",
        "      return resultados\n",
        "\n",
        "    def calcular_metricas_clasificacion(self, modelo, X_test, y_test, predicciones, modelo_nombre):\n",
        "      \"\"\"\n",
        "      Calcula las métricas de evaluación para modelos de clasificación y guarda los resultados en un diccionario.\n",
        "      \"\"\"\n",
        "      from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "      accuracy = accuracy_score(y_test, predicciones)\n",
        "      precision = precision_score(y_test, predicciones, average='weighted')\n",
        "      recall = recall_score(y_test, predicciones, average='weighted')\n",
        "      f1 = f1_score(y_test, predicciones, average='weighted')\n",
        "\n",
        "      return {\n",
        "          'modelo': modelo_nombre,\n",
        "          'accuracy': accuracy,\n",
        "          'precision': precision,\n",
        "          'recall': recall,\n",
        "          'f1_score': f1\n",
        "      }\n",
        "\n",
        "#------------------------Regression Models--------------------------------------------------------------\n",
        "\n",
        "    def regre_lineal_simple(self, X_train, X_test, y_train, y_test):\n",
        "      \"\"\"\n",
        "      Realiza una Regresión Lineal Simple y calcula múltiples métricas de rendimiento.\n",
        "      Returns:\n",
        "      - resultados: Diccionario con métricas de rendimiento del modelo\n",
        "      \"\"\"\n",
        "      print(\"Iniciando Regresión Lineal Simple...\")\n",
        "      modelo = LinearRegression()\n",
        "      modelo.fit(X_train, y_train)\n",
        "      predicciones = modelo.predict(X_test)\n",
        "      return self.calcular_metricas(modelo, X_test, y_test, predicciones, 'Regresión Lineal Simple')\n",
        "\n",
        "    def regre_svm(self, X_train, X_test, y_train, y_test):\n",
        "      \"\"\"\n",
        "      Realiza una Support Vector Machine y calcula múltiples métricas de rendimiento.\n",
        "      Returns:\n",
        "      - resultados: Diccionario con métricas de rendimiento del modelo\n",
        "      \"\"\"\n",
        "      print(\"Iniciando Support Vector Machine (SVM)...\")\n",
        "      from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "      # Escalar los datos\n",
        "      scaler = StandardScaler()\n",
        "      X_train_scaled = scaler.fit_transform(X_train)\n",
        "      X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "      modelo = SVR(kernel='rbf', C=1.0, gamma='scale')\n",
        "      modelo.fit(X_train_scaled, y_train)\n",
        "      predicciones = modelo.predict(X_test_scaled)\n",
        "\n",
        "      return self.calcular_metricas(modelo, X_test_scaled, y_test, predicciones, 'Support Vector Machine')\n",
        "\n",
        "\n",
        "    def regre_regridge(self, X_train, X_test, y_train, y_test):\n",
        "      \"\"\"\n",
        "      Realiza un Regresión Ridge y calcula múltiples métricas de rendimiento.\n",
        "      Returns:\n",
        "      - resultados: Diccionario con métricas de rendimiento del modelo\n",
        "      \"\"\"\n",
        "      print(\"Iniciando Regresión Ridge...\")\n",
        "      modelo = Ridge(alpha = 1.0)\n",
        "      modelo.fit(X_train, y_train)\n",
        "      predicciones = modelo.predict(X_test)\n",
        "      return self.calcular_metricas(modelo, X_test, y_test, predicciones, 'Regresión Ridge')\n",
        "\n",
        "    def regre_decisionTree(self, X_train, X_test, y_train, y_test):\n",
        "      \"\"\"\n",
        "      Realiza un Decision Tree Regressor y calcula múltiples métricas de rendimiento.\n",
        "      Returns:\n",
        "      - resultados: Diccionario con métricas de rendimiento del modelo\n",
        "      \"\"\"\n",
        "      print(\"Iniciando DecisionTreeRegressor..\")\n",
        "      modelo = DecisionTreeRegressor(random_state=0)\n",
        "      modelo.fit(X_train, y_train)\n",
        "      predicciones = modelo.predict(X_test)\n",
        "      return self.calcular_metricas(modelo, X_test, y_test, predicciones, 'Decision Tree Regressor')\n",
        "\n",
        "    def regre_randomforest(self, X_train, X_test, y_train, y_test):\n",
        "      \"\"\"\n",
        "      Realiza un Random Forest Regressor y calcula múltiples métricas de rendimiento.\n",
        "      Returns:\n",
        "      - resultados: Diccionario con métricas de rendimiento del modelo\n",
        "      \"\"\"\n",
        "      print(\"Iniciando RandomForest Regressor..\")\n",
        "      modelo = RandomForestRegressor(max_depth=2, random_state=0)\n",
        "      modelo.fit(X_train, y_train)\n",
        "      predicciones = modelo.predict(X_test)\n",
        "      return self.calcular_metricas(modelo, X_test, y_test, predicciones, 'Random Forest Regressor')\n",
        "\n",
        "    def regre_gradient_boosting(self, X_train, X_test, y_train, y_test):\n",
        "      \"\"\"\n",
        "      Realiza un Grandient Boostsing Regressor y calcula múltiples métricas de rendimiento.\n",
        "      Returns:\n",
        "      - resultados: Diccionario con métricas de rendimiento del modelo\n",
        "      \"\"\"\n",
        "      print(\"Iniciando Grandient Boostsing Regressor..\")\n",
        "      modelo = GradientBoostingRegressor(random_state=0)\n",
        "      modelo.fit(X_train, y_train)\n",
        "      predicciones = modelo.predict(X_test)\n",
        "      return self.calcular_metricas(modelo, X_test, y_test, predicciones, 'Grandient Boostsing Regressor')\n",
        "\n",
        "    def regre_xgboost(self, X_train, X_test, y_train, y_test):\n",
        "      \"\"\"\n",
        "      Realiza un XGBoost Regressor y calcula múltiples métricas de rendimiento.\n",
        "      Returns:\n",
        "      - resultados: Diccionario con métricas de rendimiento del modelo\n",
        "      \"\"\"\n",
        "      print(\"Iniciando XGBoost Regressor..\")\n",
        "      modelo = GradientBoostingRegressor(random_state=0)\n",
        "      modelo.fit(X_train, y_train)\n",
        "      predicciones = modelo.predict(X_test)\n",
        "      return self.calcular_metricas(modelo, X_test, y_test, predicciones, 'XGBoost Regressor')\n",
        "\n",
        "#------------------------Classification Models--------------------------------------------------------------\n",
        "\n",
        "    def classi_decision_tree(self, X_train, X_test, y_train, y_test):\n",
        "      \"\"\"\n",
        "      Realiza un modelo de clasificación usando Árbol de Decisión y calcula métricas de rendimiento.\n",
        "      \"\"\"\n",
        "      print(\"Iniciando Decision Tree Classifier...\")\n",
        "      from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "      modelo = DecisionTreeClassifier(random_state=0)\n",
        "      modelo.fit(X_train, y_train)\n",
        "      predicciones = modelo.predict(X_test)\n",
        "\n",
        "      return self.calcular_metricas_clasificacion(modelo, X_test, y_test, predicciones, 'Decision Tree')\n",
        "\n",
        "    def classi_knn(self, X_train, X_test, y_train, y_test):\n",
        "      \"\"\"\n",
        "      Realiza un modelo de clasificación usando K-Nearest Neighbors y calcula métricas de rendimiento.\n",
        "      \"\"\"\n",
        "      print(\"Iniciando K-Nearest Neighbors Classifier...\")\n",
        "      from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "      modelo = KNeighborsClassifier()\n",
        "      modelo.fit(X_train, y_train)\n",
        "      predicciones = modelo.predict(X_test)\n",
        "\n",
        "      return self.calcular_metricas_clasificacion(modelo, X_test, y_test, predicciones, 'K-Nearest Neighbors')\n",
        "\n",
        "    def classi_random_forest(self, X_train, X_test, y_train, y_test):\n",
        "      \"\"\"\n",
        "      Realiza un modelo de clasificación usando Random Forest y calcula métricas de rendimiento.\n",
        "      \"\"\"\n",
        "      print(\"Iniciando Random Forest Classifier...\")\n",
        "      from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "      modelo = RandomForestClassifier(random_state=0)\n",
        "      modelo.fit(X_train, y_train)\n",
        "      predicciones = modelo.predict(X_test)\n",
        "\n",
        "      return self.calcular_metricas_clasificacion(modelo, X_test, y_test, predicciones, 'Random Forest')\n",
        "\n",
        "    def classi_adaboost(self, X_train, X_test, y_train, y_test):\n",
        "      \"\"\"\n",
        "      Realiza un modelo de clasificación usando AdaBoost y calcula métricas de rendimiento.\n",
        "      \"\"\"\n",
        "      print(\"Iniciando AdaBoost Classifier...\")\n",
        "      from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "      modelo = AdaBoostClassifier(random_state=0)\n",
        "      modelo.fit(X_train, y_train)\n",
        "      predicciones = modelo.predict(X_test)\n",
        "\n",
        "      return self.calcular_metricas_clasificacion(modelo, X_test, y_test, predicciones, 'AdaBoost')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUAM4Lsdna5a"
      },
      "outputs": [],
      "source": [
        "class Start:\n",
        "    def __init__(self):\n",
        "        self.eda = None\n",
        "        self.supervisado = None\n",
        "        self.no_supervisado = None\n",
        "        self.split_data = None\n",
        "\n",
        "\n",
        "    def mostrar_menu(self):\n",
        "        while True:\n",
        "            print(\"\\n--- Menú Principal ---\")\n",
        "            print(\"1. 📁 Carga de datos en formato CSV y completar EDA\")\n",
        "            print(\"2. 🪐 Ejecutar modelo\")\n",
        "            print(\"3. 🛑 Salir\")\n",
        "            opcion = input(\"Seleccione una opción: \")\n",
        "\n",
        "            if opcion == \"1\":\n",
        "                self.datos_eda()\n",
        "            elif opcion == \"2\":\n",
        "                self.models_menu()\n",
        "            elif opcion == \"3\":\n",
        "                print(\"Saliendo del programa...\")\n",
        "                break\n",
        "            else:\n",
        "                print(\"Opción no válida. Intente de nuevo.\")\n",
        "\n",
        "    def models_menu(self):\n",
        "      while True:\n",
        "        print(\"\\n--- ¿Qué problema necesita resolver? ---\")\n",
        "        print(\"1. 🔍 Clasificación: Asigne etiquetas a sus datos\")\n",
        "        print(\"2. 📈 Regresión: Prediga valores continuos\")\n",
        "        print(\"3. 🧩 Aprendizaje No Supervisado: Descubra patrones ocultos\")\n",
        "        print(\"4. 🛑 Volver al menú principal\")\n",
        "        opcion = input(\"Seleccione una opción: \")\n",
        "\n",
        "        # Resetear split_data antes de cambiar de modelo\n",
        "        self.split_data = None\n",
        "        self.supervisado = None\n",
        "\n",
        "        if opcion == \"1\":\n",
        "            self.classi_modelos()\n",
        "        elif opcion == \"2\":\n",
        "            self.regre_modelos()\n",
        "        elif opcion == \"3\":\n",
        "                if self.eda and not self.eda.get_df().empty:\n",
        "                  self.no_supervisado = NoSupervisado(self.eda)\n",
        "                  n_clusters = int(input(\"Ingrese el número de clusters: \"))\n",
        "                  self.no_supervisado.comparar_algoritmos(n_clusters=n_clusters)\n",
        "                else:\n",
        "                    print(\"Primero cargue los datos para poder realizar aprendizaje no supervisado.\")\n",
        "        elif opcion == \"4\":\n",
        "            print(\"Saliendo del programa...\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"Opción no válida. Intente de nuevo.\")\n",
        "\n",
        "    def datos_eda(self):\n",
        "        while True:\n",
        "            print(\"\\n ----EDA----\")\n",
        "            print(\"1. 📂 Carga de datos\")\n",
        "            print(\"2. 🔍 Mostrar head del DataFrame\")\n",
        "            print(\"3. 📊 Revisar los tipos de datos\")\n",
        "            print(\"4. ✂️ Eliminar columnas\")\n",
        "            print(\"5. 🧹 Eliminar valores NULOS\")\n",
        "            print(\"6. ⚠️ Detectar valores atipicos\")\n",
        "            print(\"7. 📈 Graficar relación entre dos variables\")\n",
        "            print(\"8. 📉 Graficar histograma\")\n",
        "            print(\"9. 🌡 HeatMap: Generar mapa de calor\")\n",
        "            print(\"0. 🛑 Volver al menú principal\")\n",
        "            opcion = input(\"Seleccione una opción: \")\n",
        "\n",
        "            if opcion == \"1\":\n",
        "                my_data = input(\"¿Cómo se llama el CSV? \")\n",
        "                try:\n",
        "                    self.eda = EDA(file=my_data)\n",
        "                    print(\"Instancia de EDA creada y datos cargados exitosamente.\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error al cargar los datos: {e}\")\n",
        "            elif opcion == \"2\":\n",
        "                if self.eda:\n",
        "                    print(self.eda.head_df())\n",
        "                else:\n",
        "                    print(\"Primero cargue los datos.\")\n",
        "            elif opcion == \"3\":\n",
        "                if self.eda:\n",
        "                    print(self.eda.check_data_types())\n",
        "                else:\n",
        "                    print(\"Primero cargue los datos.\")\n",
        "            elif opcion == \"4\":\n",
        "                if self.eda:\n",
        "                    columnas = input(\"Ingrese los nombres de las columnas a eliminar, separadas por comas: \").split(',')\n",
        "                    columnas = [col.strip() for col in columnas]\n",
        "                    try:\n",
        "                        self.eda.drop_irrelevant_columns(columnas)\n",
        "                        print(f\"Columnas eliminadas: {', '.join(columnas)}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error al eliminar columnas: {e}\")\n",
        "                else:\n",
        "                    print(\"Primero cargue los datos.\")\n",
        "            elif opcion == \"5\":\n",
        "                if self.eda:\n",
        "                    self.eda.drop_missing_values()\n",
        "                    print(\"Valores nulos eliminados.\")\n",
        "                else:\n",
        "                    print(\"Primero cargue los datos.\")\n",
        "            elif opcion == \"6\":\n",
        "                if self.eda:\n",
        "                    print(self.eda.detect_outliers())\n",
        "                else:\n",
        "                    print(\"Primero cargue los datos.\")\n",
        "            elif opcion == \"7\":\n",
        "                if self.eda:\n",
        "                    print(\"\\n ***Variables disponibles***\")\n",
        "                    print(self.eda.check_data_types())\n",
        "                    col1 = input(\"Ingrese el nombre de la primera variable: \")\n",
        "                    col2 = input(\"Ingrese el nombre de la segunda variable: \")\n",
        "                    try:\n",
        "                        self.eda.plot_scatter(col1, col2)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error al graficar: {e}\")\n",
        "                        break\n",
        "                else:\n",
        "                    print(\"Primero cargue los datos.\")\n",
        "            elif opcion == \"8\":\n",
        "                if self.eda:\n",
        "                    print(\"\\n ***Variables disponibles***\")\n",
        "                    print(self.eda.check_data_types())\n",
        "                    histogram_col = input(\"Ingrese el nombre de una variable a graficar: \")\n",
        "                    try:\n",
        "                        self.eda.plot_histogram(histogram_col)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error al graficar: {e}\")\n",
        "                else:\n",
        "                    print(\"Primero cargue los datos.\")\n",
        "            elif opcion == \"9\":\n",
        "                if self.eda:\n",
        "                    self.eda.plot_heatmap()\n",
        "                    print(\"El programa se detendrá después de mostrar el gráfico.\")\n",
        "                    exit()\n",
        "                else:\n",
        "                    print(\"Primero cargue los datos.\")\n",
        "            elif opcion == \"0\":\n",
        "                break\n",
        "            else:\n",
        "                print(\"Opción no válida. Intente de nuevo.\")\n",
        "\n",
        "    def regre_modelos(self):\n",
        "      if self.supervisado is None:\n",
        "          if self.eda:\n",
        "              self.supervisado = Supervisado(self.eda)\n",
        "          else:\n",
        "              print(\"Primero debe cargar los datos\")\n",
        "              return\n",
        "\n",
        "      if self.split_data is None:\n",
        "          print(\"\\n ***Variables disponibles***\")\n",
        "          print(self.eda.check_data_types())\n",
        "          target_column = input(\"\\n Ingrese el nombre de la columna objetivo: \")\n",
        "          self.split_data = self.supervisado.split_df(target_column)\n",
        "\n",
        "      X_train, X_test, y_train, y_test = self.split_data\n",
        "\n",
        "      modelos = [\n",
        "          self.supervisado.regre_lineal_simple,\n",
        "          self.supervisado.regre_svm,\n",
        "          self.supervisado.regre_regridge,\n",
        "          self.supervisado.regre_decisionTree,\n",
        "          self.supervisado.regre_randomforest,\n",
        "          self.supervisado.regre_gradient_boosting,\n",
        "          self.supervisado.regre_xgboost\n",
        "      ]\n",
        "\n",
        "      resultados = []\n",
        "      for modelo in modelos:\n",
        "          resultados.append(modelo(X_train, X_test, y_train, y_test))\n",
        "\n",
        "      print(\"\\n--- Resultados del Benchmarking ---\")\n",
        "      for resultado in resultados:\n",
        "          print(f\"{resultado['modelo']}: R2={resultado['R2']:.4f}, RMSE={resultado['RMSE']:.4f}, MAE={resultado['MAE']:.4f}\")\n",
        "\n",
        "      # Opción de graficar resultados\n",
        "      graficar = input(\"\\n¿Desea graficar los resultados? (S/N): \").strip().upper()\n",
        "      if graficar == 'S':\n",
        "        # Preparar datos para la gráfica\n",
        "        nombres_modelos = [resultado['modelo'] for resultado in resultados]\n",
        "        r2_scores = [resultado['R2'] for resultado in resultados]\n",
        "        rmse_scores = [resultado['RMSE'] for resultado in resultados]\n",
        "        mae_scores = [resultado['MAE'] for resultado in resultados]\n",
        "\n",
        "        # Crear la gráfica de barras comparativa\n",
        "        plt.figure(figsize=(15, 6))\n",
        "\n",
        "        # Gráfica de R2\n",
        "        plt.subplot(1, 3, 1)\n",
        "        bars1 = plt.bar(nombres_modelos, r2_scores)\n",
        "        plt.title('R2 Scores')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.ylabel('R2')\n",
        "        # Añadir valores en las barras\n",
        "        for bar in bars1:\n",
        "            height = bar.get_height()\n",
        "            plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                     f'{height:.4f}',\n",
        "                     ha='center', va='bottom', rotation=0)\n",
        "\n",
        "        # Gráfica de RMSE\n",
        "        plt.subplot(1, 3, 2)\n",
        "        bars2 = plt.bar(nombres_modelos, rmse_scores)\n",
        "        plt.title('Root Mean Squared Error')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.ylabel('RMSE')\n",
        "        # Añadir valores en las barras\n",
        "        for bar in bars2:\n",
        "            height = bar.get_height()\n",
        "            plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                     f'{height:.4f}',\n",
        "                     ha='center', va='bottom', rotation=0)\n",
        "\n",
        "        # Gráfica de MAE\n",
        "        plt.subplot(1, 3, 3)\n",
        "        bars3 = plt.bar(nombres_modelos, mae_scores)\n",
        "        plt.title('Mean Absolute Error')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.ylabel('MAE')\n",
        "        # Añadir valores en las barras\n",
        "        for bar in bars3:\n",
        "            height = bar.get_height()\n",
        "            plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                     f'{height:.4f}',\n",
        "                     ha='center', va='bottom', rotation=0)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        exit()\n",
        "\n",
        "    def classi_modelos(self):\n",
        "        if graficar == 'S':\n",
        "          # Preparar datos para la gráfica\n",
        "          nombres_modelos = [resultado['modelo'] for resultado in resultados]\n",
        "          accuracy_scores = [resultado['accuracy'] for resultado in resultados]\n",
        "          precision_scores = [resultado['precision'] for resultado in resultados]\n",
        "          recall_scores = [resultado['recall'] for resultado in resultados]\n",
        "          f1_scores = [resultado['f1_score'] for resultado in resultados]\n",
        "\n",
        "          # Crear la gráfica de barras comparativa\n",
        "          plt.figure(figsize=(15, 6))\n",
        "\n",
        "          # Gráfica de Accuracy\n",
        "          plt.subplot(1, 4, 1)\n",
        "          bars1 = plt.bar(nombres_modelos, accuracy_scores)\n",
        "          plt.title('Accuracy')\n",
        "          plt.xticks(rotation=45, ha='right')\n",
        "          plt.ylabel('Accuracy')\n",
        "          # Añadir valores en las barras\n",
        "          for bar in bars1:\n",
        "              height = bar.get_height()\n",
        "              plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                      f'{height:.4f}',\n",
        "                      ha='center', va='bottom', rotation=0)\n",
        "\n",
        "          # Gráfica de Precision\n",
        "          plt.subplot(1, 4, 2)\n",
        "          bars2 = plt.bar(nombres_modelos, precision_scores)\n",
        "          plt.title('Precision')\n",
        "          plt.xticks(rotation=45, ha='right')\n",
        "          plt.ylabel('Precision')\n",
        "          # Añadir valores en las barras\n",
        "          for bar in bars2:\n",
        "              height = bar.get_height()\n",
        "              plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                      f'{height:.4f}',\n",
        "                      ha='center', va='bottom', rotation=0)\n",
        "\n",
        "          # Gráfica de Recall\n",
        "          plt.subplot(1, 4, 3)\n",
        "          bars3 = plt.bar(nombres_modelos, recall_scores)\n",
        "          plt.title('Recall')\n",
        "          plt.xticks(rotation=45, ha='right')\n",
        "          plt.ylabel('Recall')\n",
        "          # Añadir valores en las barras\n",
        "          for bar in bars3:\n",
        "              height = bar.get_height()\n",
        "              plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                      f'{height:.4f}',\n",
        "                      ha='center', va='bottom', rotation=0)\n",
        "\n",
        "          # Gráfica de F1-Score\n",
        "          plt.subplot(1, 4, 4)\n",
        "          bars4 = plt.bar(nombres_modelos, f1_scores)\n",
        "          plt.title('F1-Score')\n",
        "          plt.xticks(rotation=45, ha='right')\n",
        "          plt.ylabel('F1-Score')\n",
        "          # Añadir valores en las barras\n",
        "          for bar in bars4:\n",
        "              height = bar.get_height()\n",
        "              plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                      f'{height:.4f}',\n",
        "                      ha='center', va='bottom', rotation=0)\n",
        "\n",
        "          plt.tight_layout()\n",
        "          plt.show()\n",
        "          exit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWprvJMPYo_D"
      },
      "source": [
        "Ejecucion del programa con estructura pythonica"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pkb3yN8QYtEw",
        "outputId": "42c5f0ac-d091-4376-bbe5-a985977f1af8"
      },
      "outputs": [],
      "source": [
        "# Ejecución del menú principal\n",
        "if __name__ == \"__main__\":\n",
        "    start = Start()\n",
        "    start.mostrar_menu()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvnwMBH3YgSh"
      },
      "source": [
        "# Codigo para resolver preguntas de investigación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQhPeW50v10X"
      },
      "outputs": [],
      "source": [
        "class ComparacionModelos:\n",
        "    def __init__(self, df):\n",
        "        \"\"\"\n",
        "        Inicializa la clase ComparacionModelos con un DataFrame.\n",
        "\n",
        "        Parámetros:\n",
        "            df (DataFrame): DataFrame que contiene los datos originales.\n",
        "        \"\"\"\n",
        "        self.df = df\n",
        "        self.model = LinearRegression()\n",
        "\n",
        "    def aplicar_pca(self, n_componentes=2):\n",
        "        \"\"\"\n",
        "        Aplica PCA al DataFrame y devuelve el conjunto de datos transformado.\n",
        "\n",
        "        Parámetros:\n",
        "            n_componentes (int): Número de componentes principales a conservar.\n",
        "\n",
        "        Retorna:\n",
        "            DataFrame: Conjunto de datos transformado por PCA.\n",
        "        \"\"\"\n",
        "        pca = PCA(n_components=n_componentes)\n",
        "        resultado_pca = pca.fit_transform(self.df.select_dtypes(include=[np.number]))\n",
        "        return pd.DataFrame(data=resultado_pca, columns=[f'PC{i+1}' for i in range(n_componentes)])\n",
        "\n",
        "    def aplicar_umap(self, n_componentes=2):\n",
        "        \"\"\"\n",
        "        Aplica UMAP al DataFrame y devuelve el conjunto de datos transformado.\n",
        "\n",
        "        Parámetros:\n",
        "            n_componentes (int): Número de componentes a conservar.\n",
        "\n",
        "        Retorna:\n",
        "            DataFrame: Conjunto de datos transformado por UMAP.\n",
        "        \"\"\"\n",
        "        modelo_umap = UMAP(n_components=n_componentes)\n",
        "        resultado_umap = modelo_umap.fit_transform(self.df.select_dtypes(include=[np.number]))\n",
        "        return pd.DataFrame(data=resultado_umap, columns=[f'UMAP{i+1}' for i in range(n_componentes)])\n",
        "\n",
        "    def evaluar_modelo(self, X, y):\n",
        "        \"\"\"\n",
        "        Evalúa el modelo de regresión lineal y devuelve las métricas de rendimiento.\n",
        "\n",
        "        Parámetros:\n",
        "            X (DataFrame): Conjunto de características.\n",
        "            y (Series): Variable objetivo.\n",
        "\n",
        "        Retorna:\n",
        "            dict: Métricas de rendimiento del modelo.\n",
        "        \"\"\"\n",
        "        X_entrenamiento, X_prueba, y_entrenamiento, y_prueba = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "        self.model.fit(X_entrenamiento, y_entrenamiento)\n",
        "        predicciones = self.model.predict(X_prueba)\n",
        "\n",
        "        return {\n",
        "            'MSE': mean_squared_error(y_prueba, predicciones),\n",
        "            'R2': r2_score(y_prueba, predicciones),\n",
        "            'MAE': mean_absolute_error(y_prueba, predicciones)\n",
        "        }\n",
        "\n",
        "    def importancia_caracteristicas(self):\n",
        "        \"\"\"\n",
        "        Devuelve la importancia de las características del modelo de regresión lineal.\n",
        "\n",
        "        Retorna:\n",
        "            Series: Importancia de las características.\n",
        "        \"\"\"\n",
        "        return pd.Series(self.model.coef_, index=self.df.columns).sort_values(ascending=False)\n",
        "\n",
        "    def comparar_modelos(self, columna_objetivo):\n",
        "        \"\"\"\n",
        "        Compara el rendimiento del modelo en el conjunto de datos original, PCA y UMAP.\n",
        "\n",
        "        Parámetros:\n",
        "            columna_objetivo (str): Nombre de la columna objetivo.\n",
        "\n",
        "        Retorna:\n",
        "            dict: Resultados de la comparación de modelos.\n",
        "        \"\"\"\n",
        "        y = self.df[columna_objetivo]\n",
        "        metrics_original = self.evaluar_modelo(self.df.drop(columns=[columna_objetivo]), y)\n",
        "\n",
        "        datos_pca = self.aplicar_pca()\n",
        "        metrics_pca = self.evaluar_modelo(datos_pca, y)\n",
        "\n",
        "        datos_umap = self.aplicar_umap()\n",
        "        metrics_umap = self.evaluar_modelo(datos_umap, y)\n",
        "\n",
        "        return {\n",
        "            'Original': metrics_original,\n",
        "            'PCA': metrics_pca,\n",
        "            'UMAP': metrics_umap,\n",
        "            'Importancia de Características': self.importancia_caracteristicas()\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpLQ4NWXyg1r",
        "outputId": "62332d72-22a0-4459-e048-b2f012f9321a"
      },
      "outputs": [],
      "source": [
        "# Verificar la longitud del DataFrame y de la columna objetivo\n",
        "print(\"Número de filas en el DataFrame:\", len(datos))\n",
        "print(\"Número de filas en la columna objetivo:\", len(datos['Price']))\n",
        "print(datos.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8C663tOp2c5s"
      },
      "outputs": [],
      "source": [
        "class ModelComparison:\n",
        "    def __init__(self, df, target_column):\n",
        "        self.df = df\n",
        "        self.target_column = target_column\n",
        "        self.X = self.df.drop(columns=[self.target_column])\n",
        "        self.y = self.df[self.target_column]\n",
        "        self.model = LinearRegression()\n",
        "\n",
        "    def apply_pca(self, n_components=2):\n",
        "\n",
        "        pca = PCA(n_components=n_components)\n",
        "        pca_result = pca.fit_transform(self.X)\n",
        "        return pd.DataFrame(data=pca_result, columns=[f'PC{i+1}' for i in range(n_components)])\n",
        "\n",
        "    def apply_umap(self, n_components=2):\n",
        "\n",
        "        umap_model = UMAP(n_components=n_components)\n",
        "        umap_result = umap_model.fit_transform(self.X)\n",
        "        return pd.DataFrame(data=umap_result, columns=[f'UMAP{i+1}' for i in range(n_components)])\n",
        "\n",
        "    def evaluate_model(self, X):\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, self.y, test_size=0.2, random_state=42)\n",
        "        self.model.fit(X_train, y_train)\n",
        "        predictions = self.model.predict(X_test)\n",
        "\n",
        "        return {\n",
        "            'MSE': mean_squared_error(y_test, predictions),\n",
        "            'R2': r2_score(y_test, predictions),\n",
        "            'MAE': mean_absolute_error(y_test, predictions)\n",
        "        }\n",
        "\n",
        "    def feature_importance(self):\n",
        "\n",
        "        return pd.Series(self.model.coef_, index=self.X.columns).sort_values(ascending=False)\n",
        "\n",
        "    def compare_models(self):\n",
        "\n",
        "        original_metrics = self.evaluate_model(self.X)\n",
        "\n",
        "        pca_data = self.apply_pca()\n",
        "        pca_metrics = self.evaluate_model(pca_data)\n",
        "\n",
        "        umap_data = self.apply_umap()\n",
        "        umap_metrics = self.evaluate_model(umap_data)\n",
        "\n",
        "        # Entrenar el modelo en el conjunto de datos original para obtener los coeficientes\n",
        "        self.model.fit(self.X, self.y)\n",
        "        importance = self.feature_importance()\n",
        "\n",
        "        return {\n",
        "            'Original': original_metrics,\n",
        "            'PCA': pca_metrics,\n",
        "            'UMAP': umap_metrics,\n",
        "            'Feature Importance': importance\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlppFSXg0tWG",
        "outputId": "6efe82d1-a308-4dbc-ed66-938db2b1ccd9"
      },
      "outputs": [],
      "source": [
        "X = datos.drop(columns=['Price'])\n",
        "y = datos['Price']\n",
        "\n",
        "# Aplicar PCA\n",
        "pca = PCA(n_components=2)\n",
        "pca_result = pca.fit_transform(X)\n",
        "\n",
        "# Crear un DataFrame con los resultados de PCA\n",
        "pca_df = pd.DataFrame(data=pca_result, columns=['PC1', 'PC2'])\n",
        "\n",
        "# Aplicar UMAP\n",
        "umap_model = UMAP(n_components=2)\n",
        "umap_result = umap_model.fit_transform(X)\n",
        "\n",
        "# Crear un DataFrame con los resultados de UMAP\n",
        "umap_df = pd.DataFrame(data=umap_result, columns=['UMAP1', 'UMAP2'])\n",
        "\n",
        "# Función para evaluar el modelo\n",
        "def evaluate_model(X, y):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "    predictions = model.predict(X_test)\n",
        "\n",
        "    return {\n",
        "        'MSE': mean_squared_error(y_test, predictions),\n",
        "        'R2': r2_score(y_test, predictions),\n",
        "        'MAE': mean_absolute_error(y_test, predictions)\n",
        "    }\n",
        "\n",
        "# Evaluar el modelo en el conjunto de datos original\n",
        "original_metrics = evaluate_model(X, y)\n",
        "\n",
        "# Evaluar el modelo en el conjunto de datos transformado por PCA\n",
        "pca_metrics = evaluate_model(pca_df, y)\n",
        "\n",
        "# Evaluar el modelo en el conjunto de datos transformado por UMAP\n",
        "umap_metrics = evaluate_model(umap_df, y)\n",
        "\n",
        "# Imprimir las métricas\n",
        "print(\"Métricas del conjunto de datos original:\", original_metrics)\n",
        "print(\"Métricas de PCA:\", pca_metrics)\n",
        "print(\"Métricas de UMAP:\", umap_metrics)\n",
        "\n",
        "# Función para obtener la importancia de las características\n",
        "def feature_importance(X, model):\n",
        "    return pd.Series(model.coef_, index=X.columns).sort_values(ascending=False)\n",
        "\n",
        "# Entrenar el modelo en el conjunto de datos original para obtener los coeficientes\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Obtener la importancia de las características\n",
        "importance = feature_importance(X, model)\n",
        "\n",
        "# Imprimir la importancia de las características\n",
        "print(\"Importancia de las características:\")\n",
        "print(importance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lux-ld6y13t1",
        "outputId": "7966b6f9-0043-43a2-9cec-b4e510bc55df"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Métricas\n",
        "modelos = ['Original', 'PCA', 'UMAP']\n",
        "mse = [original_metrics['MSE'], pca_metrics['MSE'], umap_metrics['MSE']]\n",
        "r2 = [original_metrics['R2'], pca_metrics['R2'], umap_metrics['R2']]\n",
        "mae = [original_metrics['MAE'], pca_metrics['MAE'], umap_metrics['MAE']]\n",
        "\n",
        "# Crear subgráficos para comparar el rendimiento del modelo\n",
        "fig, ax = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Gráfico de MSE\n",
        "ax[0].bar(modelos, mse, color='blue')\n",
        "ax[0].set_title('MSE por Modelo')\n",
        "ax[0].set_ylabel('MSE')\n",
        "ax[0].set_ylim(0, max(mse) + 50)  # Ajustar el límite superior para mejor visualización\n",
        "\n",
        "# Añadir etiquetas de valor a las barras\n",
        "for i, v in enumerate(mse):\n",
        "    ax[0].text(i, v + 2, f\"{v:.2f}\", ha='center', color='black')\n",
        "\n",
        "# Gráfico de R²\n",
        "ax[1].bar(modelos, r2, color='green')\n",
        "ax[1].set_title('R² por Modelo')\n",
        "ax[1].set_ylabel('R²')\n",
        "ax[1].set_ylim(min(r2) - 0.1, 1)  # Ajustar el límite inferior para mejor visualización\n",
        "\n",
        "# Añadir etiquetas de valor a las barras\n",
        "for i, v in enumerate(r2):\n",
        "    ax[1].text(i, v + 0.02, f\"{v:.2f}\", ha='center', color='black')\n",
        "\n",
        "# Gráfico de MAE\n",
        "ax[2].bar(modelos, mae, color='orange')\n",
        "ax[2].set_title('MAE por Modelo')\n",
        "ax[2].set_ylabel('MAE')\n",
        "ax[2].set_ylim(0, max(mae) + 5)  # Ajustar el límite superior para mejor visualización\n",
        "\n",
        "# Añadir etiquetas de valor a las barras\n",
        "for i, v in enumerate(mae):\n",
        "    ax[2].text(i, v + 0.5, f\"{v:.2f}\", ha='center', color='black')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Gráfico de la importancia de las características\n",
        "plt.figure(figsize=(10, 6))\n",
        "importance.sort_values(ascending=True).plot(kind='barh', color='skyblue')\n",
        "plt.title('Importancia de las Características en el Modelo Original')\n",
        "plt.xlabel('Coeficiente')\n",
        "plt.ylabel('Características')\n",
        "plt.axvline(0, color='red', linestyle='--')  # Línea vertical en 0 para referencia\n",
        "\n",
        "# Añadir etiquetas de valor a las barras de importancia\n",
        "for index, value in enumerate(importance.sort_values(ascending=True)):\n",
        "    plt.text(value, index, f\"{value:.2f}\", va='center', ha='left', color='black')\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
