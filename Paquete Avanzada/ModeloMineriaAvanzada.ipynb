{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fGfAaC9dUfO"
      },
      "source": [
        "**LEAD University - Minería de datos**\n",
        "\n",
        "Python Project\n",
        "\n",
        "**Contributors**\n",
        "- Carolina Salas Moreno\n",
        "- Deykel Bernard Salazar\n",
        "- Esteban Ramirez Montano\n",
        "- Kristhel Porras Mata\n",
        "- Marla Gomez Hernández\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pxx9r1xc-1c"
      },
      "source": [
        "## Requirements\n",
        "**Step 1:** Please install Microsoft C++ Build Tools in your machine.\n",
        "\n",
        "**Step 2:** Install Python 3.11.7\n",
        "\n",
        "**Step 3:** Run the following code if this is your first time running it `pip install -r requirements.txt`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZXUiJAacFRT"
      },
      "source": [
        "# Importar las librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "dQw57BLb3dFh"
      },
      "outputs": [],
      "source": [
        "# Main Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import umap.umap_ as umap\n",
        "\n",
        "# Data Optimization\n",
        "from sklearn_genetic import GASearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import SelectFromModel \n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn_genetic.space import Integer, Categorical, Continuous\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "#Feature Selection\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LassoCV \n",
        "\n",
        "# Clustering Libraries\n",
        "from abc import ABCMeta, abstractmethod\n",
        "from scipy.cluster.hierarchy import dendrogram, ward, single, complete, average, linkage, fcluster\n",
        "from sklearn.cluster import KMeans\n",
        "from pyclustering.cluster.kmedoids import kmedoids\n",
        "\n",
        "# Dimensionality Reduction\n",
        "from prince import PCA as PCA_Prince\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Evaluation Metrics\n",
        "from sklearn.metrics import silhouette_score, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "# Regression Models\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "\n",
        "# Classification Models\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, RandomForestRegressor\n",
        "\n",
        "# Additional Tools\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScX-De7IdB7L"
      },
      "source": [
        "# EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "HIssRCV8cKBa"
      },
      "outputs": [],
      "source": [
        "# Análisis Exploratorio de Datos (EDA)\n",
        "class EDA:\n",
        "    def __init__(self, file=None):\n",
        "        \"\"\"\n",
        "        Inicializa la clase EDA y carga datos desde un archivo CSV si se proporciona.\n",
        "\n",
        "        Parámetros:\n",
        "            file (str): Ruta al archivo CSV. Si no se proporciona, se inicializa un DataFrame vacío.\n",
        "        \"\"\"\n",
        "        self.__df = pd.read_csv(file) if file else pd.DataFrame()\n",
        "\n",
        "    def head_df(self, n=5):\n",
        "        return self.__df.head(n) if not self.__df.empty else \"No se cargaron los datos :(\"\n",
        "\n",
        "    def tail_df(self, n=5):\n",
        "        return self.__df.tail(n) if not self.__df.empty else \"No se cargaron los datos :(\"\n",
        "\n",
        "    def check_data_types(self):\n",
        "        return self.__df.dtypes\n",
        "\n",
        "    def drop_irrelevant_columns(self, columns):\n",
        "        self.__df.drop(columns=columns, inplace=True)\n",
        "\n",
        "    def drop_missing_values(self):\n",
        "        self.__df.dropna(inplace=True)\n",
        "\n",
        "    def detect_outliers(self):\n",
        "        num_df = self.__df.select_dtypes(include=['float64', 'int64'])\n",
        "        if num_df.empty:\n",
        "            return \"No hay columnas numéricas en el DataFrame.\"\n",
        "\n",
        "        Q1 = num_df.quantile(0.25)\n",
        "        Q3 = num_df.quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        outliers = ((num_df < (Q1 - 1.5 * IQR)) | (num_df > (Q3 + 1.5 * IQR))).sum()\n",
        "        Dicc_outliers = {col: outliers[col] for col in num_df.columns if outliers[col] > 0}\n",
        "\n",
        "        return Dicc_outliers if Dicc_outliers else \"No se detectaron valores atípicos en las columnas numéricas.\"\n",
        "\n",
        "    def plot_scatter(self, col1, col2):\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.scatterplot(x=self.__df[col1], y=self.__df[col2])\n",
        "        plt.title(f'Gráfico de Dispersión: {col1} vs {col2}')\n",
        "        plt.xlabel(col1)\n",
        "        plt.ylabel(col2)\n",
        "        plt.grid()\n",
        "        plt.show()\n",
        "\n",
        "    def plot_histogram(self, col):\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.histplot(self.__df[col], kde=True)\n",
        "        plt.title(f'Histograma de {col}')\n",
        "        plt.xlabel(col)\n",
        "        plt.ylabel('Frecuencia')\n",
        "        plt.show()\n",
        "\n",
        "    def plot_heatmap(self):\n",
        "        num_df = self.__df.select_dtypes(include=['float64', 'int64'])\n",
        "        if num_df.empty:\n",
        "            return \"No hay columnas numéricas para generar el mapa de calor.\"\n",
        "\n",
        "        num_df = num_df.loc[:, num_df.apply(lambda x: np.std(x) > 0.01)]\n",
        "\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        sns.heatmap(num_df.corr(), cmap=\"coolwarm\", annot= True, linewidths=0.5, cbar=True) #annot=False es para que no se vean los numeros en los cuadros\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.yticks(rotation=0)\n",
        "        plt.title(\"Correlation heatmap\", fontsize=18)\n",
        "        plt.ion()\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"Clase EDA - DataFrame de la forma: {self.__df.shape}\"\n",
        "\n",
        "    def get_df(self):\n",
        "        \"\"\"Devuelve una copia del df para que las familias de los algoritmos las utilicen\"\"\"\n",
        "        return self.__df.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DataOptimization(EDA):\n",
        "    def __init__(self, datos_eda):\n",
        "        \"\"\"\n",
        "        Use the processed DataFrame from EDA to optimize models.\n",
        "\n",
        "        Parameters:\n",
        "        - datos_eda: This is the processed DataFrame from the EDA class.\n",
        "        \"\"\"\n",
        "        self.__df = datos_eda.get_df()\n",
        "        \n",
        "        # Data components\n",
        "        self.X_train = None\n",
        "        self.X_test = None\n",
        "        self.y_train = None\n",
        "        self.y_test = None\n",
        "        \n",
        "        # Regression models\n",
        "        self.regression_models = {\n",
        "            'LinearRegression': LinearRegression(),\n",
        "            'SVM': SVR(),\n",
        "            'Ridge': Ridge(),\n",
        "            'DecisionTreeRegressor': DecisionTreeRegressor(),\n",
        "            'RandomForestRegressor': RandomForestRegressor(),\n",
        "            'GradientBoostingRegressor': GradientBoostingRegressor(),\n",
        "            'XGBRegressor': XGBRegressor(random_state=42)\n",
        "        }\n",
        "\n",
        "        # Classification models\n",
        "        self.classification_models = {\n",
        "            'DecisionTreeClassifier': DecisionTreeClassifier(),\n",
        "            'KNeighborsClassifier': KNeighborsClassifier(),\n",
        "            'RandomForestClassifier': RandomForestClassifier(),\n",
        "            'AdaBoostClassifier': AdaBoostClassifier(random_state=42)\n",
        "        }\n",
        "        \n",
        "        # Current active models based on problem type\n",
        "        self.models = None\n",
        "        \n",
        "        # Parameter grids\n",
        "        self.param_grids_genetic = None\n",
        "        self.param_grids_exhaustive = None\n",
        "\n",
        "#------------------------Data Split Components--------------------------------------------------------------\n",
        "\n",
        "    def split_df(self, target_column, test_size=None, random_state=42):\n",
        "        \"\"\"\n",
        "        Splits the dataframe into training and test sets.\n",
        "\n",
        "        Parameters:\n",
        "        - target_column: str -> Name of the target column (y).\n",
        "        - test_size: float -> Proportion of the test set (if not provided, it is calculated from the entered percentage).\n",
        "        - random_state: int -> Seed for randomization.\n",
        "\n",
        "        Returns:\n",
        "        - X_train, X_test, y_train, y_test: Split and preprocessed datasets.\n",
        "        \"\"\"\n",
        "        while True:\n",
        "            try:\n",
        "                percent = float(input(\"Enter the percentage for the training set: (Example: 80) \\n\"))\n",
        "                if 0 < percent < 100:\n",
        "                    train_size = percent / 100\n",
        "                    break # Exit the loop\n",
        "                else:\n",
        "                    print(\"The percentage must be between 1 and 99.\")\n",
        "            except ValueError:\n",
        "                print(\"Invalid number. Try again.\")\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                # Separate features (X) and target variable (y)\n",
        "                X = self.__df.drop(columns=[target_column])\n",
        "                y = self.__df[target_column]\n",
        "                break  # Exit the loop if there are no errors\n",
        "            except KeyError:\n",
        "                print(f\"The column '{target_column}' does not exist. Try again.\")\n",
        "                print(\"Available columns:\")\n",
        "                print(self.check_data_types())\n",
        "                target_column = input(\"Enter the correct name of the target column: \")\n",
        "\n",
        "        # Preprocess features (X), convert categorical variables to One-Hot Encoding\n",
        "        X = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "        # Check if the target variable (y) is categorical and needs encoding\n",
        "        if y.dtypes == 'object' or y.dtypes.name == 'category':\n",
        "            from sklearn.preprocessing import LabelEncoder\n",
        "            le = LabelEncoder()\n",
        "            y = le.fit_transform(y)\n",
        "\n",
        "        # Perform the split\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size= 1 - train_size, random_state=random_state\n",
        "        )\n",
        "\n",
        "        print(f\"Data split:\\n- Training: {X_train.shape[0]} rows\\n- Test: {X_test.shape[0]} rows\")\n",
        "        return X_train, X_test, y_train, y_test\n",
        "    \n",
        "#------------------------Parameter Grid for Regression--------------------------------------------------------------\n",
        "\n",
        "    def _get_param_grids_regression_genetic(self):\n",
        "        return {\n",
        "            'LinearRegression': {\n",
        "                \"clf__copy_X\": Categorical([True, False]),\n",
        "                \"clf__fit_intercept\": Categorical([True, False]),\n",
        "                \"clf__positive\": Categorical([True, False])\n",
        "            },\n",
        "            'SVM': {\n",
        "                'clf__C': Continuous(0.1, 10.0),\n",
        "                'clf__kernel': Categorical(['linear', 'poly', 'rbf']),\n",
        "                'clf__gamma': Categorical(['scale', 'auto'])\n",
        "            },\n",
        "            'Ridge': {\n",
        "                'clf__alpha': Continuous(0.1, 10.0),\n",
        "                'clf__fit_intercept': Categorical([True, False]),\n",
        "                'clf__solver': Categorical(['auto', 'svd', 'cholesky'])\n",
        "            },\n",
        "            'DecisionTreeRegressor': {\n",
        "                'clf__max_depth': Integer(3, 10),\n",
        "                'clf__min_samples_split': Integer(2, 10),\n",
        "                'clf__min_samples_leaf': Integer(1, 5)\n",
        "            },\n",
        "            'RandomForestRegressor': {\n",
        "                'clf__n_estimators': Integer(50, 200),\n",
        "                'clf__max_depth': Integer(5, 15),\n",
        "                'clf__min_samples_split': Integer(2, 10)\n",
        "            },\n",
        "            'GradientBoostingRegressor': {\n",
        "                'clf__n_estimators': Integer(50, 200),\n",
        "                'clf__learning_rate': Continuous(0.01, 0.2),\n",
        "                'clf__max_depth': Integer(3, 10)\n",
        "            },\n",
        "            'XGBRegressor': {\n",
        "                'clf__learning_rate': Continuous(0.01, 0.2),\n",
        "                'clf__n_estimators': Integer(50, 200),\n",
        "                'clf__max_depth': Integer(3, 10),\n",
        "                'clf__subsample': Continuous(0.7, 1.0)\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def _get_param_grids_regression_exhaustive(self):\n",
        "        return {\n",
        "            'LinearRegression': {\n",
        "                \"clf__copy_X\": [True, False],\n",
        "                \"clf__fit_intercept\": [True, False],\n",
        "                \"clf__positive\": [True, False]\n",
        "            },\n",
        "            'SVM': {\n",
        "                'clf__C': [0.1, 1, 10],\n",
        "                'clf__kernel': ['linear', 'poly', 'rbf'],\n",
        "                'clf__gamma': ['scale', 'auto']\n",
        "            },\n",
        "            'Ridge': {\n",
        "                'clf__alpha': [0.1, 1.0, 10.0],\n",
        "                'clf__fit_intercept': [True, False],\n",
        "                'clf__solver': ['auto', 'svd', 'cholesky']\n",
        "            },\n",
        "            'DecisionTreeRegressor': {\n",
        "                'clf__max_depth': [3, 5, 7, 10],\n",
        "                'clf__min_samples_split': [2, 5, 10],\n",
        "                'clf__min_samples_leaf': [1, 2, 5]\n",
        "            },\n",
        "            'RandomForestRegressor': {\n",
        "                'clf__n_estimators': [50, 100, 200],\n",
        "                'clf__max_depth': [5, 10, 15],\n",
        "                'clf__min_samples_split': [2, 5, 10]\n",
        "            },\n",
        "            'GradientBoostingRegressor': {\n",
        "                'clf__n_estimators': [50, 100, 200],\n",
        "                'clf__learning_rate': [0.01, 0.1, 0.2],\n",
        "                'clf__max_depth': [3, 5, 10]\n",
        "            },\n",
        "            'XGBRegressor': {\n",
        "                'clf__learning_rate': [0.01, 0.1, 0.2],\n",
        "                'clf__n_estimators': [50, 100, 200],\n",
        "                'clf__max_depth': [3, 5, 10],\n",
        "                'clf__subsample': [0.7, 0.8, 1.0]\n",
        "            }\n",
        "        }\n",
        "\n",
        "#------------------------Parameter Grid for Classification--------------------------------------------------------------\n",
        "\n",
        "    def _get_param_grids_classification_genetic(self):\n",
        "        return {\n",
        "            'DecisionTreeClassifier': {\n",
        "                'clf__max_depth': Integer(3, 10),\n",
        "                'clf__min_samples_split': Integer(2, 10),\n",
        "                'clf__criterion': Categorical(['gini', 'entropy'])\n",
        "            },\n",
        "            'KNeighborsClassifier': {\n",
        "                'clf__n_neighbors': Integer(3, 15),\n",
        "                'clf__weights': Categorical(['uniform', 'distance']),\n",
        "                'clf__algorithm': Categorical(['auto', 'ball_tree', 'kd_tree'])\n",
        "            },\n",
        "            'RandomForestClassifier': {\n",
        "                'clf__n_estimators': Integer(50, 200),\n",
        "                'clf__max_depth': Integer(5, 15),\n",
        "                'clf__min_samples_split': Integer(2, 10)\n",
        "            },\n",
        "            'AdaBoostClassifier': {\n",
        "                'clf__n_estimators': Integer(50, 200),\n",
        "                'clf__learning_rate': Continuous(0.01, 0.2),\n",
        "                'clf__algorithm': Categorical(['SAMME'])\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def _get_param_grids_classification_exhaustive(self):\n",
        "        return {\n",
        "            'DecisionTreeClassifier': {\n",
        "                'clf__max_depth': [3, 5, 7, 10],\n",
        "                'clf__min_samples_split': [2, 5, 10],\n",
        "                'clf__criterion': ['gini', 'entropy']\n",
        "            },\n",
        "            'KNeighborsClassifier': {\n",
        "                'clf__n_neighbors': [3, 5, 7, 10, 15],\n",
        "                'clf__weights': ['uniform', 'distance'],\n",
        "                'clf__algorithm': ['auto', 'ball_tree', 'kd_tree']\n",
        "            },\n",
        "            'RandomForestClassifier': {\n",
        "                'clf__n_estimators': [50, 100, 200],\n",
        "                'clf__max_depth': [5, 10, 15],\n",
        "                'clf__min_samples_split': [2, 5, 10]\n",
        "            },\n",
        "            'AdaBoostClassifier': {\n",
        "                'clf__n_estimators': [50, 100, 200],\n",
        "                'clf__learning_rate': [0.01, 0.1, 0.2],\n",
        "                'clf__algorithm': ['SAMME']\n",
        "            }\n",
        "        }\n",
        "\n",
        "#------------------------Search Components--------------------------------------------------------------\n",
        "\n",
        "    def genetic_search(self, scoring_metric):\n",
        "        \"\"\"\n",
        "        Optimize models using genetic algorithms.\n",
        "        \n",
        "        Parameters:\n",
        "        - scoring_metric: str -> Metric to use for evaluation: neg_root_mean_squared_error for regression, roc_auc for classification.\n",
        "        \"\"\"\n",
        "        if self.X_train is None or self.X_test is None:\n",
        "            print(\"\\n Error: You must run split_df() before calling genetic_search()\")\n",
        "            return\n",
        "\n",
        "        results = {}\n",
        "        \n",
        "        # Feature selection using LassoCV for both regression and classification\n",
        "        print(\"Performing feature selection with LassoCV...\")\n",
        "\n",
        "        lasso_cv = LassoCV(cv=5, random_state=42)\n",
        "        lasso_cv.fit(self.X_train, self.y_train)\n",
        "        f_selection = SelectFromModel(lasso_cv, threshold=\"mean\")\n",
        "\n",
        "        self.X_train = f_selection.transform(self.X_train)\n",
        "        self.X_test = f_selection.transform(self.X_test)\n",
        "\n",
        "        for name, model in self.models.items():\n",
        "            pl = Pipeline([ \n",
        "              ('clf', model), \n",
        "            ])            \n",
        "            print(f\"Training {name} with genetic method...\")\n",
        "            evolved_estimator = GASearchCV(\n",
        "                estimator=pl,\n",
        "                cv=5,\n",
        "                scoring=scoring_metric,\n",
        "                population_size=10,\n",
        "                generations=5,\n",
        "                tournament_size=3,\n",
        "                elitism=True,\n",
        "                crossover_probability=0.8,\n",
        "                mutation_probability=0.1,\n",
        "                param_grid=self.param_grids_genetic[name],\n",
        "                algorithm=\"eaSimple\",\n",
        "                n_jobs=-1,\n",
        "                error_score='raise',\n",
        "                verbose=True\n",
        "            )\n",
        "            evolved_estimator.fit(self.X_train, self.y_train)\n",
        "            results[name] = {\n",
        "                'best_params': evolved_estimator.best_params_,\n",
        "                'estimator': evolved_estimator.best_estimator_\n",
        "            }\n",
        "        return results\n",
        "\n",
        "    def exhaustive_search(self, scoring_metric):\n",
        "        \"\"\"\n",
        "        Perform exhaustive grid search for hyperparameter optimization.\n",
        "        \n",
        "        Parameters:\n",
        "        - scoring_metric: str -> Metric to use for evaluation ('neg_root_mean_squared_error' for regression, 'roc_auc' for classification)\n",
        "        \"\"\"\n",
        "        results = {}\n",
        "        \n",
        "        # Feature selection using LassoCV for both regression and classification\n",
        "        print(\"Performing feature selection with LassoCV...\")\n",
        "\n",
        "        lasso_cv = LassoCV(cv=5, random_state=42)\n",
        "        lasso_cv.fit(self.X_train, self.y_train)\n",
        "        f_selection = SelectFromModel(lasso_cv) #, threshold=\"mean\"\n",
        "            \n",
        "        self.X_train = f_selection.transform(self.X_train)\n",
        "        self.X_test = f_selection.transform(self.X_test)\n",
        "        \n",
        "        for name, model in self.models.items():\n",
        "            pl = Pipeline([\n",
        "              ('clf', model), \n",
        "            ])\n",
        "            print(f\"Training {name} with exhaustive method...\")\n",
        "            grid_search = GridSearchCV(\n",
        "                estimator=pl,\n",
        "                param_grid=self.param_grids_exhaustive[name],\n",
        "                cv=5,\n",
        "                scoring=scoring_metric,\n",
        "                n_jobs=-1,\n",
        "                verbose=1\n",
        "            )\n",
        "            grid_search.fit(self.X_train, self.y_train)\n",
        "            results[name] = {\n",
        "                'best_params': grid_search.best_params_,\n",
        "                'estimator': grid_search.best_estimator_\n",
        "            }\n",
        "        return results\n",
        "    \n",
        "#------------------------Director Function--------------------------------------------------------------\n",
        "    def opti_director(self, target_column, problem_type='regression', method='both', random_state=42):\n",
        "        \"\"\"\n",
        "        This method orchestrates the optimization process for every model in this class.\n",
        "        1. Make the data split\n",
        "        2. Performs the optimization of models (genetic, exhaustive or both)\n",
        "        3. Extract the best parameters in a clean format to use them in the models.\n",
        "        \n",
        "        Parameters:\n",
        "        - target_column: str -> Name of the target column (y).\n",
        "        - problem_type: str -> Type of problem ('regression' or 'classification').\n",
        "        - method: str -> What optimization method is going to be used ('genetic', 'exhaustive', or 'both').\n",
        "        - random_state: int -> Random seed for reproducibility.\n",
        "        \n",
        "        Returns:\n",
        "        - dict -> Keeps the best parameters for each model.\n",
        "        \"\"\"\n",
        "        # Set up models and parameter grids based on problem type\n",
        "        if problem_type.lower() == 'regression':\n",
        "            self.models = self.regression_models\n",
        "            self.param_grids_genetic = self._get_param_grids_regression_genetic()\n",
        "            self.param_grids_exhaustive = self._get_param_grids_regression_exhaustive()\n",
        "            # RMSE for regression\n",
        "            scoring_metric = 'neg_root_mean_squared_error'  \n",
        "        elif problem_type.lower() == 'classification':\n",
        "            self.models = self.classification_models\n",
        "            self.param_grids_genetic = self._get_param_grids_classification_genetic()\n",
        "            self.param_grids_exhaustive = self._get_param_grids_classification_exhaustive()\n",
        "            # AUC for classification\n",
        "            scoring_metric = 'roc_auc'  \n",
        "        else:\n",
        "            raise ValueError(\"problem_type must be 'regression' or 'classification'\")\n",
        "        \n",
        "        # 1. Make the data split\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = self.split_df(\n",
        "            target_column=target_column,\n",
        "            random_state=random_state\n",
        "        )\n",
        "        \n",
        "        # 2. Performs the optimization of models (genetic, exhaustive or both)\n",
        "        best_params = {}\n",
        "        \n",
        "        if method.lower() == 'genetic' or method.lower() == 'both':\n",
        "            genetic_results = self.genetic_search(scoring_metric)\n",
        "            \n",
        "            # 3. Extract the best parameters in a clean format\n",
        "            clean_genetic_params = {}\n",
        "            for model_name, model_result in genetic_results.items():\n",
        "                best_params_model = model_result['best_params']\n",
        "                model_params = {param.replace('clf__', ''): value for param, value in best_params_model.items()}\n",
        "                clean_genetic_params[model_name] = model_params\n",
        "            \n",
        "            best_params['genetic'] = clean_genetic_params\n",
        "            \n",
        "        if method.lower() == 'exhaustive' or method.lower() == 'both':\n",
        "            exhaustive_results = self.exhaustive_search(scoring_metric)\n",
        "            \n",
        "            # 3. Extract the best parameters in a clean format\n",
        "            clean_exhaustive_params = {}\n",
        "            for model_name, model_result in exhaustive_results.items():\n",
        "                best_params_model = model_result['best_params']\n",
        "                model_params = {param.replace('clf__', ''): value for param, value in best_params_model.items()}\n",
        "                clean_exhaustive_params[model_name] = model_params\n",
        "            \n",
        "            best_params['exhaustive'] = clean_exhaustive_params\n",
        "            \n",
        "        return best_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Models\n",
        "\n",
        "## Unsupervised"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHJDmyoocftY"
      },
      "outputs": [],
      "source": [
        "class NoSupervisado(EDA):\n",
        "    def __init__(self, datos_eda):\n",
        "        df = datos_eda.get_df()\n",
        "        super().__init__()\n",
        "        self.__df = df\n",
        "\n",
        "    @property\n",
        "    def df(self):\n",
        "        return self.__df\n",
        "\n",
        "    @df.setter\n",
        "    def df(self, p_df):\n",
        "        self.__df = p_df\n",
        "\n",
        "    def __byebye_object_values(self):\n",
        "        # Delete object columns\n",
        "        self.__df = self.__df.select_dtypes(exclude=['object'])\n",
        "\n",
        "    def calcular_metricas(self, labels):\n",
        "        \"\"\"\n",
        "        Calcula métricas de evaluación para clustering.\n",
        "        \"\"\"\n",
        "        data = self.__df.dropna()\n",
        "        data = (data - data.mean()) / data.std()\n",
        "        metrics = {\n",
        "            \"Índice de Silueta\": silhouette_score(data, labels),\n",
        "            \"Calinski-Harabasz\": calinski_harabasz_score(data, labels),\n",
        "            \"Davies-Bouldin\": davies_bouldin_score(data, labels)\n",
        "        }\n",
        "        return metrics\n",
        "\n",
        "    def kmeans(self, n_clusters):\n",
        "        self.__byebye_object_values()\n",
        "        data = self.__df\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "        labels = kmeans.fit_predict(data)\n",
        "        metrics = self.calcular_metricas(labels)\n",
        "        print(f\"Métricas para K-Means (n_clusters={n_clusters}): {metrics}\")\n",
        "        return metrics\n",
        "\n",
        "    def k_medoids(self, n_clusters, metric='euclidean'):\n",
        "        self.__byebye_object_values()\n",
        "        data = self.__df\n",
        "        \n",
        "        # Convertir a numpy array si aún no lo es\n",
        "        data_array = np.array(data)\n",
        "        \n",
        "        # Inicialización de medoides (seleccionar índices aleatorios)\n",
        "        np.random.seed(42)  # Para reproducibilidad\n",
        "        initial_medoids = np.random.choice(len(data_array), n_clusters, replace=False).tolist()\n",
        "        \n",
        "        # Crear y ejecutar el algoritmo KMedoids\n",
        "        kmedoids_instance = kmedoids(data_array, initial_medoids)\n",
        "        kmedoids_instance.process()\n",
        "        \n",
        "        # Obtener clusters y medoides\n",
        "        clusters = kmedoids_instance.get_clusters()  # Lista de listas de índices\n",
        "        medoids = kmedoids_instance.get_medoids()    # Lista de índices de medoides\n",
        "        \n",
        "        # Crear etiquetas en formato sklearn (un número para cada punto)\n",
        "        labels = np.zeros(len(data_array), dtype=int)\n",
        "        for cluster_idx, cluster in enumerate(clusters):\n",
        "            for point_idx in cluster:\n",
        "                labels[point_idx] = cluster_idx\n",
        "        \n",
        "        # Calcular métricas\n",
        "        metrics = self.calcular_metricas(labels)\n",
        "        print(f\"Métricas para K-Medoids (n_clusters={n_clusters}, metric={metric}): {metrics}\")\n",
        "        \n",
        "        return metrics\n",
        "\n",
        "    def hac(self, n_clusters=3, method='ward'):\n",
        "        self.__byebye_object_values()\n",
        "        data = self.__df\n",
        "        linkage_matrix = linkage(data, method=method)\n",
        "        labels = fcluster(linkage_matrix, t=n_clusters, criterion='maxclust')\n",
        "        metrics = self.calcular_metricas(labels)\n",
        "        print(f\"Métricas para HAC (n_clusters={n_clusters}, method={method}): {metrics}\")\n",
        "        return metrics\n",
        "\n",
        "    def umap_model(self, n_components=2, n_neighbors=15):\n",
        "        self.__byebye_object_values()\n",
        "        data = self.__df\n",
        "        modelo_umap = UMAP(n_components=n_components, n_neighbors=n_neighbors)\n",
        "        components = modelo_umap.fit_transform(data)\n",
        "        kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "        labels = kmeans.fit_predict(components)\n",
        "        metrics = self.calcular_metricas(labels)\n",
        "        print(f\"Métricas para UMAP (n_components={n_components}, n_neighbors={n_neighbors}): {metrics}\")\n",
        "        return metrics\n",
        "\n",
        "    def comparar_algoritmos(self, n_clusters):\n",
        "\n",
        "        if self.__df.isnull().any().any():\n",
        "          print(\"El DataFrame contiene valores nulos. Se eliminarán automáticamente para continuar.\")\n",
        "          self.__df.dropna(inplace=True)\n",
        "\n",
        "        print(\"\\nEjecutando K-Means...\")\n",
        "        kmeans_metrics = self.kmeans(n_clusters)\n",
        "\n",
        "        print(\"\\nEjecutando K-Medoids...\")\n",
        "        kmedoids_metrics = self.k_medoids(n_clusters)\n",
        "\n",
        "        print(\"\\nEjecutando HAC...\")\n",
        "        hac_metrics = self.hac(n_clusters=n_clusters)\n",
        "\n",
        "        print(\"\\nEjecutando UMAP...\")\n",
        "        umap_metrics = self.umap_model(n_components=2, n_neighbors=15)\n",
        "\n",
        "        resultados = pd.DataFrame({\n",
        "            \"K-Means\": kmeans_metrics,\n",
        "            \"K-Medoids\": kmedoids_metrics,\n",
        "            \"HAC\": hac_metrics,\n",
        "            \"UMAP\": umap_metrics\n",
        "        }).T\n",
        "\n",
        "        print(\"\\nComparación de Algoritmos:\")\n",
        "        print(resultados)\n",
        "        return resultados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Supervised "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grQ-5IXLjR-7"
      },
      "outputs": [],
      "source": [
        "class Supervisado:\n",
        "    def __init__(self, data_optimization, best_params=None):\n",
        "        \"\"\"\n",
        "        Initialize the Supervisado class with data and optimized parameters from DataOptimization\n",
        "        \n",
        "        Parameters:\n",
        "        - data_optimization: Instance of DataOptimization with optimized models and data\n",
        "        \"\"\"\n",
        "\n",
        "        self.resultados = {}\n",
        "\n",
        "        # Get the processed dataframe\n",
        "        self.__df = data_optimization.X_train\n",
        "        \n",
        "        # Store the data splits from DataOptimization\n",
        "        self.X_train = data_optimization.X_train\n",
        "        self.X_test = data_optimization.X_test\n",
        "        self.y_train = data_optimization.y_train\n",
        "        self.y_test = data_optimization.y_test\n",
        "        \n",
        "        # Store the best parameters from optimization\n",
        "        self.best_params = best_params\n",
        "        \n",
        "        # Problem type (regression or classification)\n",
        "        self.problem_type = 'regression' if 'Regressor' in list(data_optimization.models.keys())[0] or list(data_optimization.models.keys())[0] in ['LinearRegression', 'Lasso', 'Ridge'] else 'classification'\n",
        "        \n",
        "        # Results storage for comparison\n",
        "        self.all_model_results = []\n",
        "\n",
        "    @property\n",
        "    def df(self):\n",
        "        return self.__df\n",
        "\n",
        "    @df.setter\n",
        "    def df(self, p_df):\n",
        "        self.__df = p_df\n",
        "\n",
        "#-----------------Evaluating Models----------------------------\n",
        "    def calcular_metricas(self, modelo, X_test, y_test, predicciones, modelo_nombre):\n",
        "      \"\"\"\n",
        "      Calculate the model evaluation metrics and save the results in a dictionary.\n",
        "\n",
        "      Parameters:\n",
        "      - modelo: The model in use\n",
        "      - X_test: Test features\n",
        "      - y_test: Test labels\n",
        "      - predicciones: Model predictions\n",
        "      - modelo_nombre: Model name\n",
        "\n",
        "      Returns:\n",
        "      - resultados: Dictionary containing evaluation metrics.\n",
        "      \"\"\"\n",
        "\n",
        "      mse = mean_squared_error(y_test, predicciones)\n",
        "      r2 = r2_score(y_test, predicciones)\n",
        "      mae = mean_absolute_error(y_test, predicciones)\n",
        "      rmse = np.sqrt(mse)\n",
        "      tolerancia = 0.1  # 10% of tolerance\n",
        "      precision_global = np.mean(np.abs(y_test - predicciones) <= (tolerancia * y_test)) * 100\n",
        "\n",
        "      resultados = {\n",
        "          'modelo': modelo_nombre,\n",
        "          'MSE': mse,\n",
        "          'R2': r2,\n",
        "          'MAE': mae,\n",
        "          'RMSE': rmse,\n",
        "          'precision_global': precision_global,\n",
        "          #'predicciones': predicciones.tolist(),\n",
        "          #'valores_reales': y_test.tolist()\n",
        "      }\n",
        "      return resultados\n",
        "\n",
        "    def calcular_metricas_clasificacion(self, modelo, X_test, y_test, predicciones, model_name):\n",
        "        \"\"\"\n",
        "        Calculate evaluation metrics for classification models and store the results in a dictionary.\n",
        "\n",
        "        Parameters:\n",
        "        - y_test: True labels of the test dataset.\n",
        "        - predicciones: Predicted labels from the classification model.\n",
        "        - model_name: Name or identifier of the evaluated model.\n",
        "\n",
        "        Returns:\n",
        "        - resultados: Dictionary containing evaluation metrics.\n",
        "        \"\"\"\n",
        "        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "        accuracy = accuracy_score(y_test, predicciones)\n",
        "        precision = precision_score(y_test, predicciones, average='weighted', zero_division=0)\n",
        "        recall = recall_score(y_test, predicciones, average='weighted', zero_division=0)\n",
        "        f1 = f1_score(y_test, predicciones, average='weighted', zero_division=0)\n",
        "\n",
        "        resultados = {\n",
        "            'modelo': model_name,\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1_score': f1\n",
        "        }\n",
        "\n",
        "        # Add ROC-AUC only for binary classification\n",
        "        if hasattr(modelo, \"predict_proba\") and len(np.unique(y_test)) == 2:\n",
        "            y_proba = modelo.predict_proba(X_test)[:, 1]\n",
        "            auc = roc_auc_score(y_test, y_proba)\n",
        "            resultados['roc_auc'] = auc\n",
        "\n",
        "        return resultados\n",
        "\n",
        "\n",
        "#------------------------Regression Models--------------------------------------------------------------\n",
        "\n",
        "    def regre_lineal_simple(self, X_train, X_test, y_train, y_test):\n",
        "      \"\"\"\n",
        "      Performs Simple Linear Regression and computes multiple performance metrics.\n",
        "\n",
        "      Parameters:\n",
        "      - X_train: Features used for training the regression model.\n",
        "      - y_train: Target variable used for training.\n",
        "      - X_test: Features used to evaluate the regression model.\n",
        "      - y_test: Actual target values to compare against predictions.\n",
        "      - modelo_nombre: Name or identifier of the evaluated regression model.\n",
        "\n",
        "      Returns: Dictionary containing regression performance metrics (model name, MSE, RMSE, MAE, R² score).\n",
        "      \"\"\"\n",
        "      print(\"Starting Simple Linear Regression...\")\n",
        "      # Get optimized parameters if available\n",
        "      params = self.best_params.get('LinearRegression', {}) if self.best_params else {}\n",
        "      \n",
        "      # Create model with optimized parameters\n",
        "      modelo = LinearRegression(**params)\n",
        "      modelo.fit(self.X_train, self.y_train)\n",
        "      predicciones = modelo.predict(self.X_test)\n",
        "      return self.calcular_metricas(modelo, X_test, y_test, predicciones, 'Regresión Lineal Simple')\n",
        "\n",
        "    def regre_svm(self, X_train, X_test, y_train, y_test):\n",
        "      \"\"\"\n",
        "      Realiza una Support Vector Machine y calcula múltiples métricas de rendimiento.\n",
        "      Returns:\n",
        "      - resultados: Diccionario con métricas de rendimiento del modelo\n",
        "      \"\"\"\n",
        "      print(\"Iniciando Support Vector Machine (SVM)...\")\n",
        "      from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "      # Escalar los datos\n",
        "      scaler = StandardScaler()\n",
        "      X_train_scaled = scaler.fit_transform(X_train)\n",
        "      X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "      modelo = SVR(kernel='rbf', C=1.0, gamma='scale')\n",
        "      modelo.fit(X_train_scaled, y_train)\n",
        "      predicciones = modelo.predict(X_test_scaled)\n",
        "\n",
        "      return self.calcular_metricas(modelo, X_test_scaled, y_test, predicciones, 'Support Vector Machine')\n",
        "\n",
        "\n",
        "    def regre_regridge(self, X_train, X_test, y_train, y_test):\n",
        "      \"\"\"\n",
        "      Realiza un Regresión Ridge y calcula múltiples métricas de rendimiento.\n",
        "      Returns:\n",
        "      - resultados: Diccionario con métricas de rendimiento del modelo\n",
        "      \"\"\"\n",
        "      print(\"Iniciando Regresión Ridge...\")\n",
        "      modelo = Ridge(alpha = 1.0)\n",
        "      modelo.fit(X_train, y_train)\n",
        "      predicciones = modelo.predict(X_test)\n",
        "      return self.calcular_metricas(modelo, X_test, y_test, predicciones, 'Regresión Ridge')\n",
        "\n",
        "    def regre_decisionTree(self, X_train, X_test, y_train, y_test):\n",
        "      \"\"\"\n",
        "      Realiza un Decision Tree Regressor y calcula múltiples métricas de rendimiento.\n",
        "      Returns:\n",
        "      - resultados: Diccionario con métricas de rendimiento del modelo\n",
        "      \"\"\"\n",
        "      print(\"Iniciando DecisionTreeRegressor..\")\n",
        "      modelo = DecisionTreeRegressor(random_state=0)\n",
        "      modelo.fit(X_train, y_train)\n",
        "      predicciones = modelo.predict(X_test)\n",
        "      return self.calcular_metricas(modelo, X_test, y_test, predicciones, 'Decision Tree Regressor')\n",
        "\n",
        "    def regre_randomforest(self, X_train, X_test, y_train, y_test):\n",
        "      \"\"\"\n",
        "      Realiza un Random Forest Regressor y calcula múltiples métricas de rendimiento.\n",
        "      Returns:\n",
        "      - resultados: Diccionario con métricas de rendimiento del modelo\n",
        "      \"\"\"\n",
        "      print(\"Iniciando RandomForest Regressor..\")\n",
        "      modelo = RandomForestRegressor(max_depth=2, random_state=0)\n",
        "      modelo.fit(X_train, y_train)\n",
        "      predicciones = modelo.predict(X_test)\n",
        "      return self.calcular_metricas(modelo, X_test, y_test, predicciones, 'Random Forest Regressor')\n",
        "\n",
        "    def regre_gradient_boosting(self, X_train, X_test, y_train, y_test):\n",
        "      \"\"\"\n",
        "      Realiza un Grandient Boostsing Regressor y calcula múltiples métricas de rendimiento.\n",
        "      Returns:\n",
        "      - resultados: Diccionario con métricas de rendimiento del modelo\n",
        "      \"\"\"\n",
        "      print(\"Iniciando Grandient Boostsing Regressor..\")\n",
        "      modelo = GradientBoostingRegressor(random_state=0)\n",
        "      modelo.fit(X_train, y_train)\n",
        "      predicciones = modelo.predict(X_test)\n",
        "      return self.calcular_metricas(modelo, X_test, y_test, predicciones, 'Grandient Boostsing Regressor')\n",
        "\n",
        "    def regre_xgboost(self, X_train, X_test, y_train, y_test):\n",
        "      \"\"\"\n",
        "      Realiza un XGBoost Regressor y calcula múltiples métricas de rendimiento.\n",
        "      Returns:\n",
        "      - resultados: Diccionario con métricas de rendimiento del modelo\n",
        "      \"\"\"\n",
        "      print(\"Iniciando XGBoost Regressor..\")\n",
        "      modelo = XGBRegressor(random_state=0)\n",
        "      modelo.fit(X_train, y_train)\n",
        "      predicciones = modelo.predict(X_test)\n",
        "      return self.calcular_metricas(modelo, X_test, y_test, predicciones, 'XGBoost Regressor')\n",
        "\n",
        "#------------------------Classification Models--------------------------------------------------------------\n",
        "\n",
        "    def classi_decision_tree(self, X_train, X_test, y_train, y_test, compare_params=False):\n",
        "      \"\"\"\n",
        "      Trains and compares Decision Tree classifiers with default and optimized parameters.\n",
        "\n",
        "      Parameters:\n",
        "      - X_train, X_test: Features for training/testing\n",
        "      - y_train, y_test: Labels for training/testing\n",
        "      - compare_params: Enables parameter comparison if True\n",
        "\n",
        "      Returns:\n",
        "      - List of dictionaries with evaluation metrics\n",
        "      \"\"\"\n",
        "      print(\"Starting Decision Tree Classifier...\")\n",
        "      from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "      results = []\n",
        "\n",
        "      # 1. Default parameters\n",
        "      default_model = DecisionTreeClassifier(random_state=0)\n",
        "      default_model.fit(X_train, y_train)\n",
        "      default_preds = default_model.predict(X_test)\n",
        "      default_results = self.calcular_metricas_clasificacion(\n",
        "          default_model, X_test, y_test, default_preds, 'Decision Tree (Default)')\n",
        "      results.append(default_results)\n",
        "\n",
        "      if compare_params and self.best_params:\n",
        "        # 2. Genetic parameters if available\n",
        "        if 'genetic' in self.best_params and 'DecisionTreeClassifier' in self.best_params['genetic']:\n",
        "            gen_params = self.best_params['genetic']['DecisionTreeClassifier']\n",
        "            gen_model = DecisionTreeClassifier(random_state=0, **gen_params)\n",
        "            gen_model.fit(X_train, y_train)\n",
        "            gen_preds = gen_model.predict(X_test)\n",
        "            gen_results = self.calcular_metricas_clasificacion(\n",
        "                gen_model, X_test, y_test, gen_preds, 'Decision Tree (Genetic)')\n",
        "            results.append(gen_results)\n",
        "        \n",
        "        # 3. Exhaustive parameters if available\n",
        "        if 'exhaustive' in self.best_params and 'DecisionTreeClassifier' in self.best_params['exhaustive']:\n",
        "            ex_params = self.best_params['exhaustive']['DecisionTreeClassifier']\n",
        "            ex_model = DecisionTreeClassifier(random_state=0, **ex_params)\n",
        "            ex_model.fit(X_train, y_train)\n",
        "            ex_preds = ex_model.predict(X_test)\n",
        "            ex_results = self.calcular_metricas_clasificacion(\n",
        "                ex_model, X_test, y_test, ex_preds, 'Decision Tree (Exhaustive)')\n",
        "            results.append(ex_results)\n",
        "    \n",
        "        return results\n",
        "\n",
        "    def classi_knn(self, X_train, X_test, y_train, y_test, compare_params=False):\n",
        "      \"\"\"\n",
        "      Trains and compares K-Nearest Neighbors with default and optimized parameters.\n",
        "\n",
        "      Parameters:\n",
        "      - X_train, X_test: Features for training/testing\n",
        "      - y_train, y_test: Labels for training/testing\n",
        "      - compare_params: Enables parameter comparison if True\n",
        "\n",
        "      Returns:\n",
        "      - List of dictionaries with evaluation metrics\n",
        "      \"\"\"\n",
        "      print(\"Starting K-Nearest Neighbors Classifier...\")\n",
        "      from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "      results = []\n",
        "\n",
        "      # 1. Default parameters\n",
        "      default_model = KNeighborsClassifier()\n",
        "      default_model.fit(X_train, y_train)\n",
        "      default_preds = default_model.predict(X_test)\n",
        "      default_results = self.calcular_metricas_clasificacion(\n",
        "          default_model, X_test, y_test, default_preds, 'KNeighborsClassifier (Default)')\n",
        "      results.append(default_results)\n",
        "\n",
        "      if compare_params and self.best_params:\n",
        "        # 2. Genetic parameters if available\n",
        "        if 'genetic' in self.best_params and 'KNeighborsClassifier' in self.best_params['genetic']:\n",
        "            gen_params = self.best_params['genetic']['KNeighborsClassifier']\n",
        "            gen_model = KNeighborsClassifier(**gen_params)\n",
        "            gen_model.fit(X_train, y_train)\n",
        "            gen_preds = gen_model.predict(X_test)\n",
        "            gen_results = self.calcular_metricas_clasificacion(\n",
        "                gen_model, X_test, y_test, gen_preds, 'KNeighborsClassifier (Genetic)')\n",
        "            results.append(gen_results)\n",
        "        \n",
        "        # 3. Exhaustive parameters if available\n",
        "        if 'exhaustive' in self.best_params and 'KNeighborsClassifier' in self.best_params['exhaustive']:\n",
        "            ex_params = self.best_params['exhaustive']['KNeighborsClassifier']\n",
        "            ex_model = KNeighborsClassifier(**ex_params)\n",
        "            ex_model.fit(X_train, y_train)\n",
        "            ex_preds = ex_model.predict(X_test)\n",
        "            ex_results = self.calcular_metricas_clasificacion(\n",
        "                ex_model, X_test, y_test, ex_preds, 'KNeighborsClassifier (Exhaustive)')\n",
        "            results.append(ex_results)\n",
        "    \n",
        "        return results\n",
        "\n",
        "    def classi_random_forest(self, X_train, X_test, y_train, y_test, compare_params=False):\n",
        "      \"\"\"\n",
        "      Trains and compares Random Forest with default and optimized parameters.\n",
        "\n",
        "      Parameters:\n",
        "      - X_train, X_test: Features for training/testing\n",
        "      - y_train, y_test: Labels for training/testing\n",
        "      - compare_params: Enables parameter comparison if True\n",
        "\n",
        "      Returns:\n",
        "      - List of dictionaries with evaluation metrics\n",
        "      \"\"\"\n",
        "      print(\"Starting Random Forest Classifier...\")\n",
        "      from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "      results = []\n",
        "\n",
        "      # 1. Default parameters\n",
        "      default_model = RandomForestClassifier(random_state=0)\n",
        "      default_model.fit(X_train, y_train)\n",
        "      default_preds = default_model.predict(X_test)\n",
        "      default_results = self.calcular_metricas_clasificacion(\n",
        "          default_model, X_test, y_test, default_preds, 'Random Forest (Default)')\n",
        "      results.append(default_results)\n",
        "\n",
        "\n",
        "      if compare_params and self.best_params:\n",
        "        # 2. Genetic parameters if available\n",
        "        if 'genetic' in self.best_params and 'RandomForestClassifier' in self.best_params['genetic']:\n",
        "            gen_params = self.best_params['genetic']['RandomForestClassifier']\n",
        "            gen_model = RandomForestClassifier(random_state=0, **gen_params)\n",
        "            gen_model.fit(X_train, y_train)\n",
        "            gen_preds = gen_model.predict(X_test)\n",
        "            gen_results = self.calcular_metricas_clasificacion(\n",
        "                gen_model, X_test, y_test, gen_preds, 'Random Forest (Genetic)')\n",
        "            results.append(gen_results)\n",
        "        \n",
        "        # 3. Exhaustive parameters if available\n",
        "        if 'exhaustive' in self.best_params and 'RandomForestClassifier' in self.best_params['exhaustive']:\n",
        "            ex_params = self.best_params['exhaustive']['RandomForestClassifier']\n",
        "            ex_model = RandomForestClassifier(random_state=0, **ex_params)\n",
        "            ex_model.fit(X_train, y_train)\n",
        "            ex_preds = ex_model.predict(X_test)\n",
        "            ex_results = self.calcular_metricas_clasificacion(\n",
        "                ex_model, X_test, y_test, ex_preds, 'Random Forest (Exhaustive)')\n",
        "            results.append(ex_results)\n",
        "    \n",
        "        return results\n",
        "\n",
        "    def classi_adaboost(self, X_train, X_test, y_train, y_test, compare_params=False):\n",
        "      \"\"\"\n",
        "      Trains and compares AdaBoost classifiers with default and optimized parameters.\n",
        "\n",
        "      Parameters:\n",
        "      - X_train, X_test: Features for training/testing\n",
        "      - y_train, y_test: Labels for training/testing\n",
        "      - compare_params: Enables parameter comparison if True\n",
        "\n",
        "      Returns:\n",
        "      - List of dictionaries with evaluation metrics\n",
        "      \"\"\"\n",
        "      print(\"Starting AdaBoost Classifier...\")\n",
        "      from sklearn.ensemble import AdaBoostClassifier\n",
        "      results = []\n",
        "\n",
        "      # 1. Default parameters\n",
        "      default_model = AdaBoostClassifier(random_state=42)\n",
        "      default_model.fit(X_train, y_train)\n",
        "      default_preds = default_model.predict(X_test)\n",
        "      default_results = self.calcular_metricas_clasificacion(\n",
        "          default_model, X_test, y_test, default_preds, 'AdaBoost (Default)')\n",
        "      \n",
        "      if compare_params and self.best_params:\n",
        "        # 2. Genetic parameters if available\n",
        "        if 'genetic' in self.best_params and 'AdaBoostClassifier' in self.best_params['genetic']:\n",
        "            gen_params = self.best_params['genetic']['AdaBoostClassifier']\n",
        "            gen_model = AdaBoostClassifier(random_state=42, **gen_params)\n",
        "            gen_model.fit(X_train, y_train)\n",
        "            gen_preds = gen_model.predict(X_test)\n",
        "            gen_results = self.calcular_metricas_clasificacion(\n",
        "                gen_model, X_test, y_test, gen_preds, 'AdaBoost (Genetic)')\n",
        "            results.append(gen_results)\n",
        "        \n",
        "        # 3. Exhaustive parameters if available\n",
        "        if 'exhaustive' in self.best_params and 'AdaBoostClassifier' in self.best_params['exhaustive']:\n",
        "            ex_params = self.best_params['exhaustive']['AdaBoostClassifier']\n",
        "            ex_model = AdaBoostClassifier(random_state=42, **ex_params)\n",
        "            ex_model.fit(X_train, y_train)\n",
        "            ex_preds = ex_model.predict(X_test)\n",
        "            ex_results = self.calcular_metricas_clasificacion(\n",
        "                ex_model, X_test, y_test, ex_preds, 'AdaBoost (Exhaustive)')\n",
        "            results.append(ex_results)\n",
        "    \n",
        "        return results\n",
        "      \n",
        "    def model_director(self, compare_params=False):\n",
        "        \"\"\"\n",
        "        Executes all models and stores the results in a list: all_model_results.\n",
        "\n",
        "        Parameters:\n",
        "        - compare_params: bool, default False. If True, compare default and optimized parameters.\n",
        "        \"\"\"\n",
        "        # Clean old results\n",
        "        #self.all_model_results = []\n",
        "\n",
        "        # Execute classification models\n",
        "        dt_results = self.classi_decision_tree(self.X_train, self.X_test,\n",
        "                                            self.y_train, self.y_test,\n",
        "                                            compare_params=compare_params)\n",
        "        if dt_results:\n",
        "            self.all_model_results.extend(dt_results)\n",
        "\n",
        "        knn_results = self.classi_knn(self.X_train, self.X_test,\n",
        "                                    self.y_train, self.y_test,\n",
        "                                    compare_params=compare_params)\n",
        "        if knn_results:\n",
        "            self.all_model_results.extend(knn_results)\n",
        "\n",
        "        rf_results = self.classi_random_forest(self.X_train, self.X_test,\n",
        "                                            self.y_train, self.y_test,\n",
        "                                            compare_params=compare_params)\n",
        "        if rf_results:\n",
        "            self.all_model_results.extend(rf_results)\n",
        "\n",
        "        adaboost_results = self.classi_adaboost(self.X_train, self.X_test,\n",
        "                                                self.y_train, self.y_test,\n",
        "                                                compare_params=compare_params)\n",
        "        if adaboost_results:\n",
        "            self.all_model_results.extend(adaboost_results)\n",
        "\n",
        "# Prueba para visualizar\n",
        "\n",
        "    def visualizar_comparacion_modelos(self, metrica='roc_auc', figsize=(12, 8), guardar_grafico=False, ruta_guardado=None):\n",
        "\n",
        "        \n",
        "        if not self.all_model_results:\n",
        "            print(\"No hay resultados disponibles. Ejecuta model_director primero.\")\n",
        "            return None, None\n",
        "        \n",
        "        # Extraer los datos necesarios para la visualización\n",
        "        modelos = []\n",
        "        valores = []\n",
        "        categorias = []\n",
        "        \n",
        "        for resultado in self.all_model_results:\n",
        "            # Extraer nombre del modelo y tipo de optimización\n",
        "            nombre_completo = resultado.get('modelo_nombre') or resultado.get('modelo')\n",
        "            \n",
        "            # Separar nombre base del modelo y tipo de optimización\n",
        "            if '(' in nombre_completo:\n",
        "                nombre_base = nombre_completo.split('(')[0].strip()\n",
        "                optimizacion = nombre_completo.split('(')[1].replace(')', '').strip()\n",
        "            else:\n",
        "                nombre_base = nombre_completo\n",
        "                optimizacion = 'Default'\n",
        "            \n",
        "            # Añadir datos a las listas\n",
        "            modelos.append(nombre_base)\n",
        "            valores.append(resultado[metrica])\n",
        "            categorias.append(optimizacion)\n",
        "        \n",
        "        # Crear DataFrame para facilitar la visualización\n",
        "        df = pd.DataFrame({\n",
        "            'Modelo': modelos,\n",
        "            'Valor': valores,\n",
        "            'Categoría': categorias\n",
        "        })\n",
        "        \n",
        "        # Obtener modelos únicos y categorías únicas para organizar el gráfico\n",
        "        modelos_unicos = sorted(df['Modelo'].unique())\n",
        "        categorias_unicas = ['Default', 'Genetic', 'Exhaustive']  # Orden predefinido\n",
        "        \n",
        "        # Crear figura y ejes\n",
        "        fig, ax = plt.subplots(figsize=figsize)\n",
        "        \n",
        "        # Configurar la anchura de las barras y posiciones\n",
        "        ancho_barra = 0.25\n",
        "        indice = np.arange(len(modelos_unicos))\n",
        "        offsets = np.linspace(-ancho_barra, ancho_barra, len(categorias_unicas))\n",
        "        \n",
        "        # Generar las barras para cada categoría\n",
        "        for i, categoria in enumerate(categorias_unicas):\n",
        "            # Filtrar datos por categoría\n",
        "            datos_categoria = df[df['Categoría'] == categoria]\n",
        "            if datos_categoria.empty:\n",
        "                continue\n",
        "                \n",
        "            # Crear un diccionario para mapear modelos a valores\n",
        "            valores_por_modelo = dict(zip(datos_categoria['Modelo'], datos_categoria['Valor']))\n",
        "            \n",
        "            # Obtener valores para cada modelo en esta categoría\n",
        "            valores_a_plotear = [valores_por_modelo.get(modelo, 0) for modelo in modelos_unicos]\n",
        "            \n",
        "            # Plotear barras\n",
        "            ax.bar(indice + offsets[i], valores_a_plotear, ancho_barra, \n",
        "                label=f'{categoria}',\n",
        "                alpha=0.7)\n",
        "        \n",
        "        # Configurar etiquetas y leyenda\n",
        "        ax.set_xlabel('Algoritmos de Clasificación')\n",
        "        ax.set_ylabel(f'Valor de {metrica.upper()}')\n",
        "        ax.set_title(f'Comparación de Modelos - Métrica: {metrica.upper()}')\n",
        "        ax.set_xticks(indice)\n",
        "        ax.set_xticklabels(modelos_unicos, rotation=45, ha='right')\n",
        "        ax.legend()\n",
        "        \n",
        "        # Añadir línea de referencia para AUC = 0.5 (umbral de modelo aleatorio)\n",
        "        if metrica.lower() == 'roc_auc':\n",
        "            ax.axhline(y=0.5, color='r', linestyle='--', alpha=0.3, \n",
        "                    label='Umbral aleatorio (AUC=0.5)')\n",
        "        \n",
        "        # Añadir grid para mejor visualización\n",
        "        ax.grid(True, linestyle='--', alpha=0.3)\n",
        "        \n",
        "        # Añadir valores sobre las barras\n",
        "        for i, categoria in enumerate(categorias_unicas):\n",
        "            datos_categoria = df[df['Categoría'] == categoria]\n",
        "            if datos_categoria.empty:\n",
        "                continue\n",
        "                \n",
        "            valores_por_modelo = dict(zip(datos_categoria['Modelo'], datos_categoria['Valor']))\n",
        "            \n",
        "            for j, modelo in enumerate(modelos_unicos):\n",
        "                if modelo in valores_por_modelo:\n",
        "                    valor = valores_por_modelo[modelo]\n",
        "                    ax.text(j + offsets[i], valor + 0.01, f'{valor:.3f}', \n",
        "                        ha='center', va='bottom', fontsize=8)\n",
        "        \n",
        "        # Ajustar layout\n",
        "        plt.tight_layout()\n",
        "        \n",
        "        # Guardar gráfico si se solicita\n",
        "        if guardar_grafico and ruta_guardado:\n",
        "            plt.savefig(ruta_guardado, dpi=300, bbox_inches='tight')\n",
        "            print(f\"Gráfico guardado en: {ruta_guardado}\")\n",
        "        \n",
        "        return fig, ax\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data split:\n",
            "- Training: 1608 rows\n",
            "- Test: 403 rows\n",
            "Performing feature selection with LassoCV...\n",
            "Training DecisionTreeClassifier with genetic method...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\esteb\\Documents\\GitHub\\MineriaAvanzada\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2732: UserWarning: X has feature names, but SelectFromModel was fitted without feature names\n",
            "  warnings.warn(\n",
            "c:\\Users\\esteb\\Documents\\GitHub\\MineriaAvanzada\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2732: UserWarning: X has feature names, but SelectFromModel was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gen\tnevals\tfitness \tfitness_std\tfitness_max\tfitness_min\n",
            "0  \t10    \t0.825815\t0.0857467  \t0.922789   \t0.664066   \n",
            "1  \t9     \t0.887678\t0.0388708  \t0.923439   \t0.797351   \n",
            "2  \t6     \t0.918518\t0.00936116 \t0.923439   \t0.899364   \n",
            "3  \t8     \t0.923439\t0          \t0.923439   \t0.923439   \n",
            "4  \t6     \t0.923439\t0          \t0.923439   \t0.923439   \n",
            "5  \t8     \t0.923439\t0          \t0.923439   \t0.923439   \n",
            "Training KNeighborsClassifier with genetic method...\n",
            "gen\tnevals\tfitness\tfitness_std\tfitness_max\tfitness_min\n",
            "0  \t10    \t0.99916\t0.00109497 \t0.99979    \t0.996975   \n",
            "1  \t6     \t0.999698\t0.000154861\t0.99979    \t0.999285   \n",
            "2  \t6     \t0.99978 \t1.47689e-05\t0.99979    \t0.999758   \n",
            "3  \t7     \t0.99979 \t1.11022e-16\t0.99979    \t0.99979    \n",
            "4  \t6     \t0.99979 \t1.11022e-16\t0.99979    \t0.99979    \n",
            "5  \t7     \t0.999786\t1.20937e-05\t0.99979    \t0.99975    \n",
            "Training RandomForestClassifier with genetic method...\n",
            "gen\tnevals\tfitness \tfitness_std\tfitness_max\tfitness_min\n",
            "0  \t10    \t0.975435\t0.0320626  \t0.999637   \t0.913754   \n",
            "1  \t6     \t0.997598\t0.00192993 \t0.999661   \t0.994067   \n",
            "2  \t7     \t0.999139\t0.000765587\t0.999661   \t0.997934   \n",
            "3  \t8     \t0.99965 \t1.35678e-05\t0.999661   \t0.999629   \n",
            "4  \t8     \t0.99966 \t8.51924e-06\t0.999673   \t0.999637   \n",
            "5  \t6     \t0.999665\t5.51717e-06\t0.999673   \t0.999661   \n",
            "Training AdaBoostClassifier with genetic method...\n",
            "gen\tnevals\tfitness \tfitness_std\tfitness_max\tfitness_min\n",
            "0  \t10    \t0.646587\t0.0401121  \t0.717875   \t0.614562   \n",
            "1  \t6     \t0.682895\t0.0358577  \t0.717875   \t0.62264    \n",
            "2  \t6     \t0.695624\t0.0348287  \t0.717875   \t0.62264    \n",
            "3  \t6     \t0.717875\t1.11022e-16\t0.717875   \t0.717875   \n",
            "4  \t7     \t0.708352\t0.0285706  \t0.717875   \t0.62264    \n",
            "5  \t5     \t0.714837\t0.00911509 \t0.717875   \t0.687492   \n",
            "Performing feature selection with LassoCV...\n",
            "Training DecisionTreeClassifier with exhaustive method...\n",
            "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\esteb\\Documents\\GitHub\\MineriaAvanzada\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The parameter 'algorithm' is deprecated in 1.6 and has no effect. It will be removed in version 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training KNeighborsClassifier with exhaustive method...\n",
            "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
            "Training RandomForestClassifier with exhaustive method...\n",
            "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
            "Training AdaBoostClassifier with exhaustive method...\n",
            "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\esteb\\Documents\\GitHub\\MineriaAvanzada\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The parameter 'algorithm' is deprecated in 1.6 and has no effect. It will be removed in version 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "Supervisado.__init__() got an unexpected keyword argument 'best_params'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     11\u001b[39m best_params = optimizador.opti_director(\n\u001b[32m     12\u001b[39m     target_column=\u001b[33m'\u001b[39m\u001b[33mPotability\u001b[39m\u001b[33m'\u001b[39m,         \n\u001b[32m     13\u001b[39m     problem_type=\u001b[33m'\u001b[39m\u001b[33mclassification\u001b[39m\u001b[33m'\u001b[39m,      \n\u001b[32m     14\u001b[39m     method=\u001b[33m'\u001b[39m\u001b[33mboth\u001b[39m\u001b[33m'\u001b[39m                      \n\u001b[32m     15\u001b[39m )\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# 4. Crear la instancia del modelo supervisado pasando los mejores parámetros\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m modelo = \u001b[43mSupervisado\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizador\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbest_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# 5. Ejecutar los modelos de clasificación y comparar variantes\u001b[39;00m\n\u001b[32m     21\u001b[39m modelo.model_director(compare_params=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[31mTypeError\u001b[39m: Supervisado.__init__() got an unexpected keyword argument 'best_params'"
          ]
        }
      ],
      "source": [
        "# MI TESTING\n",
        "#Recuerda el input arriba en VSCode\n",
        "# 1. Cargar el archivo CSV con EDA\n",
        "archivo_csv = \"../dataset/potabilidad_V2.csv\"\n",
        "eda = EDA(file=archivo_csv)\n",
        "\n",
        "# 2. Pasar el EDA al optimizador\n",
        "optimizador = DataOptimization(eda)\n",
        "\n",
        "# 3. Ejecutar optimización y obtener los mejores parámetros\n",
        "best_params = optimizador.opti_director(\n",
        "    target_column='Potability',         \n",
        "    problem_type='classification',      \n",
        "    method='both'                      \n",
        ")\n",
        "\n",
        "# 4. Crear la instancia del modelo supervisado pasando los mejores parámetros\n",
        "modelo = Supervisado(optimizador, best_params=best_params)\n",
        "\n",
        "# 5. Ejecutar los modelos de clasificación y comparar variantes\n",
        "modelo.model_director(compare_params=True)\n",
        "\n",
        "# 6. Ver los resultados como lista de diccionarios\n",
        "for resultado in modelo.all_model_results:\n",
        "    print(resultado)\n",
        "\n",
        "# 7. Visualizar la comparación usando ROC AUC\n",
        "fig, ax = modelo.visualizar_comparacion_modelos(metrica='roc_auc')\n",
        "\n",
        "# 8. Mostrar gráfico\n",
        "import matplotlib.pyplot as plt\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "GUAM4Lsdna5a"
      },
      "outputs": [],
      "source": [
        "class Start:\n",
        "    def __init__(self):\n",
        "        self.eda = None\n",
        "        self.supervisado = None\n",
        "        self.no_supervisado = None\n",
        "        self.split_data = None\n",
        "\n",
        "\n",
        "    def mostrar_menu(self):\n",
        "        while True:\n",
        "            print(\"\\n--- Menú Principal ---\")\n",
        "            print(\"1. 📁 Carga de datos en formato CSV y completar EDA\")\n",
        "            print(\"2. 🪐 Ejecutar modelo\")\n",
        "            print(\"3. 🛑 Salir\")\n",
        "            opcion = input(\"Seleccione una opción: \")\n",
        "\n",
        "            if opcion == \"1\":\n",
        "                self.datos_eda()\n",
        "            elif opcion == \"2\":\n",
        "                self.models_menu()\n",
        "            elif opcion == \"3\":\n",
        "                print(\"Saliendo del programa...\")\n",
        "                break\n",
        "            else:\n",
        "                print(\"Opción no válida. Intente de nuevo.\")\n",
        "\n",
        "    def models_menu(self):\n",
        "      while True:\n",
        "        print(\"\\n--- ¿Qué problema necesita resolver? ---\")\n",
        "        print(\"1. 🔍 Clasificación: Asigne etiquetas a sus datos\")\n",
        "        print(\"2. 📈 Regresión: Prediga valores continuos\")\n",
        "        print(\"3. 🧩 Aprendizaje No Supervisado: Descubra patrones ocultos\")\n",
        "        print(\"4. 🛑 Volver al menú principal\")\n",
        "        opcion = input(\"Seleccione una opción: \")\n",
        "\n",
        "        # Resetear split_data antes de cambiar de modelo\n",
        "        self.split_data = None\n",
        "        self.supervisado = None\n",
        "\n",
        "        if opcion == \"1\":\n",
        "            self.classi_modelos()\n",
        "        elif opcion == \"2\":\n",
        "            self.regre_modelos()\n",
        "        elif opcion == \"3\":\n",
        "                if self.eda and not self.eda.get_df().empty:\n",
        "                  self.no_supervisado = NoSupervisado(self.eda)\n",
        "                  n_clusters = int(input(\"Ingrese el número de clusters: \"))\n",
        "                  self.no_supervisado.comparar_algoritmos(n_clusters=n_clusters)\n",
        "                else:\n",
        "                    print(\"Primero cargue los datos para poder realizar aprendizaje no supervisado.\")\n",
        "        elif opcion == \"4\":\n",
        "            print(\"Saliendo del programa...\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"Opción no válida. Intente de nuevo.\")\n",
        "\n",
        "    def datos_eda(self):\n",
        "        while True:\n",
        "            print(\"\\n ----EDA----\")\n",
        "            print(\"1. 📂 Carga de datos\")\n",
        "            print(\"2. 🔍 Mostrar head del DataFrame\")\n",
        "            print(\"3. 📊 Revisar los tipos de datos\")\n",
        "            print(\"4. ✂️ Eliminar columnas\")\n",
        "            print(\"5. 🧹 Eliminar valores NULOS\")\n",
        "            print(\"6. ⚠️ Detectar valores atipicos\")\n",
        "            print(\"7. 📈 Graficar relación entre dos variables\")\n",
        "            print(\"8. 📉 Graficar histograma\")\n",
        "            print(\"9. 🌡 HeatMap: Generar mapa de calor\")\n",
        "            print(\"0. 🛑 Volver al menú principal\")\n",
        "            opcion = input(\"Seleccione una opción: \")\n",
        "\n",
        "            if opcion == \"1\":\n",
        "                my_data = input(\"¿Cómo se llama el CSV? \")\n",
        "                try:\n",
        "                    self.eda = EDA(file=my_data)\n",
        "                    print(\"Instancia de EDA creada y datos cargados exitosamente.\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error al cargar los datos: {e}\")\n",
        "            elif opcion == \"2\":\n",
        "                if self.eda:\n",
        "                    print(self.eda.head_df())\n",
        "                else:\n",
        "                    print(\"Primero cargue los datos.\")\n",
        "            elif opcion == \"3\":\n",
        "                if self.eda:\n",
        "                    print(self.eda.check_data_types())\n",
        "                else:\n",
        "                    print(\"Primero cargue los datos.\")\n",
        "            elif opcion == \"4\":\n",
        "                if self.eda:\n",
        "                    columnas = input(\"Ingrese los nombres de las columnas a eliminar, separadas por comas: \").split(',')\n",
        "                    columnas = [col.strip() for col in columnas]\n",
        "                    try:\n",
        "                        self.eda.drop_irrelevant_columns(columnas)\n",
        "                        print(f\"Columnas eliminadas: {', '.join(columnas)}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error al eliminar columnas: {e}\")\n",
        "                else:\n",
        "                    print(\"Primero cargue los datos.\")\n",
        "            elif opcion == \"5\":\n",
        "                if self.eda:\n",
        "                    self.eda.drop_missing_values()\n",
        "                    print(\"Valores nulos eliminados.\")\n",
        "                else:\n",
        "                    print(\"Primero cargue los datos.\")\n",
        "            elif opcion == \"6\":\n",
        "                if self.eda:\n",
        "                    print(self.eda.detect_outliers())\n",
        "                else:\n",
        "                    print(\"Primero cargue los datos.\")\n",
        "            elif opcion == \"7\":\n",
        "                if self.eda:\n",
        "                    print(\"\\n ***Variables disponibles***\")\n",
        "                    print(self.eda.check_data_types())\n",
        "                    col1 = input(\"Ingrese el nombre de la primera variable: \")\n",
        "                    col2 = input(\"Ingrese el nombre de la segunda variable: \")\n",
        "                    try:\n",
        "                        self.eda.plot_scatter(col1, col2)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error al graficar: {e}\")\n",
        "                        break\n",
        "                else:\n",
        "                    print(\"Primero cargue los datos.\")\n",
        "            elif opcion == \"8\":\n",
        "                if self.eda:\n",
        "                    print(\"\\n ***Variables disponibles***\")\n",
        "                    print(self.eda.check_data_types())\n",
        "                    histogram_col = input(\"Ingrese el nombre de una variable a graficar: \")\n",
        "                    try:\n",
        "                        self.eda.plot_histogram(histogram_col)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error al graficar: {e}\")\n",
        "                else:\n",
        "                    print(\"Primero cargue los datos.\")\n",
        "            elif opcion == \"9\":\n",
        "                if self.eda:\n",
        "                    self.eda.plot_heatmap()\n",
        "                    print(\"El programa se detendrá después de mostrar el gráfico.\")\n",
        "                    exit()\n",
        "                else:\n",
        "                    print(\"Primero cargue los datos.\")\n",
        "            elif opcion == \"0\":\n",
        "                break\n",
        "            else:\n",
        "                print(\"Opción no válida. Intente de nuevo.\")\n",
        "\n",
        "    def regre_modelos(self):\n",
        "      if self.supervisado is None:\n",
        "          if self.eda:\n",
        "              self.supervisado = Supervisado(self.eda)\n",
        "          else:\n",
        "              print(\"Primero debe cargar los datos\")\n",
        "              return\n",
        "\n",
        "      if self.split_data is None:\n",
        "          print(\"\\n ***Variables disponibles***\")\n",
        "          print(self.eda.check_data_types())\n",
        "          target_column = input(\"\\n Ingrese el nombre de la columna objetivo: \")\n",
        "          self.split_data = self.supervisado.split_df(target_column)\n",
        "\n",
        "      X_train, X_test, y_train, y_test = self.split_data\n",
        "\n",
        "      modelos = [\n",
        "          self.supervisado.regre_lineal_simple,\n",
        "          self.supervisado.regre_svm,\n",
        "          self.supervisado.regre_regridge,\n",
        "          self.supervisado.regre_decisionTree,\n",
        "          self.supervisado.regre_randomforest,\n",
        "          self.supervisado.regre_gradient_boosting,\n",
        "          self.supervisado.regre_xgboost\n",
        "      ]\n",
        "\n",
        "      resultados = []\n",
        "      for modelo in modelos:\n",
        "          resultados.append(modelo(X_train, X_test, y_train, y_test))\n",
        "\n",
        "      print(\"\\n--- Resultados del Benchmarking ---\")\n",
        "      for resultado in resultados:\n",
        "          print(f\"{resultado['modelo']}: R2={resultado['R2']:.4f}, RMSE={resultado['RMSE']:.4f}, MAE={resultado['MAE']:.4f}\")\n",
        "\n",
        "      # Opción de graficar resultados\n",
        "      graficar = input(\"\\n¿Desea graficar los resultados? (S/N): \").strip().upper()\n",
        "      if graficar == 'S':\n",
        "        # Preparar datos para la gráfica\n",
        "        nombres_modelos = [resultado['modelo'] for resultado in resultados]\n",
        "        r2_scores = [resultado['R2'] for resultado in resultados]\n",
        "        rmse_scores = [resultado['RMSE'] for resultado in resultados]\n",
        "        mae_scores = [resultado['MAE'] for resultado in resultados]\n",
        "\n",
        "        # Crear la gráfica de barras comparativa\n",
        "        plt.figure(figsize=(15, 6))\n",
        "\n",
        "        # Gráfica de R2\n",
        "        plt.subplot(1, 3, 1)\n",
        "        bars1 = plt.bar(nombres_modelos, r2_scores)\n",
        "        plt.title('R2 Scores')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.ylabel('R2')\n",
        "        # Añadir valores en las barras\n",
        "        for bar in bars1:\n",
        "            height = bar.get_height()\n",
        "            plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                     f'{height:.4f}',\n",
        "                     ha='center', va='bottom', rotation=0)\n",
        "\n",
        "        # Gráfica de RMSE\n",
        "        plt.subplot(1, 3, 2)\n",
        "        bars2 = plt.bar(nombres_modelos, rmse_scores)\n",
        "        plt.title('Root Mean Squared Error')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.ylabel('RMSE')\n",
        "        # Añadir valores en las barras\n",
        "        for bar in bars2:\n",
        "            height = bar.get_height()\n",
        "            plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                     f'{height:.4f}',\n",
        "                     ha='center', va='bottom', rotation=0)\n",
        "\n",
        "        # Gráfica de MAE\n",
        "        plt.subplot(1, 3, 3)\n",
        "        bars3 = plt.bar(nombres_modelos, mae_scores)\n",
        "        plt.title('Mean Absolute Error')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.ylabel('MAE')\n",
        "        # Añadir valores en las barras\n",
        "        for bar in bars3:\n",
        "            height = bar.get_height()\n",
        "            plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                     f'{height:.4f}',\n",
        "                     ha='center', va='bottom', rotation=0)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        exit()\n",
        "\n",
        "    def classi_modelos(self):\n",
        "        if graficar == 'S':\n",
        "          # Preparar datos para la gráfica\n",
        "          nombres_modelos = [resultado['modelo'] for resultado in resultados]\n",
        "          accuracy_scores = [resultado['accuracy'] for resultado in resultados]\n",
        "          precision_scores = [resultado['precision'] for resultado in resultados]\n",
        "          recall_scores = [resultado['recall'] for resultado in resultados]\n",
        "          f1_scores = [resultado['f1_score'] for resultado in resultados]\n",
        "\n",
        "          # Crear la gráfica de barras comparativa\n",
        "          plt.figure(figsize=(15, 6))\n",
        "\n",
        "          # Gráfica de Accuracy\n",
        "          plt.subplot(1, 4, 1)\n",
        "          bars1 = plt.bar(nombres_modelos, accuracy_scores)\n",
        "          plt.title('Accuracy')\n",
        "          plt.xticks(rotation=45, ha='right')\n",
        "          plt.ylabel('Accuracy')\n",
        "          # Añadir valores en las barras\n",
        "          for bar in bars1:\n",
        "              height = bar.get_height()\n",
        "              plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                      f'{height:.4f}',\n",
        "                      ha='center', va='bottom', rotation=0)\n",
        "\n",
        "          # Gráfica de Precision\n",
        "          plt.subplot(1, 4, 2)\n",
        "          bars2 = plt.bar(nombres_modelos, precision_scores)\n",
        "          plt.title('Precision')\n",
        "          plt.xticks(rotation=45, ha='right')\n",
        "          plt.ylabel('Precision')\n",
        "          # Añadir valores en las barras\n",
        "          for bar in bars2:\n",
        "              height = bar.get_height()\n",
        "              plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                      f'{height:.4f}',\n",
        "                      ha='center', va='bottom', rotation=0)\n",
        "\n",
        "          # Gráfica de Recall\n",
        "          plt.subplot(1, 4, 3)\n",
        "          bars3 = plt.bar(nombres_modelos, recall_scores)\n",
        "          plt.title('Recall')\n",
        "          plt.xticks(rotation=45, ha='right')\n",
        "          plt.ylabel('Recall')\n",
        "          # Añadir valores en las barras\n",
        "          for bar in bars3:\n",
        "              height = bar.get_height()\n",
        "              plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                      f'{height:.4f}',\n",
        "                      ha='center', va='bottom', rotation=0)\n",
        "\n",
        "          # Gráfica de F1-Score\n",
        "          plt.subplot(1, 4, 4)\n",
        "          bars4 = plt.bar(nombres_modelos, f1_scores)\n",
        "          plt.title('F1-Score')\n",
        "          plt.xticks(rotation=45, ha='right')\n",
        "          plt.ylabel('F1-Score')\n",
        "          # Añadir valores en las barras\n",
        "          for bar in bars4:\n",
        "              height = bar.get_height()\n",
        "              plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                      f'{height:.4f}',\n",
        "                      ha='center', va='bottom', rotation=0)\n",
        "\n",
        "          plt.tight_layout()\n",
        "          plt.show()\n",
        "          exit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWprvJMPYo_D"
      },
      "source": [
        "Ejecucion del programa con estructura pythonica"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pkb3yN8QYtEw",
        "outputId": "42c5f0ac-d091-4376-bbe5-a985977f1af8"
      },
      "outputs": [],
      "source": [
        "# Ejecución del menú principal\n",
        "if __name__ == \"__main__\":\n",
        "    start = Start()\n",
        "    start.mostrar_menu()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
