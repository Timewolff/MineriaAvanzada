{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fGfAaC9dUfO"
      },
      "source": [
        "**LEAD University - Minería de datos**\n",
        "\n",
        "Python Project\n",
        "\n",
        "**Contributors**\n",
        "- Carolina Salas Moreno\n",
        "- Deykel Bernard Salazar\n",
        "- Esteban Ramirez Montano\n",
        "- Kristhel Porras Mata\n",
        "- Marla Gomez Hernández\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pxx9r1xc-1c"
      },
      "source": [
        "## Requirements\n",
        "**Step 1:** Please install Microsoft C++ Build Tools in your machine.\n",
        "\n",
        "**Step 2:** Install Python 3.11.7\n",
        "\n",
        "**Step 3:** Run the following code if this is your first time running it `pip install -r requirements.txt`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZXUiJAacFRT"
      },
      "source": [
        "# Importar las librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQw57BLb3dFh"
      },
      "outputs": [],
      "source": [
        "# Main Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import umap.umap_ as umap\n",
        "\n",
        "# Data Optimization\n",
        "from sklearn_genetic import GASearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import SelectFromModel \n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn_genetic.space import Integer, Categorical, Continuous\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "#Feature Selection\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LassoCV \n",
        "\n",
        "# Clustering Libraries\n",
        "from abc import ABCMeta, abstractmethod\n",
        "from scipy.cluster.hierarchy import dendrogram, ward, single, complete, average, linkage, fcluster\n",
        "from sklearn.cluster import KMeans\n",
        "from pyclustering.cluster.kmedoids import kmedoids\n",
        "\n",
        "# Dimensionality Reduction\n",
        "from prince import PCA as PCA_Prince\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Evaluation Metrics\n",
        "from sklearn.metrics import silhouette_score, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "# Regression Models\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "\n",
        "# Classification Models\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, RandomForestRegressor\n",
        "\n",
        "# Additional Tools\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScX-De7IdB7L"
      },
      "source": [
        "# EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIssRCV8cKBa"
      },
      "outputs": [],
      "source": [
        "# Análisis Exploratorio de Datos (EDA)\n",
        "class EDA:\n",
        "    def __init__(self, file=None):\n",
        "        \"\"\"\n",
        "        Inicializa la clase EDA y carga datos desde un archivo CSV si se proporciona.\n",
        "\n",
        "        Parámetros:\n",
        "            file (str): Ruta al archivo CSV. Si no se proporciona, se inicializa un DataFrame vacío.\n",
        "        \"\"\"\n",
        "        self.__df = pd.read_csv(file) if file else pd.DataFrame()\n",
        "\n",
        "    def head_df(self, n=5):\n",
        "        return self.__df.head(n) if not self.__df.empty else \"No se cargaron los datos :(\"\n",
        "\n",
        "    def tail_df(self, n=5):\n",
        "        return self.__df.tail(n) if not self.__df.empty else \"No se cargaron los datos :(\"\n",
        "\n",
        "    def check_data_types(self):\n",
        "        return self.__df.dtypes\n",
        "\n",
        "    def drop_irrelevant_columns(self, columns):\n",
        "        self.__df.drop(columns=columns, inplace=True)\n",
        "\n",
        "    def drop_missing_values(self):\n",
        "        self.__df.dropna(inplace=True)\n",
        "\n",
        "    def detect_outliers(self):\n",
        "        num_df = self.__df.select_dtypes(include=['float64', 'int64'])\n",
        "        if num_df.empty:\n",
        "            return \"No hay columnas numéricas en el DataFrame.\"\n",
        "\n",
        "        Q1 = num_df.quantile(0.25)\n",
        "        Q3 = num_df.quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        outliers = ((num_df < (Q1 - 1.5 * IQR)) | (num_df > (Q3 + 1.5 * IQR))).sum()\n",
        "        Dicc_outliers = {col: outliers[col] for col in num_df.columns if outliers[col] > 0}\n",
        "\n",
        "        return Dicc_outliers if Dicc_outliers else \"No se detectaron valores atípicos en las columnas numéricas.\"\n",
        "\n",
        "    def plot_scatter(self, col1, col2):\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.scatterplot(x=self.__df[col1], y=self.__df[col2])\n",
        "        plt.title(f'Gráfico de Dispersión: {col1} vs {col2}')\n",
        "        plt.xlabel(col1)\n",
        "        plt.ylabel(col2)\n",
        "        plt.grid()\n",
        "        plt.show()\n",
        "\n",
        "    def plot_histogram(self, col):\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.histplot(self.__df[col], kde=True)\n",
        "        plt.title(f'Histograma de {col}')\n",
        "        plt.xlabel(col)\n",
        "        plt.ylabel('Frecuencia')\n",
        "        plt.show()\n",
        "\n",
        "    def plot_heatmap(self):\n",
        "        num_df = self.__df.select_dtypes(include=['float64', 'int64'])\n",
        "        if num_df.empty:\n",
        "            return \"No hay columnas numéricas para generar el mapa de calor.\"\n",
        "\n",
        "        num_df = num_df.loc[:, num_df.apply(lambda x: np.std(x) > 0.01)]\n",
        "\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        sns.heatmap(num_df.corr(), cmap=\"coolwarm\", annot= True, linewidths=0.5, cbar=True) #annot=False es para que no se vean los numeros en los cuadros\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.yticks(rotation=0)\n",
        "        plt.title(\"Correlation heatmap\", fontsize=18)\n",
        "        plt.ion()\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"Clase EDA - DataFrame de la forma: {self.__df.shape}\"\n",
        "\n",
        "    def get_df(self):\n",
        "        \"\"\"Devuelve una copia del df para que las familias de los algoritmos las utilicen\"\"\"\n",
        "        return self.__df.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DataOptimization(EDA):\n",
        "    def __init__(self, datos_eda):\n",
        "        \"\"\"\n",
        "        Use the processed DataFrame from EDA to optimize models.\n",
        "\n",
        "        Parameters:\n",
        "        - datos_eda: This is the processed DataFrame from the EDA class.\n",
        "        \"\"\"\n",
        "        self.__df = datos_eda.get_df()\n",
        "        \n",
        "        # Data components\n",
        "        self.X_train = None\n",
        "        self.X_test = None\n",
        "        self.y_train = None\n",
        "        self.y_test = None\n",
        "        \n",
        "        # Regression models\n",
        "        self.regression_models = {\n",
        "            'LinearRegression': LinearRegression(),\n",
        "            'SVM': SVR(),\n",
        "            'Ridge': Ridge(),\n",
        "            'DecisionTreeRegressor': DecisionTreeRegressor(),\n",
        "            'RandomForestRegressor': RandomForestRegressor(),\n",
        "            'GradientBoostingRegressor': GradientBoostingRegressor(),\n",
        "            'XGBRegressor': XGBRegressor(random_state=42)\n",
        "        }\n",
        "\n",
        "        # Classification models\n",
        "        self.classification_models = {\n",
        "            'DecisionTreeClassifier': DecisionTreeClassifier(),\n",
        "            'KNeighborsClassifier': KNeighborsClassifier(),\n",
        "            'RandomForestClassifier': RandomForestClassifier(),\n",
        "            'AdaBoostClassifier': AdaBoostClassifier(random_state=42)\n",
        "        }\n",
        "        \n",
        "        # Current active models based on problem type\n",
        "        self.models = None\n",
        "        \n",
        "        # Parameter grids\n",
        "        self.param_grids_genetic = None\n",
        "        self.param_grids_exhaustive = None\n",
        "\n",
        "#------------------------Data Split Components--------------------------------------------------------------\n",
        "\n",
        "    def split_df(self, target_column, test_size=None, random_state=42):\n",
        "        \"\"\"\n",
        "        Splits the dataframe into training and test sets.\n",
        "\n",
        "        Parameters:\n",
        "        - target_column: str -> Name of the target column (y).\n",
        "        - test_size: float -> Proportion of the test set (if not provided, it is calculated from the entered percentage).\n",
        "        - random_state: int -> Seed for randomization.\n",
        "\n",
        "        Returns:\n",
        "        - X_train, X_test, y_train, y_test: Split and preprocessed datasets.\n",
        "        \"\"\"\n",
        "        while True:\n",
        "            try:\n",
        "                percent = float(input(\"Enter the percentage for the training set: (Example: 80) \\n\"))\n",
        "                if 0 < percent < 100:\n",
        "                    train_size = percent / 100\n",
        "                    break # Exit the loop\n",
        "                else:\n",
        "                    print(\"The percentage must be between 1 and 99.\")\n",
        "            except ValueError:\n",
        "                print(\"Invalid number. Try again.\")\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                # Separate features (X) and target variable (y)\n",
        "                X = self.__df.drop(columns=[target_column])\n",
        "                y = self.__df[target_column]\n",
        "                break  # Exit the loop if there are no errors\n",
        "            except KeyError:\n",
        "                print(f\"The column '{target_column}' does not exist. Try again.\")\n",
        "                print(\"Available columns:\")\n",
        "                print(self.check_data_types())\n",
        "                target_column = input(\"Enter the correct name of the target column: \")\n",
        "\n",
        "        # Preprocess features (X), convert categorical variables to One-Hot Encoding\n",
        "        import pandas as pd\n",
        "        X = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "        # Check if the target variable (y) is categorical and needs encoding\n",
        "        if y.dtypes == 'object' or y.dtypes.name == 'category':\n",
        "            from sklearn.preprocessing import LabelEncoder\n",
        "            le = LabelEncoder()\n",
        "            y = le.fit_transform(y)\n",
        "\n",
        "        # Perform the split\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size= 1 - train_size, random_state=random_state\n",
        "        )\n",
        "\n",
        "        print(f\"Data split:\\n- Training: {X_train.shape[0]} rows\\n- Test: {X_test.shape[0]} rows\")\n",
        "        return X_train, X_test, y_train, y_test\n",
        "    \n",
        "#------------------------Parameter Grid for Regression--------------------------------------------------------------\n",
        "\n",
        "    def _get_param_grids_regression_genetic(self):\n",
        "        return {\n",
        "            'LinearRegression': {\n",
        "                \"clf__copy_X\": Categorical([True, False]),\n",
        "                \"clf__fit_intercept\": Categorical([True, False]),\n",
        "                \"clf__positive\": Categorical([True, False])\n",
        "            },\n",
        "            'SVM': {\n",
        "                'clf__C': Continuous(0.1, 10.0),\n",
        "                'clf__kernel': Categorical(['linear', 'poly', 'rbf']),\n",
        "                'clf__gamma': Categorical(['scale', 'auto'])\n",
        "            },\n",
        "            'Ridge': {\n",
        "                'clf__alpha': Continuous(0.1, 10.0),\n",
        "                'clf__fit_intercept': Categorical([True, False]),\n",
        "                'clf__solver': Categorical(['auto', 'svd', 'cholesky'])\n",
        "            },\n",
        "            'DecisionTreeRegressor': {\n",
        "                'clf__max_depth': Integer(3, 10),\n",
        "                'clf__min_samples_split': Integer(2, 10),\n",
        "                'clf__min_samples_leaf': Integer(1, 5)\n",
        "            },\n",
        "            'RandomForestRegressor': {\n",
        "                'clf__n_estimators': Integer(50, 200),\n",
        "                'clf__max_depth': Integer(5, 15),\n",
        "                'clf__min_samples_split': Integer(2, 10)\n",
        "            },\n",
        "            'GradientBoostingRegressor': {\n",
        "                'clf__n_estimators': Integer(50, 200),\n",
        "                'clf__learning_rate': Continuous(0.01, 0.2),\n",
        "                'clf__max_depth': Integer(3, 10)\n",
        "            },\n",
        "            'XGBRegressor': {\n",
        "                'clf__learning_rate': Continuous(0.01, 0.2),\n",
        "                'clf__n_estimators': Integer(50, 200),\n",
        "                'clf__max_depth': Integer(3, 10),\n",
        "                'clf__subsample': Continuous(0.7, 1.0)\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def _get_param_grids_regression_exhaustive(self):\n",
        "        return {\n",
        "            'LinearRegression': {\n",
        "                \"clf__copy_X\": [True, False],\n",
        "                \"clf__fit_intercept\": [True, False],\n",
        "                \"clf__positive\": [True, False]\n",
        "            },\n",
        "            'SVM': {\n",
        "                'clf__C': [0.1, 1, 10],\n",
        "                'clf__kernel': ['linear', 'poly', 'rbf'],\n",
        "                'clf__gamma': ['scale', 'auto']\n",
        "            },\n",
        "            'Ridge': {\n",
        "                'clf__alpha': [0.1, 1.0, 10.0],\n",
        "                'clf__fit_intercept': [True, False],\n",
        "                'clf__solver': ['auto', 'svd', 'cholesky']\n",
        "            },\n",
        "            'DecisionTreeRegressor': {\n",
        "                'clf__max_depth': [3, 5, 7, 10],\n",
        "                'clf__min_samples_split': [2, 5, 10],\n",
        "                'clf__min_samples_leaf': [1, 2, 5]\n",
        "            },\n",
        "            'RandomForestRegressor': {\n",
        "                'clf__n_estimators': [50, 100, 200],\n",
        "                'clf__max_depth': [5, 10, 15],\n",
        "                'clf__min_samples_split': [2, 5, 10]\n",
        "            },\n",
        "            'GradientBoostingRegressor': {\n",
        "                'clf__n_estimators': [50, 100, 200],\n",
        "                'clf__learning_rate': [0.01, 0.1, 0.2],\n",
        "                'clf__max_depth': [3, 5, 10]\n",
        "            },\n",
        "            'XGBRegressor': {\n",
        "                'clf__learning_rate': [0.01, 0.1, 0.2],\n",
        "                'clf__n_estimators': [50, 100, 200],\n",
        "                'clf__max_depth': [3, 5, 10],\n",
        "                'clf__subsample': [0.7, 0.8, 1.0]\n",
        "            }\n",
        "        }\n",
        "\n",
        "#------------------------Parameter Grid for Classification--------------------------------------------------------------\n",
        "\n",
        "    def _get_param_grids_classification_genetic(self):\n",
        "        return {\n",
        "            'DecisionTreeClassifier': {\n",
        "                'clf__max_depth': Integer(3, 10),\n",
        "                'clf__min_samples_split': Integer(2, 10),\n",
        "                'clf__criterion': Categorical(['gini', 'entropy'])\n",
        "            },\n",
        "            'KNeighborsClassifier': {\n",
        "                'clf__n_neighbors': Integer(3, 15),\n",
        "                'clf__weights': Categorical(['uniform', 'distance']),\n",
        "                'clf__algorithm': Categorical(['auto', 'ball_tree', 'kd_tree'])\n",
        "            },\n",
        "            'RandomForestClassifier': {\n",
        "                'clf__n_estimators': Integer(50, 200),\n",
        "                'clf__max_depth': Integer(5, 15),\n",
        "                'clf__min_samples_split': Integer(2, 10)\n",
        "            },\n",
        "            'AdaBoostClassifier': {\n",
        "                'clf__n_estimators': Integer(50, 200),\n",
        "                'clf__learning_rate': Continuous(0.01, 0.2),\n",
        "                'clf__algorithm': Categorical(['SAMME'])\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def _get_param_grids_classification_exhaustive(self):\n",
        "        return {\n",
        "            'DecisionTreeClassifier': {\n",
        "                'clf__max_depth': [3, 5, 7, 10],\n",
        "                'clf__min_samples_split': [2, 5, 10],\n",
        "                'clf__criterion': ['gini', 'entropy']\n",
        "            },\n",
        "            'KNeighborsClassifier': {\n",
        "                'clf__n_neighbors': [3, 5, 7, 10, 15],\n",
        "                'clf__weights': ['uniform', 'distance'],\n",
        "                'clf__algorithm': ['auto', 'ball_tree', 'kd_tree']\n",
        "            },\n",
        "            'RandomForestClassifier': {\n",
        "                'clf__n_estimators': [50, 100, 200],\n",
        "                'clf__max_depth': [5, 10, 15],\n",
        "                'clf__min_samples_split': [2, 5, 10]\n",
        "            },\n",
        "            'AdaBoostClassifier': {\n",
        "                'clf__n_estimators': [50, 100, 200],\n",
        "                'clf__learning_rate': [0.01, 0.1, 0.2],\n",
        "                'clf__algorithm': ['SAMME']\n",
        "            }\n",
        "        }\n",
        "\n",
        "#------------------------Search Components--------------------------------------------------------------\n",
        "\n",
        "    def genetic_search(self, scoring_metric):\n",
        "        \"\"\"\n",
        "        Optimize models using genetic algorithms.\n",
        "        \n",
        "        Parameters:\n",
        "        - scoring_metric: str -> Metric to use for evaluation ('neg_root_mean_squared_error' for regression, 'roc_auc' for classification)\n",
        "        \"\"\"\n",
        "        if self.X_train is None or self.X_test is None:\n",
        "            print(\"Error: You must run split_df() before calling genetic_search().\")\n",
        "            return\n",
        "\n",
        "        results = {}\n",
        "        \n",
        "        # Feature selection based on current model type\n",
        "        if 'Regressor' in list(self.models.keys())[0] or list(self.models.keys())[0] in ['LinearRegression', 'Lasso', 'Ridge']:\n",
        "            # Regression feature selection\n",
        "            lasso_cv = LassoCV(cv=5) \n",
        "            lasso_cv.fit(self.X_train, self.y_train)\n",
        "            f_selection = SelectFromModel(lasso_cv)\n",
        "        else:\n",
        "            # Classification feature selection\n",
        "            model_base = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "            model_base.fit(self.X_train, self.y_train)\n",
        "            f_selection = SelectFromModel(model_base)\n",
        "\n",
        "        self.X_train = f_selection.transform(self.X_train)\n",
        "        self.X_test = f_selection.transform(self.X_test)\n",
        "\n",
        "        for name, model in self.models.items():\n",
        "            pl = Pipeline([\n",
        "              ('fs', f_selection), \n",
        "              ('clf', model), \n",
        "            ])            \n",
        "            print(f\"Training {name} with genetic method...\")\n",
        "            evolved_estimator = GASearchCV(\n",
        "                estimator=pl,\n",
        "                cv=5,\n",
        "                scoring=scoring_metric,\n",
        "                population_size=10,\n",
        "                generations=5,\n",
        "                tournament_size=3,\n",
        "                elitism=True,\n",
        "                crossover_probability=0.8,\n",
        "                mutation_probability=0.1,\n",
        "                param_grid=self.param_grids_genetic[name],\n",
        "                algorithm=\"eaSimple\",\n",
        "                n_jobs=-1,\n",
        "                error_score='raise',\n",
        "                verbose=True\n",
        "            )\n",
        "            evolved_estimator.fit(self.X_train, self.y_train)\n",
        "            results[name] = {\n",
        "                'best_params': evolved_estimator.best_params_,\n",
        "                'estimator': evolved_estimator.best_estimator_\n",
        "            }\n",
        "        return results\n",
        "\n",
        "    def exhaustive_search(self, scoring_metric):\n",
        "        \"\"\"\n",
        "        Perform exhaustive grid search for hyperparameter optimization.\n",
        "        \n",
        "        Parameters:\n",
        "        - scoring_metric: str -> Metric to use for evaluation ('neg_root_mean_squared_error' for regression, 'roc_auc' for classification)\n",
        "        \"\"\"\n",
        "        results = {}\n",
        "        \n",
        "        # Feature selection based on current model type\n",
        "        if 'Regressor' in list(self.models.keys())[0] or list(self.models.keys())[0] in ['LinearRegression', 'Lasso', 'Ridge']:\n",
        "            # Regression feature selection\n",
        "            lasso_cv = LassoCV(cv=5) \n",
        "            lasso_cv.fit(self.X_train, self.y_train)\n",
        "            f_selection = SelectFromModel(lasso_cv)\n",
        "        else:\n",
        "            # Classification feature selection\n",
        "            model_base = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "            model_base.fit(self.X_train, self.y_train)\n",
        "            f_selection = SelectFromModel(model_base)\n",
        "            \n",
        "        self.X_train = f_selection.transform(self.X_train)\n",
        "        self.X_test = f_selection.transform(self.X_test)\n",
        "        \n",
        "        for name, model in self.models.items():\n",
        "            pl = Pipeline([\n",
        "              ('clf', model), \n",
        "            ])\n",
        "            print(f\"Training {name} with exhaustive method...\")\n",
        "            grid_search = GridSearchCV(\n",
        "                estimator=pl,\n",
        "                param_grid=self.param_grids_exhaustive[name],\n",
        "                cv=5,\n",
        "                scoring=scoring_metric,\n",
        "                n_jobs=-1,\n",
        "                verbose=1\n",
        "            )\n",
        "            grid_search.fit(self.X_train, self.y_train)\n",
        "            results[name] = {\n",
        "                'best_params': grid_search.best_params_,\n",
        "                'estimator': grid_search.best_estimator_\n",
        "            }\n",
        "        return results\n",
        "    \n",
        "#------------------------Director Function--------------------------------------------------------------\n",
        "    def opti_director(self, target_column, problem_type='regression', method='both', random_state=42):\n",
        "        \"\"\"\n",
        "        This method orchestrates the optimization process for every model in this class.\n",
        "        1. Make the data split\n",
        "        2. Performs the optimization of models (genetic, exhaustive or both)\n",
        "        3. Extract the best parameters in a clean format to use them in the models.\n",
        "        \n",
        "        Parameters:\n",
        "        - target_column: str -> Name of the target column (y).\n",
        "        - problem_type: str -> Type of problem ('regression' or 'classification').\n",
        "        - method: str -> What optimization method is going to be used ('genetic', 'exhaustive', or 'both').\n",
        "        - random_state: int -> Random seed for reproducibility.\n",
        "        \n",
        "        Returns:\n",
        "        - dict -> Keeps the best parameters for each model.\n",
        "        \"\"\"\n",
        "        # Set up models and parameter grids based on problem type\n",
        "        if problem_type.lower() == 'regression':\n",
        "            self.models = self.regression_models\n",
        "            self.param_grids_genetic = self._get_param_grids_regression_genetic()\n",
        "            self.param_grids_exhaustive = self._get_param_grids_regression_exhaustive()\n",
        "            # RMSE for regression\n",
        "            scoring_metric = 'neg_root_mean_squared_error'  \n",
        "        elif problem_type.lower() == 'classification':\n",
        "            self.models = self.classification_models\n",
        "            self.param_grids_genetic = self._get_param_grids_classification_genetic()\n",
        "            self.param_grids_exhaustive = self._get_param_grids_classification_exhaustive()\n",
        "            # AUC for classification\n",
        "            scoring_metric = 'roc_auc'  \n",
        "        else:\n",
        "            raise ValueError(\"problem_type must be 'regression' or 'classification'\")\n",
        "        \n",
        "        # 1. Make the data split\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = self.split_df(\n",
        "            target_column=target_column,\n",
        "            random_state=random_state\n",
        "        )\n",
        "        \n",
        "        # 2. Performs the optimization of models (genetic, exhaustive or both)\n",
        "        best_params = {}\n",
        "        \n",
        "        if method.lower() == 'genetic' or method.lower() == 'both':\n",
        "            genetic_results = self.genetic_search(scoring_metric)\n",
        "            \n",
        "            # 3. Extract the best parameters in a clean format\n",
        "            clean_genetic_params = {}\n",
        "            for model_name, model_result in genetic_results.items():\n",
        "                best_params_model = model_result['best_params']\n",
        "                model_params = {param.replace('clf__', ''): value for param, value in best_params_model.items()}\n",
        "                clean_genetic_params[model_name] = model_params\n",
        "            \n",
        "            best_params['genetic'] = clean_genetic_params\n",
        "            \n",
        "        if method.lower() == 'exhaustive' or method.lower() == 'both':\n",
        "            exhaustive_results = self.exhaustive_search(scoring_metric)\n",
        "            \n",
        "            # 3. Extract the best parameters in a clean format\n",
        "            clean_exhaustive_params = {}\n",
        "            for model_name, model_result in exhaustive_results.items():\n",
        "                best_params_model = model_result['best_params']\n",
        "                model_params = {param.replace('clf__', ''): value for param, value in best_params_model.items()}\n",
        "                clean_exhaustive_params[model_name] = model_params\n",
        "            \n",
        "            best_params['exhaustive'] = clean_exhaustive_params\n",
        "            \n",
        "        return best_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Id  Year  Kilometers_Driven  Mileage  Engine   Power  Seats  Price  \\\n",
            "0   0  2010              72000    26.60   998.0   58.16    5.0   1.75   \n",
            "1   1  2015              41000    19.67  1582.0  126.20    5.0  12.50   \n",
            "2   2  2011              46000    18.20  1199.0   88.70    5.0   4.50   \n",
            "3   3  2012              87000    20.77  1248.0   88.76    7.0   6.00   \n",
            "4   4  2013              40670    15.20  1968.0  140.80    5.0  17.74   \n",
            "\n",
            "   Fuel_Type_CNG  Fuel_Type_Diesel  ...  Fuel_Type_LPG  Fuel_Type_Petrol  \\\n",
            "0              1                 0  ...              0                 0   \n",
            "1              0                 1  ...              0                 0   \n",
            "2              0                 0  ...              0                 1   \n",
            "3              0                 1  ...              0                 0   \n",
            "4              0                 1  ...              0                 0   \n",
            "\n",
            "   Owner_Type_First  Owner_Type_Fourth & Above  Owner_Type_Second  \\\n",
            "0                 1                          0                  0   \n",
            "1                 1                          0                  0   \n",
            "2                 1                          0                  0   \n",
            "3                 1                          0                  0   \n",
            "4                 0                          0                  1   \n",
            "\n",
            "   Owner_Type_Third  Mileage_Type_km/kg  Mileage_Type_kmpl  \\\n",
            "0                 0                   1                  0   \n",
            "1                 0                   0                  1   \n",
            "2                 0                   0                  1   \n",
            "3                 0                   0                  1   \n",
            "4                 0                   0                  1   \n",
            "\n",
            "   Transmission_Automatic  Transmission_Manual  \n",
            "0                       0                    1  \n",
            "1                       0                    1  \n",
            "2                       0                    1  \n",
            "3                       0                    1  \n",
            "4                       1                    0  \n",
            "\n",
            "[5 rows x 21 columns]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data split:\n",
            "- Training: 2056 rows\n",
            "- Test: 882 rows\n",
            "Training LinearRegression with genetic method...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\esteb\\Documents\\GitHub\\MineriaAvanzada\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2732: UserWarning: X has feature names, but SelectFromModel was fitted without feature names\n",
            "  warnings.warn(\n",
            "c:\\Users\\esteb\\Documents\\GitHub\\MineriaAvanzada\\.venv\\Lib\\site-packages\\sklearn\\feature_selection\\_base.py:122: UserWarning: No features were selected: either the data is too noisy or the selection test too strict.\n",
            "  warnings.warn(\n",
            "c:\\Users\\esteb\\Documents\\GitHub\\MineriaAvanzada\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2732: UserWarning: X has feature names, but SelectFromModel was fitted without feature names\n",
            "  warnings.warn(\n",
            "c:\\Users\\esteb\\Documents\\GitHub\\MineriaAvanzada\\.venv\\Lib\\site-packages\\sklearn\\feature_selection\\_base.py:122: UserWarning: No features were selected: either the data is too noisy or the selection test too strict.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Found array with 0 feature(s) (shape=(1645, 0)) while a minimum of 1 is required by LassoCV.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31m_RemoteTraceback\u001b[39m                          Traceback (most recent call last)",
            "\u001b[31m_RemoteTraceback\u001b[39m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\Users\\esteb\\Documents\\GitHub\\MineriaAvanzada\\.venv\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 463, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"c:\\Users\\esteb\\Documents\\GitHub\\MineriaAvanzada\\.venv\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\esteb\\Documents\\GitHub\\MineriaAvanzada\\.venv\\Lib\\site-packages\\joblib\\parallel.py\", line 598, in __call__\n    return [func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\esteb\\Documents\\GitHub\\MineriaAvanzada\\.venv\\Lib\\site-packages\\joblib\\parallel.py\", line 598, in <listcomp>\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\esteb\\Documents\\GitHub\\MineriaAvanzada\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 139, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\esteb\\Documents\\GitHub\\MineriaAvanzada\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\esteb\\Documents\\GitHub\\MineriaAvanzada\\.venv\\Lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\esteb\\Documents\\GitHub\\MineriaAvanzada\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py\", line 654, in fit\n    Xt = self._fit(X, y, routed_params, raw_params=params)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\esteb\\Documents\\GitHub\\MineriaAvanzada\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py\", line 588, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\esteb\\Documents\\GitHub\\MineriaAvanzada\\.venv\\Lib\\site-packages\\joblib\\memory.py\", line 312, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\esteb\\Documents\\GitHub\\MineriaAvanzada\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py\", line 1551, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\esteb\\Documents\\GitHub\\MineriaAvanzada\\.venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 319, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\esteb\\Documents\\GitHub\\MineriaAvanzada\\.venv\\Lib\\site-packages\\sklearn\\base.py\", line 921, in fit_transform\n    return self.fit(X, y, **fit_params).transform(X)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\esteb\\Documents\\GitHub\\MineriaAvanzada\\.venv\\Lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\esteb\\Documents\\GitHub\\MineriaAvanzada\\.venv\\Lib\\site-packages\\sklearn\\feature_selection\\_from_model.py\", line 372, in fit\n    self.estimator_.fit(X, y, **fit_params)\n  File \"c:\\Users\\esteb\\Documents\\GitHub\\MineriaAvanzada\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py\", line 2129, in fit\n    return super().fit(X, y, sample_weight=sample_weight, **params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\esteb\\Documents\\GitHub\\MineriaAvanzada\\.venv\\Lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\esteb\\Documents\\GitHub\\MineriaAvanzada\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py\", line 1627, in fit\n    X, y = validate_data(\n           ^^^^^^^^^^^^^^\n  File \"c:\\Users\\esteb\\Documents\\GitHub\\MineriaAvanzada\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 2956, in validate_data\n    X = check_array(X, input_name=\"X\", **check_X_params)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\esteb\\Documents\\GitHub\\MineriaAvanzada\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1139, in check_array\n    raise ValueError(\nValueError: Found array with 0 feature(s) (shape=(1645, 0)) while a minimum of 1 is required by LassoCV.\n\"\"\"",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(eda.head_df())\n\u001b[32m      6\u001b[39m optimizador = DataOptimization(eda)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m best_params = \u001b[43moptimizador\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopti_director\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_column\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPrice\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproblem_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mregression\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mboth\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m             \u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 376\u001b[39m, in \u001b[36mDataOptimization.opti_director\u001b[39m\u001b[34m(self, target_column, problem_type, method, random_state)\u001b[39m\n\u001b[32m    373\u001b[39m best_params = {}\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m method.lower() == \u001b[33m'\u001b[39m\u001b[33mgenetic\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m method.lower() == \u001b[33m'\u001b[39m\u001b[33mboth\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m     genetic_results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenetic_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscoring_metric\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m     \u001b[38;5;66;03m# 3. Extract the best parameters in a clean format\u001b[39;00m\n\u001b[32m    379\u001b[39m     clean_genetic_params = {}\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 282\u001b[39m, in \u001b[36mDataOptimization.genetic_search\u001b[39m\u001b[34m(self, scoring_metric)\u001b[39m\n\u001b[32m    265\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with genetic method...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    266\u001b[39m     evolved_estimator = GASearchCV(\n\u001b[32m    267\u001b[39m         estimator=pl,\n\u001b[32m    268\u001b[39m         cv=\u001b[32m5\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    280\u001b[39m         verbose=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    281\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[43mevolved_estimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m     results[name] = {\n\u001b[32m    284\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mbest_params\u001b[39m\u001b[33m'\u001b[39m: evolved_estimator.best_params_,\n\u001b[32m    285\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mestimator\u001b[39m\u001b[33m'\u001b[39m: evolved_estimator.best_estimator_\n\u001b[32m    286\u001b[39m     }\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\esteb\\Documents\\GitHub\\MineriaAvanzada\\.venv\\Lib\\site-packages\\sklearn_genetic\\genetic_search.py:548\u001b[39m, in \u001b[36mGASearchCV.fit\u001b[39m\u001b[34m(self, X, y, callbacks)\u001b[39m\n\u001b[32m    545\u001b[39m \u001b[38;5;28mself\u001b[39m._register()\n\u001b[32m    547\u001b[39m \u001b[38;5;66;03m# Optimization routine from the selected evolutionary algorithm\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m pop, log, n_gen = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_select_algorithm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstats\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhof\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_hof\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[38;5;66;03m# Update the _n_iterations value as the algorithm could stop earlier due a callback\u001b[39;00m\n\u001b[32m    551\u001b[39m \u001b[38;5;28mself\u001b[39m._n_iterations = n_gen\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\esteb\\Documents\\GitHub\\MineriaAvanzada\\.venv\\Lib\\site-packages\\sklearn_genetic\\genetic_search.py:628\u001b[39m, in \u001b[36mGASearchCV._select_algorithm\u001b[39m\u001b[34m(self, pop, stats, hof)\u001b[39m\n\u001b[32m    626\u001b[39m selected_algorithm = algorithms_factory.get(\u001b[38;5;28mself\u001b[39m.algorithm, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    627\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m selected_algorithm:\n\u001b[32m--> \u001b[39m\u001b[32m628\u001b[39m     pop, log, gen = \u001b[43mselected_algorithm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtoolbox\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmu\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpopulation_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlambda_\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpopulation_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcxpb\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcrossover_adapter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    635\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmutpb\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmutation_adapter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    636\u001b[39m \u001b[43m        \u001b[49m\u001b[43mngen\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    637\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhalloffame\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhof\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    638\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    639\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    640\u001b[39m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    641\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    644\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    645\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe algorithm \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.algorithm\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not supported, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    646\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mplease select one from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mAlgorithms.list()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    647\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\esteb\\Documents\\GitHub\\MineriaAvanzada\\.venv\\Lib\\site-packages\\sklearn_genetic\\algorithms.py:86\u001b[39m, in \u001b[36meaSimple\u001b[39m\u001b[34m(population, toolbox, cxpb, mutpb, ngen, stats, halloffame, callbacks, verbose, estimator, **kwargs)\u001b[39m\n\u001b[32m     84\u001b[39m invalid_ind = [ind \u001b[38;5;28;01mfor\u001b[39;00m ind \u001b[38;5;129;01min\u001b[39;00m population \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ind.fitness.valid]\n\u001b[32m     85\u001b[39m fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minvalid_ind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfitnesses\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m    \u001b[49m\u001b[43mind\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfitness\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfit\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m halloffame \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\esteb\\Documents\\GitHub\\MineriaAvanzada\\.venv\\Lib\\site-packages\\sklearn_genetic\\genetic_search.py:444\u001b[39m, in \u001b[36mGASearchCV.evaluate\u001b[39m\u001b[34m(self, individual)\u001b[39m\n\u001b[32m    441\u001b[39m local_estimator.set_params(**current_generation_params)\n\u001b[32m    443\u001b[39m \u001b[38;5;66;03m# Compute the cv-metrics\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m444\u001b[39m cv_results = \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mX_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43my_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    452\u001b[39m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    456\u001b[39m cv_scores = cv_results[\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtest_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.refit_metric\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m]\n\u001b[32m    457\u001b[39m score = np.mean(cv_scores)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\esteb\\Documents\\GitHub\\MineriaAvanzada\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\esteb\\Documents\\GitHub\\MineriaAvanzada\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:411\u001b[39m, in \u001b[36mcross_validate\u001b[39m\u001b[34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[32m    409\u001b[39m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[32m    410\u001b[39m parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)\n\u001b[32m--> \u001b[39m\u001b[32m411\u001b[39m results = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    418\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    419\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    420\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    422\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscore_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    423\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    431\u001b[39m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[32m    433\u001b[39m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[32m    434\u001b[39m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[32m    435\u001b[39m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\esteb\\Documents\\GitHub\\MineriaAvanzada\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     72\u001b[39m config = get_config()\n\u001b[32m     73\u001b[39m iterable_with_config = (\n\u001b[32m     74\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     76\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\esteb\\Documents\\GitHub\\MineriaAvanzada\\.venv\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2001\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2002\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2003\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2004\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2005\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2007\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\esteb\\Documents\\GitHub\\MineriaAvanzada\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1647\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1649\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1650\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1652\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1653\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1654\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1655\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1656\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\esteb\\Documents\\GitHub\\MineriaAvanzada\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1754\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._wait_retrieval():\n\u001b[32m   1748\u001b[39m \n\u001b[32m   1749\u001b[39m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[32m   1751\u001b[39m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[32m   1752\u001b[39m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[32m   1753\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._aborting:\n\u001b[32m-> \u001b[39m\u001b[32m1754\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1755\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1757\u001b[39m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[32m   1758\u001b[39m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\esteb\\Documents\\GitHub\\MineriaAvanzada\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1789\u001b[39m, in \u001b[36mParallel._raise_error_fast\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1785\u001b[39m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[32m   1786\u001b[39m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[32m   1787\u001b[39m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[32m   1788\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1789\u001b[39m     \u001b[43merror_job\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\esteb\\Documents\\GitHub\\MineriaAvanzada\\.venv\\Lib\\site-packages\\joblib\\parallel.py:745\u001b[39m, in \u001b[36mBatchCompletionCallBack.get_result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    739\u001b[39m backend = \u001b[38;5;28mself\u001b[39m.parallel._backend\n\u001b[32m    741\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m backend.supports_retrieve_callback:\n\u001b[32m    742\u001b[39m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[32m    743\u001b[39m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[32m    744\u001b[39m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    747\u001b[39m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\esteb\\Documents\\GitHub\\MineriaAvanzada\\.venv\\Lib\\site-packages\\joblib\\parallel.py:763\u001b[39m, in \u001b[36mBatchCompletionCallBack._return_or_raise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    761\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    762\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.status == TASK_ERROR:\n\u001b[32m--> \u001b[39m\u001b[32m763\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n\u001b[32m    764\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n\u001b[32m    765\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
            "\u001b[31mValueError\u001b[39m: Found array with 0 feature(s) (shape=(1645, 0)) while a minimum of 1 is required by LassoCV."
          ]
        }
      ],
      "source": [
        "# MI TESTING\n",
        "#Recuerda el input arriba en VSCode\n",
        "archivo_csv = \"../dataset/dataset.csv\"\n",
        "eda = EDA(file=archivo_csv)\n",
        "print(eda.head_df())\n",
        "optimizador = DataOptimization(eda)\n",
        "\n",
        "best_params = optimizador.opti_director(\n",
        "    target_column='Price',\n",
        "    problem_type='regression',\n",
        "    method='both',             \n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Models\n",
        "\n",
        "## Unsupervised"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "DHJDmyoocftY"
      },
      "outputs": [],
      "source": [
        "class NoSupervisado(EDA):\n",
        "    def __init__(self, datos_eda):\n",
        "        # La clase ya utiliza el df procesado en la clase EDA\n",
        "        df = datos_eda.get_df()\n",
        "        super().__init__()\n",
        "        self.__df = df\n",
        "\n",
        "    @property\n",
        "    def df(self):\n",
        "        return self.__df\n",
        "\n",
        "    @df.setter\n",
        "    def df(self, p_df):\n",
        "        self.__df = p_df\n",
        "\n",
        "    def __byebye_object_values(self):\n",
        "        # Elimina columnas de tipo 'object'\n",
        "        self.__df = self.__df.select_dtypes(exclude=['object'])\n",
        "\n",
        "    def calcular_metricas(self, labels):\n",
        "        \"\"\"\n",
        "        Calcula métricas de evaluación para clustering.\n",
        "        \"\"\"\n",
        "        data = self.__df.dropna()\n",
        "        data = (data - data.mean()) / data.std()\n",
        "        metrics = {\n",
        "            \"Índice de Silueta\": silhouette_score(data, labels),\n",
        "            \"Calinski-Harabasz\": calinski_harabasz_score(data, labels),\n",
        "            \"Davies-Bouldin\": davies_bouldin_score(data, labels)\n",
        "        }\n",
        "        return metrics\n",
        "\n",
        "    def kmeans(self, n_clusters):\n",
        "        self.__byebye_object_values()\n",
        "        data = self.__df\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "        labels = kmeans.fit_predict(data)\n",
        "        metrics = self.calcular_metricas(labels)\n",
        "        print(f\"Métricas para K-Means (n_clusters={n_clusters}): {metrics}\")\n",
        "        return metrics\n",
        "\n",
        "    def k_medoids(self, n_clusters, metric='euclidean'):\n",
        "        self.__byebye_object_values()\n",
        "        data = self.__df\n",
        "        \n",
        "        # Convertir a numpy array si aún no lo es\n",
        "        data_array = np.array(data)\n",
        "        \n",
        "        # Inicialización de medoides (seleccionar índices aleatorios)\n",
        "        np.random.seed(42)  # Para reproducibilidad\n",
        "        initial_medoids = np.random.choice(len(data_array), n_clusters, replace=False).tolist()\n",
        "        \n",
        "        # Crear y ejecutar el algoritmo KMedoids\n",
        "        kmedoids_instance = kmedoids(data_array, initial_medoids)\n",
        "        kmedoids_instance.process()\n",
        "        \n",
        "        # Obtener clusters y medoides\n",
        "        clusters = kmedoids_instance.get_clusters()  # Lista de listas de índices\n",
        "        medoids = kmedoids_instance.get_medoids()    # Lista de índices de medoides\n",
        "        \n",
        "        # Crear etiquetas en formato sklearn (un número para cada punto)\n",
        "        labels = np.zeros(len(data_array), dtype=int)\n",
        "        for cluster_idx, cluster in enumerate(clusters):\n",
        "            for point_idx in cluster:\n",
        "                labels[point_idx] = cluster_idx\n",
        "        \n",
        "        # Calcular métricas\n",
        "        metrics = self.calcular_metricas(labels)\n",
        "        print(f\"Métricas para K-Medoids (n_clusters={n_clusters}, metric={metric}): {metrics}\")\n",
        "        \n",
        "        return metrics\n",
        "\n",
        "    def hac(self, n_clusters=3, method='ward'):\n",
        "        self.__byebye_object_values()\n",
        "        data = self.__df\n",
        "        linkage_matrix = linkage(data, method=method)\n",
        "        labels = fcluster(linkage_matrix, t=n_clusters, criterion='maxclust')\n",
        "        metrics = self.calcular_metricas(labels)\n",
        "        print(f\"Métricas para HAC (n_clusters={n_clusters}, method={method}): {metrics}\")\n",
        "        return metrics\n",
        "\n",
        "    def umap_model(self, n_components=2, n_neighbors=15):\n",
        "        self.__byebye_object_values()\n",
        "        data = self.__df\n",
        "        modelo_umap = UMAP(n_components=n_components, n_neighbors=n_neighbors)\n",
        "        components = modelo_umap.fit_transform(data)\n",
        "        kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "        labels = kmeans.fit_predict(components)\n",
        "        metrics = self.calcular_metricas(labels)\n",
        "        print(f\"Métricas para UMAP (n_components={n_components}, n_neighbors={n_neighbors}): {metrics}\")\n",
        "        return metrics\n",
        "\n",
        "    def comparar_algoritmos(self, n_clusters):\n",
        "\n",
        "        if self.__df.isnull().any().any():\n",
        "          print(\"El DataFrame contiene valores nulos. Se eliminarán automáticamente para continuar.\")\n",
        "          self.__df.dropna(inplace=True)\n",
        "\n",
        "        print(\"\\nEjecutando K-Means...\")\n",
        "        kmeans_metrics = self.kmeans(n_clusters)\n",
        "\n",
        "        print(\"\\nEjecutando K-Medoids...\")\n",
        "        kmedoids_metrics = self.k_medoids(n_clusters)\n",
        "\n",
        "        print(\"\\nEjecutando HAC...\")\n",
        "        hac_metrics = self.hac(n_clusters=n_clusters)\n",
        "\n",
        "        print(\"\\nEjecutando UMAP...\")\n",
        "        umap_metrics = self.umap_model(n_components=2, n_neighbors=15)\n",
        "\n",
        "        resultados = pd.DataFrame({\n",
        "            \"K-Means\": kmeans_metrics,\n",
        "            \"K-Medoids\": kmedoids_metrics,\n",
        "            \"HAC\": hac_metrics,\n",
        "            \"UMAP\": umap_metrics\n",
        "        }).T\n",
        "\n",
        "        print(\"\\nComparación de Algoritmos:\")\n",
        "        print(resultados)\n",
        "        return resultados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Supervised "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grQ-5IXLjR-7"
      },
      "outputs": [],
      "source": [
        "class Supervisado:\n",
        "    def __init__(self, data_optimization):\n",
        "        \"\"\"\n",
        "        Initialize the Supervisado class with data and optimized parameters from DataOptimization\n",
        "        \n",
        "        Parameters:\n",
        "        - data_optimization: Instance of DataOptimization with optimized models and data\n",
        "        \"\"\"\n",
        "        # Get the processed dataframe\n",
        "        self.__df = data_optimization.X_train  # Store a reference to the training data\n",
        "        \n",
        "        # Store the data splits from DataOptimization\n",
        "        self.X_train = data_optimization.X_train\n",
        "        self.X_test = data_optimization.X_test\n",
        "        self.y_train = data_optimization.y_train\n",
        "        self.y_test = data_optimization.y_test\n",
        "        \n",
        "        # Store the best parameters from optimization if available\n",
        "        self.best_params = None\n",
        "        \n",
        "        # Problem type (regression or classification)\n",
        "        self.problem_type = 'regression' if 'Regressor' in list(data_optimization.models.keys())[0] or list(data_optimization.models.keys())[0] in ['LinearRegression', 'Lasso', 'Ridge'] else 'classification'\n",
        "\n",
        "    @property\n",
        "    def df(self):\n",
        "        return self.__df\n",
        "\n",
        "    @df.setter\n",
        "    def df(self, p_df):\n",
        "        self.__df = p_df\n",
        "\n",
        "    def set_optimized_parameters(self, best_params, method='genetic'):\n",
        "        \"\"\"\n",
        "        Set the best parameters from the optimization process\n",
        "        \n",
        "        Parameters:\n",
        "        - best_params: Dictionary with best parameters for each model\n",
        "        - method: Which optimization method to use ('genetic' or 'exhaustive')\n",
        "        \"\"\"\n",
        "        if method in best_params:\n",
        "            self.best_params = best_params[method]\n",
        "            print(f\"Using {method} optimization parameters\")\n",
        "        else:\n",
        "            print(f\"Warning: {method} parameters not found. Using default parameters.\")\n",
        "            self.best_params = {}\n",
        "\n",
        "#-----------------Evaluacion de modelos----------------------------\n",
        "    def calcular_metricas(self, modelo, X_test, y_test, predicciones, modelo_nombre):\n",
        "      \"\"\"\n",
        "      Calculate the model evaluation metrics and save the results in a dictionary.\n",
        "\n",
        "      Parameters:\n",
        "      - modelo: The model in use\n",
        "      - X_test: Test features\n",
        "      - y_test: Test labels\n",
        "      - predicciones: Model predictions\n",
        "      - modelo_nombre: Model name\n",
        "\n",
        "      Returns:\n",
        "      - resultados: Dictionary containing evaluation metrics.\n",
        "      \"\"\"\n",
        "\n",
        "      mse = mean_squared_error(y_test, predicciones)\n",
        "      r2 = r2_score(y_test, predicciones)\n",
        "      mae = mean_absolute_error(y_test, predicciones)\n",
        "      rmse = np.sqrt(mse)\n",
        "      tolerancia = 0.1  # 10% of tolerance\n",
        "      precision_global = np.mean(np.abs(y_test - predicciones) <= (tolerancia * y_test)) * 100\n",
        "\n",
        "      resultados = {\n",
        "          'modelo': modelo_nombre,\n",
        "          'MSE': mse,\n",
        "          'R2': r2,\n",
        "          'MAE': mae,\n",
        "          'RMSE': rmse,\n",
        "          'precision_global': precision_global,\n",
        "          #'predicciones': predicciones.tolist(),\n",
        "          #'valores_reales': y_test.tolist()\n",
        "      }\n",
        "      return resultados\n",
        "\n",
        "    def calcular_metricas_clasificacion(self, modelo, X_test, y_test, predicciones, modelo_nombre):\n",
        "      \"\"\"\n",
        "      Calculate evaluation metrics for classification models and store the results in a dictionary.\n",
        "\n",
        "      Parameters:\n",
        "      - y_test: True labels of the test dataset.\n",
        "      - predicciones: Predicted labels from the classification model.\n",
        "      - modelo_nombre: Name or identifier of the evaluated model.\n",
        "\n",
        "      Returns:\n",
        "      - resultados: Dictionary containing evaluation metrics.\n",
        "      \"\"\"\n",
        "      from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "      accuracy = accuracy_score(y_test, predicciones)\n",
        "      precision = precision_score(y_test, predicciones, average='weighted')\n",
        "      recall = recall_score(y_test, predicciones, average='weighted')\n",
        "      f1 = f1_score(y_test, predicciones, average='weighted')\n",
        "\n",
        "      resultados = {\n",
        "          'modelo': modelo_nombre,\n",
        "          'accuracy': accuracy,\n",
        "          'precision': precision,\n",
        "          'recall': recall,\n",
        "          'f1_score': f1\n",
        "      }\n",
        "      return resultados\n",
        "\n",
        "#------------------------Regression Models--------------------------------------------------------------\n",
        "\n",
        "    def regre_lineal_simple(self, X_train, X_test, y_train, y_test):\n",
        "      \"\"\"\n",
        "      Performs Simple Linear Regression and computes multiple performance metrics.\n",
        "\n",
        "      Parameters:\n",
        "      - X_train: Features used for training the regression model.\n",
        "      - y_train: Target variable used for training.\n",
        "      - X_test: Features used to evaluate the regression model.\n",
        "      - y_test: Actual target values to compare against predictions.\n",
        "      - modelo_nombre: Name or identifier of the evaluated regression model.\n",
        "\n",
        "      Returns: Dictionary containing regression performance metrics (model name, MSE, RMSE, MAE, R² score).\n",
        "      \"\"\"\n",
        "      print(\"Starting Simple Linear Regression...\")\n",
        "      # Get optimized parameters if available\n",
        "      params = self.best_params.get('LinearRegression', {}) if self.best_params else {}\n",
        "      \n",
        "      # Create model with optimized parameters\n",
        "      modelo = LinearRegression(**params)\n",
        "      modelo.fit(self.X_train, self.y_train)\n",
        "      predicciones = modelo.predict(self.X_test)\n",
        "      return self.calcular_metricas(modelo, X_test, y_test, predicciones, 'Regresión Lineal Simple')\n",
        "\n",
        "    def regre_svm(self, X_train, X_test, y_train, y_test):\n",
        "      \"\"\"\n",
        "      Realiza una Support Vector Machine y calcula múltiples métricas de rendimiento.\n",
        "      Returns:\n",
        "      - resultados: Diccionario con métricas de rendimiento del modelo\n",
        "      \"\"\"\n",
        "      print(\"Iniciando Support Vector Machine (SVM)...\")\n",
        "      from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "      # Escalar los datos\n",
        "      scaler = StandardScaler()\n",
        "      X_train_scaled = scaler.fit_transform(X_train)\n",
        "      X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "      modelo = SVR(kernel='rbf', C=1.0, gamma='scale')\n",
        "      modelo.fit(X_train_scaled, y_train)\n",
        "      predicciones = modelo.predict(X_test_scaled)\n",
        "\n",
        "      return self.calcular_metricas(modelo, X_test_scaled, y_test, predicciones, 'Support Vector Machine')\n",
        "\n",
        "\n",
        "    def regre_regridge(self, X_train, X_test, y_train, y_test):\n",
        "      \"\"\"\n",
        "      Realiza un Regresión Ridge y calcula múltiples métricas de rendimiento.\n",
        "      Returns:\n",
        "      - resultados: Diccionario con métricas de rendimiento del modelo\n",
        "      \"\"\"\n",
        "      print(\"Iniciando Regresión Ridge...\")\n",
        "      modelo = Ridge(alpha = 1.0)\n",
        "      modelo.fit(X_train, y_train)\n",
        "      predicciones = modelo.predict(X_test)\n",
        "      return self.calcular_metricas(modelo, X_test, y_test, predicciones, 'Regresión Ridge')\n",
        "\n",
        "    def regre_decisionTree(self, X_train, X_test, y_train, y_test):\n",
        "      \"\"\"\n",
        "      Realiza un Decision Tree Regressor y calcula múltiples métricas de rendimiento.\n",
        "      Returns:\n",
        "      - resultados: Diccionario con métricas de rendimiento del modelo\n",
        "      \"\"\"\n",
        "      print(\"Iniciando DecisionTreeRegressor..\")\n",
        "      modelo = DecisionTreeRegressor(random_state=0)\n",
        "      modelo.fit(X_train, y_train)\n",
        "      predicciones = modelo.predict(X_test)\n",
        "      return self.calcular_metricas(modelo, X_test, y_test, predicciones, 'Decision Tree Regressor')\n",
        "\n",
        "    def regre_randomforest(self, X_train, X_test, y_train, y_test):\n",
        "      \"\"\"\n",
        "      Realiza un Random Forest Regressor y calcula múltiples métricas de rendimiento.\n",
        "      Returns:\n",
        "      - resultados: Diccionario con métricas de rendimiento del modelo\n",
        "      \"\"\"\n",
        "      print(\"Iniciando RandomForest Regressor..\")\n",
        "      modelo = RandomForestRegressor(max_depth=2, random_state=0)\n",
        "      modelo.fit(X_train, y_train)\n",
        "      predicciones = modelo.predict(X_test)\n",
        "      return self.calcular_metricas(modelo, X_test, y_test, predicciones, 'Random Forest Regressor')\n",
        "\n",
        "    def regre_gradient_boosting(self, X_train, X_test, y_train, y_test):\n",
        "      \"\"\"\n",
        "      Realiza un Grandient Boostsing Regressor y calcula múltiples métricas de rendimiento.\n",
        "      Returns:\n",
        "      - resultados: Diccionario con métricas de rendimiento del modelo\n",
        "      \"\"\"\n",
        "      print(\"Iniciando Grandient Boostsing Regressor..\")\n",
        "      modelo = GradientBoostingRegressor(random_state=0)\n",
        "      modelo.fit(X_train, y_train)\n",
        "      predicciones = modelo.predict(X_test)\n",
        "      return self.calcular_metricas(modelo, X_test, y_test, predicciones, 'Grandient Boostsing Regressor')\n",
        "\n",
        "    def regre_xgboost(self, X_train, X_test, y_train, y_test):\n",
        "      \"\"\"\n",
        "      Realiza un XGBoost Regressor y calcula múltiples métricas de rendimiento.\n",
        "      Returns:\n",
        "      - resultados: Diccionario con métricas de rendimiento del modelo\n",
        "      \"\"\"\n",
        "      print(\"Iniciando XGBoost Regressor..\")\n",
        "      modelo = XGBRegressor(random_state=0)\n",
        "      modelo.fit(X_train, y_train)\n",
        "      predicciones = modelo.predict(X_test)\n",
        "      return self.calcular_metricas(modelo, X_test, y_test, predicciones, 'XGBoost Regressor')\n",
        "\n",
        "#------------------------Classification Models--------------------------------------------------------------\n",
        "\n",
        "    def classi_decision_tree(self, X_train, X_test, y_train, y_test):\n",
        "      \"\"\"\n",
        "      Realiza un modelo de clasificación usando Árbol de Decisión y calcula métricas de rendimiento.\n",
        "      \"\"\"\n",
        "      print(\"Iniciando Decision Tree Classifier...\")\n",
        "      from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "      modelo = DecisionTreeClassifier(random_state=0)\n",
        "      modelo.fit(X_train, y_train)\n",
        "      predicciones = modelo.predict(X_test)\n",
        "\n",
        "      return self.calcular_metricas_clasificacion(modelo, X_test, y_test, predicciones, 'Decision Tree')\n",
        "\n",
        "    def classi_knn(self, X_train, X_test, y_train, y_test):\n",
        "      \"\"\"\n",
        "      Realiza un modelo de clasificación usando K-Nearest Neighbors y calcula métricas de rendimiento.\n",
        "      \"\"\"\n",
        "      print(\"Iniciando K-Nearest Neighbors Classifier...\")\n",
        "      from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "      modelo = KNeighborsClassifier()\n",
        "      modelo.fit(X_train, y_train)\n",
        "      predicciones = modelo.predict(X_test)\n",
        "\n",
        "      return self.calcular_metricas_clasificacion(modelo, X_test, y_test, predicciones, 'K-Nearest Neighbors')\n",
        "\n",
        "    def classi_random_forest(self, X_train, X_test, y_train, y_test):\n",
        "      \"\"\"\n",
        "      Realiza un modelo de clasificación usando Random Forest y calcula métricas de rendimiento.\n",
        "      \"\"\"\n",
        "      print(\"Iniciando Random Forest Classifier...\")\n",
        "      from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "      modelo = RandomForestClassifier(random_state=0)\n",
        "      modelo.fit(X_train, y_train)\n",
        "      predicciones = modelo.predict(X_test)\n",
        "\n",
        "      return self.calcular_metricas_clasificacion(modelo, X_test, y_test, predicciones, 'Random Forest')\n",
        "\n",
        "    def classi_adaboost(self, X_train, X_test, y_train, y_test):\n",
        "      \"\"\"\n",
        "      Realiza un modelo de clasificación usando AdaBoost y calcula métricas de rendimiento.\n",
        "      \"\"\"\n",
        "      print(\"Iniciando AdaBoost Classifier...\")\n",
        "      from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "      modelo = AdaBoostClassifier(random_state=0)\n",
        "      modelo.fit(X_train, y_train)\n",
        "      predicciones = modelo.predict(X_test)\n",
        "\n",
        "      return self.calcular_metricas_clasificacion(modelo, X_test, y_test, predicciones, 'AdaBoost')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUAM4Lsdna5a"
      },
      "outputs": [],
      "source": [
        "class Start:\n",
        "    def __init__(self):\n",
        "        self.eda = None\n",
        "        self.supervisado = None\n",
        "        self.no_supervisado = None\n",
        "        self.split_data = None\n",
        "\n",
        "\n",
        "    def mostrar_menu(self):\n",
        "        while True:\n",
        "            print(\"\\n--- Menú Principal ---\")\n",
        "            print(\"1. 📁 Carga de datos en formato CSV y completar EDA\")\n",
        "            print(\"2. 🪐 Ejecutar modelo\")\n",
        "            print(\"3. 🛑 Salir\")\n",
        "            opcion = input(\"Seleccione una opción: \")\n",
        "\n",
        "            if opcion == \"1\":\n",
        "                self.datos_eda()\n",
        "            elif opcion == \"2\":\n",
        "                self.models_menu()\n",
        "            elif opcion == \"3\":\n",
        "                print(\"Saliendo del programa...\")\n",
        "                break\n",
        "            else:\n",
        "                print(\"Opción no válida. Intente de nuevo.\")\n",
        "\n",
        "    def models_menu(self):\n",
        "      while True:\n",
        "        print(\"\\n--- ¿Qué problema necesita resolver? ---\")\n",
        "        print(\"1. 🔍 Clasificación: Asigne etiquetas a sus datos\")\n",
        "        print(\"2. 📈 Regresión: Prediga valores continuos\")\n",
        "        print(\"3. 🧩 Aprendizaje No Supervisado: Descubra patrones ocultos\")\n",
        "        print(\"4. 🛑 Volver al menú principal\")\n",
        "        opcion = input(\"Seleccione una opción: \")\n",
        "\n",
        "        # Resetear split_data antes de cambiar de modelo\n",
        "        self.split_data = None\n",
        "        self.supervisado = None\n",
        "\n",
        "        if opcion == \"1\":\n",
        "            self.classi_modelos()\n",
        "        elif opcion == \"2\":\n",
        "            self.regre_modelos()\n",
        "        elif opcion == \"3\":\n",
        "                if self.eda and not self.eda.get_df().empty:\n",
        "                  self.no_supervisado = NoSupervisado(self.eda)\n",
        "                  n_clusters = int(input(\"Ingrese el número de clusters: \"))\n",
        "                  self.no_supervisado.comparar_algoritmos(n_clusters=n_clusters)\n",
        "                else:\n",
        "                    print(\"Primero cargue los datos para poder realizar aprendizaje no supervisado.\")\n",
        "        elif opcion == \"4\":\n",
        "            print(\"Saliendo del programa...\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"Opción no válida. Intente de nuevo.\")\n",
        "\n",
        "    def datos_eda(self):\n",
        "        while True:\n",
        "            print(\"\\n ----EDA----\")\n",
        "            print(\"1. 📂 Carga de datos\")\n",
        "            print(\"2. 🔍 Mostrar head del DataFrame\")\n",
        "            print(\"3. 📊 Revisar los tipos de datos\")\n",
        "            print(\"4. ✂️ Eliminar columnas\")\n",
        "            print(\"5. 🧹 Eliminar valores NULOS\")\n",
        "            print(\"6. ⚠️ Detectar valores atipicos\")\n",
        "            print(\"7. 📈 Graficar relación entre dos variables\")\n",
        "            print(\"8. 📉 Graficar histograma\")\n",
        "            print(\"9. 🌡 HeatMap: Generar mapa de calor\")\n",
        "            print(\"0. 🛑 Volver al menú principal\")\n",
        "            opcion = input(\"Seleccione una opción: \")\n",
        "\n",
        "            if opcion == \"1\":\n",
        "                my_data = input(\"¿Cómo se llama el CSV? \")\n",
        "                try:\n",
        "                    self.eda = EDA(file=my_data)\n",
        "                    print(\"Instancia de EDA creada y datos cargados exitosamente.\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error al cargar los datos: {e}\")\n",
        "            elif opcion == \"2\":\n",
        "                if self.eda:\n",
        "                    print(self.eda.head_df())\n",
        "                else:\n",
        "                    print(\"Primero cargue los datos.\")\n",
        "            elif opcion == \"3\":\n",
        "                if self.eda:\n",
        "                    print(self.eda.check_data_types())\n",
        "                else:\n",
        "                    print(\"Primero cargue los datos.\")\n",
        "            elif opcion == \"4\":\n",
        "                if self.eda:\n",
        "                    columnas = input(\"Ingrese los nombres de las columnas a eliminar, separadas por comas: \").split(',')\n",
        "                    columnas = [col.strip() for col in columnas]\n",
        "                    try:\n",
        "                        self.eda.drop_irrelevant_columns(columnas)\n",
        "                        print(f\"Columnas eliminadas: {', '.join(columnas)}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error al eliminar columnas: {e}\")\n",
        "                else:\n",
        "                    print(\"Primero cargue los datos.\")\n",
        "            elif opcion == \"5\":\n",
        "                if self.eda:\n",
        "                    self.eda.drop_missing_values()\n",
        "                    print(\"Valores nulos eliminados.\")\n",
        "                else:\n",
        "                    print(\"Primero cargue los datos.\")\n",
        "            elif opcion == \"6\":\n",
        "                if self.eda:\n",
        "                    print(self.eda.detect_outliers())\n",
        "                else:\n",
        "                    print(\"Primero cargue los datos.\")\n",
        "            elif opcion == \"7\":\n",
        "                if self.eda:\n",
        "                    print(\"\\n ***Variables disponibles***\")\n",
        "                    print(self.eda.check_data_types())\n",
        "                    col1 = input(\"Ingrese el nombre de la primera variable: \")\n",
        "                    col2 = input(\"Ingrese el nombre de la segunda variable: \")\n",
        "                    try:\n",
        "                        self.eda.plot_scatter(col1, col2)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error al graficar: {e}\")\n",
        "                        break\n",
        "                else:\n",
        "                    print(\"Primero cargue los datos.\")\n",
        "            elif opcion == \"8\":\n",
        "                if self.eda:\n",
        "                    print(\"\\n ***Variables disponibles***\")\n",
        "                    print(self.eda.check_data_types())\n",
        "                    histogram_col = input(\"Ingrese el nombre de una variable a graficar: \")\n",
        "                    try:\n",
        "                        self.eda.plot_histogram(histogram_col)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error al graficar: {e}\")\n",
        "                else:\n",
        "                    print(\"Primero cargue los datos.\")\n",
        "            elif opcion == \"9\":\n",
        "                if self.eda:\n",
        "                    self.eda.plot_heatmap()\n",
        "                    print(\"El programa se detendrá después de mostrar el gráfico.\")\n",
        "                    exit()\n",
        "                else:\n",
        "                    print(\"Primero cargue los datos.\")\n",
        "            elif opcion == \"0\":\n",
        "                break\n",
        "            else:\n",
        "                print(\"Opción no válida. Intente de nuevo.\")\n",
        "\n",
        "    def regre_modelos(self):\n",
        "      if self.supervisado is None:\n",
        "          if self.eda:\n",
        "              self.supervisado = Supervisado(self.eda)\n",
        "          else:\n",
        "              print(\"Primero debe cargar los datos\")\n",
        "              return\n",
        "\n",
        "      if self.split_data is None:\n",
        "          print(\"\\n ***Variables disponibles***\")\n",
        "          print(self.eda.check_data_types())\n",
        "          target_column = input(\"\\n Ingrese el nombre de la columna objetivo: \")\n",
        "          self.split_data = self.supervisado.split_df(target_column)\n",
        "\n",
        "      X_train, X_test, y_train, y_test = self.split_data\n",
        "\n",
        "      modelos = [\n",
        "          self.supervisado.regre_lineal_simple,\n",
        "          self.supervisado.regre_svm,\n",
        "          self.supervisado.regre_regridge,\n",
        "          self.supervisado.regre_decisionTree,\n",
        "          self.supervisado.regre_randomforest,\n",
        "          self.supervisado.regre_gradient_boosting,\n",
        "          self.supervisado.regre_xgboost\n",
        "      ]\n",
        "\n",
        "      resultados = []\n",
        "      for modelo in modelos:\n",
        "          resultados.append(modelo(X_train, X_test, y_train, y_test))\n",
        "\n",
        "      print(\"\\n--- Resultados del Benchmarking ---\")\n",
        "      for resultado in resultados:\n",
        "          print(f\"{resultado['modelo']}: R2={resultado['R2']:.4f}, RMSE={resultado['RMSE']:.4f}, MAE={resultado['MAE']:.4f}\")\n",
        "\n",
        "      # Opción de graficar resultados\n",
        "      graficar = input(\"\\n¿Desea graficar los resultados? (S/N): \").strip().upper()\n",
        "      if graficar == 'S':\n",
        "        # Preparar datos para la gráfica\n",
        "        nombres_modelos = [resultado['modelo'] for resultado in resultados]\n",
        "        r2_scores = [resultado['R2'] for resultado in resultados]\n",
        "        rmse_scores = [resultado['RMSE'] for resultado in resultados]\n",
        "        mae_scores = [resultado['MAE'] for resultado in resultados]\n",
        "\n",
        "        # Crear la gráfica de barras comparativa\n",
        "        plt.figure(figsize=(15, 6))\n",
        "\n",
        "        # Gráfica de R2\n",
        "        plt.subplot(1, 3, 1)\n",
        "        bars1 = plt.bar(nombres_modelos, r2_scores)\n",
        "        plt.title('R2 Scores')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.ylabel('R2')\n",
        "        # Añadir valores en las barras\n",
        "        for bar in bars1:\n",
        "            height = bar.get_height()\n",
        "            plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                     f'{height:.4f}',\n",
        "                     ha='center', va='bottom', rotation=0)\n",
        "\n",
        "        # Gráfica de RMSE\n",
        "        plt.subplot(1, 3, 2)\n",
        "        bars2 = plt.bar(nombres_modelos, rmse_scores)\n",
        "        plt.title('Root Mean Squared Error')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.ylabel('RMSE')\n",
        "        # Añadir valores en las barras\n",
        "        for bar in bars2:\n",
        "            height = bar.get_height()\n",
        "            plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                     f'{height:.4f}',\n",
        "                     ha='center', va='bottom', rotation=0)\n",
        "\n",
        "        # Gráfica de MAE\n",
        "        plt.subplot(1, 3, 3)\n",
        "        bars3 = plt.bar(nombres_modelos, mae_scores)\n",
        "        plt.title('Mean Absolute Error')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.ylabel('MAE')\n",
        "        # Añadir valores en las barras\n",
        "        for bar in bars3:\n",
        "            height = bar.get_height()\n",
        "            plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                     f'{height:.4f}',\n",
        "                     ha='center', va='bottom', rotation=0)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        exit()\n",
        "\n",
        "    def classi_modelos(self):\n",
        "        if graficar == 'S':\n",
        "          # Preparar datos para la gráfica\n",
        "          nombres_modelos = [resultado['modelo'] for resultado in resultados]\n",
        "          accuracy_scores = [resultado['accuracy'] for resultado in resultados]\n",
        "          precision_scores = [resultado['precision'] for resultado in resultados]\n",
        "          recall_scores = [resultado['recall'] for resultado in resultados]\n",
        "          f1_scores = [resultado['f1_score'] for resultado in resultados]\n",
        "\n",
        "          # Crear la gráfica de barras comparativa\n",
        "          plt.figure(figsize=(15, 6))\n",
        "\n",
        "          # Gráfica de Accuracy\n",
        "          plt.subplot(1, 4, 1)\n",
        "          bars1 = plt.bar(nombres_modelos, accuracy_scores)\n",
        "          plt.title('Accuracy')\n",
        "          plt.xticks(rotation=45, ha='right')\n",
        "          plt.ylabel('Accuracy')\n",
        "          # Añadir valores en las barras\n",
        "          for bar in bars1:\n",
        "              height = bar.get_height()\n",
        "              plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                      f'{height:.4f}',\n",
        "                      ha='center', va='bottom', rotation=0)\n",
        "\n",
        "          # Gráfica de Precision\n",
        "          plt.subplot(1, 4, 2)\n",
        "          bars2 = plt.bar(nombres_modelos, precision_scores)\n",
        "          plt.title('Precision')\n",
        "          plt.xticks(rotation=45, ha='right')\n",
        "          plt.ylabel('Precision')\n",
        "          # Añadir valores en las barras\n",
        "          for bar in bars2:\n",
        "              height = bar.get_height()\n",
        "              plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                      f'{height:.4f}',\n",
        "                      ha='center', va='bottom', rotation=0)\n",
        "\n",
        "          # Gráfica de Recall\n",
        "          plt.subplot(1, 4, 3)\n",
        "          bars3 = plt.bar(nombres_modelos, recall_scores)\n",
        "          plt.title('Recall')\n",
        "          plt.xticks(rotation=45, ha='right')\n",
        "          plt.ylabel('Recall')\n",
        "          # Añadir valores en las barras\n",
        "          for bar in bars3:\n",
        "              height = bar.get_height()\n",
        "              plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                      f'{height:.4f}',\n",
        "                      ha='center', va='bottom', rotation=0)\n",
        "\n",
        "          # Gráfica de F1-Score\n",
        "          plt.subplot(1, 4, 4)\n",
        "          bars4 = plt.bar(nombres_modelos, f1_scores)\n",
        "          plt.title('F1-Score')\n",
        "          plt.xticks(rotation=45, ha='right')\n",
        "          plt.ylabel('F1-Score')\n",
        "          # Añadir valores en las barras\n",
        "          for bar in bars4:\n",
        "              height = bar.get_height()\n",
        "              plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                      f'{height:.4f}',\n",
        "                      ha='center', va='bottom', rotation=0)\n",
        "\n",
        "          plt.tight_layout()\n",
        "          plt.show()\n",
        "          exit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWprvJMPYo_D"
      },
      "source": [
        "Ejecucion del programa con estructura pythonica"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pkb3yN8QYtEw",
        "outputId": "42c5f0ac-d091-4376-bbe5-a985977f1af8"
      },
      "outputs": [],
      "source": [
        "# Ejecución del menú principal\n",
        "if __name__ == \"__main__\":\n",
        "    start = Start()\n",
        "    start.mostrar_menu()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
